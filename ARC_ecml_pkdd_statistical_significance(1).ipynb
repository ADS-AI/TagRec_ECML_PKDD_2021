{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "QC_ecml_pkdd_statistical_significance.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "sR9av2JU3kf6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d76fa70-6c61-4a21-992f-94994abcafc5"
      },
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "import torch\n",
        "import logging\n",
        "logging.basicConfig(level=logging.ERROR)\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJCr2Bi89Zw5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76f3c23a-96e3-45fd-dacf-4684f5441003"
      },
      "source": [
        "!pip install tensorflow==1.13.1\n",
        "! pip install tensorflow-hub==0.7.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.13.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/29/6b4f1e02417c3a1ccc85380f093556ffd0b35dc354078074c5195c8447f2/tensorflow-1.13.1-cp37-cp37m-manylinux1_x86_64.whl (92.6MB)\n",
            "\u001b[K     |████████████████████████████████| 92.6MB 96kB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (0.10.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (0.8.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (0.36.2)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (0.3.3)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.19.5)\n",
            "Collecting tensorflow-estimator<1.14.0rc0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/48/13f49fc3fa0fdf916aa1419013bb8f2ad09674c275b4046d5ee669a46873/tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 53.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.1.2)\n",
            "Collecting keras-applications>=1.0.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.9MB/s \n",
            "\u001b[?25hCollecting tensorboard<1.14.0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/39/bdd75b08a6fba41f098b6cb091b9e8c7a80e1b4d679a581a0ccd17b10373/tensorboard-1.13.1-py3-none-any.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 51.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.15.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.32.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (3.12.4)\n",
            "Collecting mock>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/5c/03/b7e605db4a57c0f6fba744b11ef3ddf4ddebcada35022927a2b5fc623fdf/mock-4.0.3-py3-none-any.whl\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.6->tensorflow==1.13.1) (2.10.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.3.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.6.1->tensorflow==1.13.1) (54.0.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.4.1)\n",
            "Installing collected packages: mock, tensorflow-estimator, keras-applications, tensorboard, tensorflow\n",
            "  Found existing installation: tensorflow-estimator 2.4.0\n",
            "    Uninstalling tensorflow-estimator-2.4.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
            "  Found existing installation: tensorboard 2.4.1\n",
            "    Uninstalling tensorboard-2.4.1:\n",
            "      Successfully uninstalled tensorboard-2.4.1\n",
            "  Found existing installation: tensorflow 2.4.1\n",
            "    Uninstalling tensorflow-2.4.1:\n",
            "      Successfully uninstalled tensorflow-2.4.1\n",
            "Successfully installed keras-applications-1.0.8 mock-4.0.3 tensorboard-1.13.1 tensorflow-1.13.1 tensorflow-estimator-1.13.0\n",
            "Collecting tensorflow-hub==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/00/0e/a91780d07592b1abf9c91344ce459472cc19db3b67fdf3a61dca6ebb2f5c/tensorflow_hub-0.7.0-py2.py3-none-any.whl (89kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 5.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-hub==0.7.0) (3.12.4)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-hub==0.7.0) (1.19.5)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-hub==0.7.0) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.4.0->tensorflow-hub==0.7.0) (54.0.0)\n",
            "Installing collected packages: tensorflow-hub\n",
            "  Found existing installation: tensorflow-hub 0.11.0\n",
            "    Uninstalling tensorflow-hub-0.11.0:\n",
            "      Successfully uninstalled tensorflow-hub-0.11.0\n",
            "Successfully installed tensorflow-hub-0.7.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "da-zaZJUGMFU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e262f59-0868-4ff9-c21d-c41847a6ecfd"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slZnjEXG5_-Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a06b5684-ec2d-45d8-ea73-94f57a8da232"
      },
      "source": [
        "!pip install sentence-transformers==0.2.6.1\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentence-transformers==0.2.6.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b9/46/b7d6c37d92d1bd65319220beabe4df845434930e3f30e42d3cfaecb74dc4/sentence-transformers-0.2.6.1.tar.gz (55kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 5.3MB/s \n",
            "\u001b[?25hCollecting transformers>=2.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f9/54/5ca07ec9569d2f232f3166de5457b63943882f7950ddfcc887732fc7fb23/transformers-4.3.3-py3-none-any.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 10.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers==0.2.6.1) (4.41.1)\n",
            "Requirement already satisfied: torch>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers==0.2.6.1) (1.8.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers==0.2.6.1) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers==0.2.6.1) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers==0.2.6.1) (1.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers==0.2.6.1) (3.2.5)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers>=2.8.0->sentence-transformers==0.2.6.1) (3.7.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers>=2.8.0->sentence-transformers==0.2.6.1) (2.23.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/23/2ddc317b2121117bf34dd00f5b0de194158f2a44ee2bf5e47c7166878a97/tokenizers-0.10.1-cp37-cp37m-manylinux2010_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 48.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers>=2.8.0->sentence-transformers==0.2.6.1) (20.9)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=2.8.0->sentence-transformers==0.2.6.1) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 53.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=2.8.0->sentence-transformers==0.2.6.1) (3.0.12)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0.1->sentence-transformers==0.2.6.1) (3.7.4.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers==0.2.6.1) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers==0.2.6.1) (1.15.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers>=2.8.0->sentence-transformers==0.2.6.1) (3.4.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=2.8.0->sentence-transformers==0.2.6.1) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=2.8.0->sentence-transformers==0.2.6.1) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=2.8.0->sentence-transformers==0.2.6.1) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=2.8.0->sentence-transformers==0.2.6.1) (3.0.4)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers>=2.8.0->sentence-transformers==0.2.6.1) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=2.8.0->sentence-transformers==0.2.6.1) (7.1.2)\n",
            "Building wheels for collected packages: sentence-transformers, sacremoses\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-0.2.6.1-cp37-none-any.whl size=74031 sha256=4f8543c5aa2f827dac455c16b9c225df0c7e24f6145889564b9657c257e1fbc8\n",
            "  Stored in directory: /root/.cache/pip/wheels/d7/fa/17/2b081a8cd8b0a86753fb0e9826b3cc19f0207062c0b2da7008\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp37-none-any.whl size=893262 sha256=291e65133a0792d638d0deac7ff52de149074874cea8a140768a60a2dad38f8a\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sentence-transformers sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers, sentence-transformers\n",
            "Successfully installed sacremoses-0.0.43 sentence-transformers-0.2.6.1 tokenizers-0.10.1 transformers-4.3.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzKeqoCs3kgA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2176fa4-8fa3-46f9-f778-fe8bad59dddc"
      },
      "source": [
        "!pip install transformers==2.8.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers==2.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/78/92cedda05552398352ed9784908b834ee32a0bd071a9b32de287327370b7/transformers-2.8.0-py3-none-any.whl (563kB)\n",
            "\u001b[K     |████████████████████████████████| 573kB 7.1MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d6/e3/5e49e9a83fb605aaa34a1c1173e607302fecae529428c28696fb18f1c2c9/tokenizers-0.5.2-cp37-cp37m-manylinux1_x86_64.whl (5.6MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6MB 14.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (0.0.43)\n",
            "Collecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7b/3c/92634ca0da72db047a4957bdd3984d72fbcc82fb9dcda04ea628c9a87dba/boto3-1.17.27-py2.py3-none-any.whl (130kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 49.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (2019.12.20)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 54.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (4.41.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.8.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.8.0) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.8.0) (1.15.0)\n",
            "Collecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Collecting s3transfer<0.4.0,>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/43/4b4a1b26eb03a429a4c37ca7fdf369d938bd60018fc194e94b8379b0c77c/s3transfer-0.3.4-py2.py3-none-any.whl (69kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 10.5MB/s \n",
            "\u001b[?25hCollecting botocore<1.21.0,>=1.20.27\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6f/b3/4cf3d9ba97f09a4081c4428f1014eef0e1f9b5deb95e4733799e3666f1a9/botocore-1.20.27-py2.py3-none-any.whl (7.3MB)\n",
            "\u001b[K     |████████████████████████████████| 7.3MB 53.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.8.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.8.0) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.8.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.8.0) (2.10)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.27->boto3->transformers==2.8.0) (2.8.1)\n",
            "\u001b[31mERROR: botocore 1.20.27 has requirement urllib3<1.27,>=1.25.4, but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tokenizers, jmespath, botocore, s3transfer, boto3, sentencepiece, transformers\n",
            "  Found existing installation: tokenizers 0.10.1\n",
            "    Uninstalling tokenizers-0.10.1:\n",
            "      Successfully uninstalled tokenizers-0.10.1\n",
            "  Found existing installation: transformers 4.3.3\n",
            "    Uninstalling transformers-4.3.3:\n",
            "      Successfully uninstalled transformers-4.3.3\n",
            "Successfully installed boto3-1.17.27 botocore-1.20.27 jmespath-0.10.0 s3transfer-0.3.4 sentencepiece-0.1.95 tokenizers-0.5.2 transformers-2.8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZFv4UU8sLTM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7184b03-7b52-4445-a089-d71472bb082f"
      },
      "source": [
        "!pip install git+https://github.com/geoopt/geoopt.git\n",
        "! pip install git+https://github.com/ferrine/hyrnn.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/geoopt/geoopt.git\n",
            "  Cloning https://github.com/geoopt/geoopt.git to /tmp/pip-req-build-eqy2w89e\n",
            "  Running command git clone -q https://github.com/geoopt/geoopt.git /tmp/pip-req-build-eqy2w89e\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from geoopt==0.3.1) (1.8.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from geoopt==0.3.1) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4.0->geoopt==0.3.1) (3.7.4.3)\n",
            "Building wheels for collected packages: geoopt\n",
            "  Building wheel for geoopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for geoopt: filename=geoopt-0.3.1-cp37-none-any.whl size=76168 sha256=a054af51698ec71e4c064cf8abebe76f98da43db9e249a28c413ebaa63c9d3db\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-zjoykplj/wheels/10/df/30/e0d857f034c142ca5f38af048b62aae3da773b272553e5dd21\n",
            "Successfully built geoopt\n",
            "Installing collected packages: geoopt\n",
            "Successfully installed geoopt-0.3.1\n",
            "Collecting git+https://github.com/ferrine/hyrnn.git\n",
            "  Cloning https://github.com/ferrine/hyrnn.git to /tmp/pip-req-build-c3nf13ud\n",
            "  Running command git clone -q https://github.com/ferrine/hyrnn.git /tmp/pip-req-build-c3nf13ud\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from hyrnn==0.0.0) (1.8.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from hyrnn==0.0.0) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0.0->hyrnn==0.0.0) (3.7.4.3)\n",
            "Building wheels for collected packages: hyrnn\n",
            "  Building wheel for hyrnn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hyrnn: filename=hyrnn-0.0.0-cp37-none-any.whl size=13955 sha256=4d82ed631a239c1981e6f1b1b3d4741e2d3a657c64cdb85b0d72c761085a3122\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-w3q__cxs/wheels/24/c3/64/cc0e9d25d466081dc154a2a8843157f54d845b916b4ba66418\n",
            "Successfully built hyrnn\n",
            "Installing collected packages: hyrnn\n",
            "Successfully installed hyrnn-0.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsADhaO93kgD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 959
        },
        "outputId": "df2e4eaf-9003-40f8-881c-3db6800a17e8"
      },
      "source": [
        "import pandas as pd\n",
        "train_data = pd.read_csv(\"train_QC_data.csv\")\n",
        "val_data = pd.read_csv(\"val_QC_data.csv\")\n",
        "test_data = pd.read_csv(\"test_QC_data.csv\")\n",
        "\n",
        "train_data\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>questionID</th>\n",
              "      <th>originalQuestionID</th>\n",
              "      <th>totalPossiblePoint</th>\n",
              "      <th>AnswerKey</th>\n",
              "      <th>isMultipleChoiceQuestion</th>\n",
              "      <th>includesDiagram</th>\n",
              "      <th>examName</th>\n",
              "      <th>grade</th>\n",
              "      <th>year</th>\n",
              "      <th>QCLabel</th>\n",
              "      <th>Question</th>\n",
              "      <th>subject</th>\n",
              "      <th>category</th>\n",
              "      <th>fold</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>VASoL_2008_3_34</td>\n",
              "      <td>34</td>\n",
              "      <td>1</td>\n",
              "      <td>C</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Virginia Standards of Learning - Science</td>\n",
              "      <td>3</td>\n",
              "      <td>2008</td>\n",
              "      <td>matter_properties of objects_TEXT</td>\n",
              "      <td>A student is asked to bring something that fee...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Train</td>\n",
              "      <td>Easy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>MCAS_2015_8_6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>B</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>MCAS</td>\n",
              "      <td>8</td>\n",
              "      <td>2015</td>\n",
              "      <td>celestial_FEATURES_STELLAR</td>\n",
              "      <td>Which of the following statements best describ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Test</td>\n",
              "      <td>Easy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Mercury_SC_417677</td>\n",
              "      <td>417677</td>\n",
              "      <td>1</td>\n",
              "      <td>B</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Mercury</td>\n",
              "      <td>4</td>\n",
              "      <td>2015</td>\n",
              "      <td>energy_LIGHT_REFLECT</td>\n",
              "      <td>A polished metal ball looks very shiny and bri...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Test</td>\n",
              "      <td>Challenge</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Mercury_7230423</td>\n",
              "      <td>7230423</td>\n",
              "      <td>1</td>\n",
              "      <td>A</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Mercury</td>\n",
              "      <td>9</td>\n",
              "      <td>2015</td>\n",
              "      <td>LIFE_EXTINCTION_MASSEX</td>\n",
              "      <td>Which was a main force driving extensive speci...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Test</td>\n",
              "      <td>Easy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>NYSEDREGENTS_2007_8_6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>NYSEDREGENTS</td>\n",
              "      <td>8</td>\n",
              "      <td>2007</td>\n",
              "      <td>Life_functions_features and functions_CELLBIO_...</td>\n",
              "      <td>Compared to the amount of hereditary informati...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Train</td>\n",
              "      <td>Challenge</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5592</th>\n",
              "      <td>Mercury_402502</td>\n",
              "      <td>402502</td>\n",
              "      <td>1</td>\n",
              "      <td>D</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Mercury</td>\n",
              "      <td>8</td>\n",
              "      <td>2015</td>\n",
              "      <td>matter_chemistry_periodic table</td>\n",
              "      <td>According to the periodic table, argon is foun...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Test</td>\n",
              "      <td>Challenge</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5593</th>\n",
              "      <td>MCAS_2006_9_20</td>\n",
              "      <td>20</td>\n",
              "      <td>1</td>\n",
              "      <td>B</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>MCAS</td>\n",
              "      <td>9</td>\n",
              "      <td>2006</td>\n",
              "      <td>FOR_MOMENTUM</td>\n",
              "      <td>Which of the following has the least momentum?...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Train</td>\n",
              "      <td>Challenge</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5594</th>\n",
              "      <td>NYSEDREGENTS_2013_8_35</td>\n",
              "      <td>35</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>NYSEDREGENTS</td>\n",
              "      <td>8</td>\n",
              "      <td>2013</td>\n",
              "      <td>Life_functions_features and functions_PLANT_PH...</td>\n",
              "      <td>The amount of which greenhouse gas in the air ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Test</td>\n",
              "      <td>Easy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5595</th>\n",
              "      <td>Mercury_7082670</td>\n",
              "      <td>7082670</td>\n",
              "      <td>1</td>\n",
              "      <td>C</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Mercury</td>\n",
              "      <td>7</td>\n",
              "      <td>2015</td>\n",
              "      <td>energy_LIGHT_electromagnetic spectrum</td>\n",
              "      <td>The visible light spectrum can be subdivided a...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Test</td>\n",
              "      <td>Easy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5596</th>\n",
              "      <td>Mercury_7004970</td>\n",
              "      <td>7004970</td>\n",
              "      <td>1</td>\n",
              "      <td>C</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Mercury</td>\n",
              "      <td>8</td>\n",
              "      <td>2015</td>\n",
              "      <td>Life_reproduction_DNA inheritance_DOMRECESS</td>\n",
              "      <td>A scientist crosses a red-flowered plant with ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Train</td>\n",
              "      <td>Easy</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5597 rows × 14 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                  questionID originalQuestionID  ...  category       fold\n",
              "0            VASoL_2008_3_34                 34  ...     Train       Easy\n",
              "1              MCAS_2015_8_6                  6  ...      Test       Easy\n",
              "2          Mercury_SC_417677             417677  ...      Test  Challenge\n",
              "3            Mercury_7230423            7230423  ...      Test       Easy\n",
              "4      NYSEDREGENTS_2007_8_6                  6  ...     Train  Challenge\n",
              "...                      ...                ...  ...       ...        ...\n",
              "5592          Mercury_402502             402502  ...      Test  Challenge\n",
              "5593          MCAS_2006_9_20                 20  ...     Train  Challenge\n",
              "5594  NYSEDREGENTS_2013_8_35                 35  ...      Test       Easy\n",
              "5595         Mercury_7082670            7082670  ...      Test       Easy\n",
              "5596         Mercury_7004970            7004970  ...     Train       Easy\n",
              "\n",
              "[5597 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGAnZbmnGrVI"
      },
      "source": [
        "!cp -r \"/content/drive/My Drive/research_lo_content_taxonomy_classification/model_euclidean_SENT_BERT_cos_QC/\" /content/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nm71p_ewXe4D"
      },
      "source": [
        "!cp -r \"/content/drive/My Drive/research_lo_content_taxonomy_classification/model_euclidean_glove_QC/\" /content/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bocSFvkT1pOG"
      },
      "source": [
        "!cp -r \"/content/drive/My Drive/research_lo_content_taxonomy_classification/model_save_categorized_reduced_QC/\" /content/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oqlcl1XTmkz2"
      },
      "source": [
        "!cp -r \"/content/drive/My Drive/research_lo_content_taxonomy_classification/model_euclidean_USE_cos_final/\" /content/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AyvDLcAPmk2O"
      },
      "source": [
        "# for i in range(100000000000000000000000):\n",
        "#   j=i"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQhO6qqt6lge"
      },
      "source": [
        "from google.colab import files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIrS5sxE3kgk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b8af4ed-c10b-4934-a9a4-b92a8c93aabc"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('model_euclidean_SENT_BERT_cos_QC', do_lower_case=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading BERT tokenizer...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijn1nIpByb3e"
      },
      "source": [
        "train_features = train_data[\"Question\"]\n",
        "test_features = test_data[\"Question\"]\n",
        "train_labels = train_data[\"QCLabel\"]\n",
        "test_labels = test_data[\"QCLabel\"]\n",
        "val_features = val_data[\"Question\"]\n",
        "val_labels = val_data[\"QCLabel\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkyM7gqv3khI"
      },
      "source": [
        "\n",
        "question_answer = train_features.values\n",
        "categories = train_labels.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FN1zRXMOXwLK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f5194b4-89d9-4515-c1eb-23f3ff8a2159"
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "!pip install inflection\n",
        "\n",
        "from bokeh.io import output_file, output_notebook, show\n",
        "from bokeh.plotting import figure\n",
        "from bokeh.transform import linear_cmap\n",
        "from bokeh.util.hex import hexbin\n",
        "from bokeh.models import HoverTool\n",
        "from bokeh import colors\n",
        "import inflection\n",
        "\n",
        "from nltk.stem import PorterStemmer \n",
        "ps = PorterStemmer()\n",
        "from gzip import open as gopen\n",
        "from pandas.core.common import flatten\n",
        "import gensim.models.poincare as poincare\n",
        "def get_cleaned_taxonomy(taxonomy):\n",
        "  cleaned_taxonomy = []\n",
        "  for value in taxonomy:\n",
        "      value = ' '.join(value.split(\"_\"))\n",
        "      # taxonomy_words = [inflection.singularize(val)  for token in value for val in token.split(\" \") if val.isalpha()]\n",
        "      cleaned_taxonomy.append( value )\n",
        "  return cleaned_taxonomy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: inflection in /usr/local/lib/python3.7/dist-packages (0.5.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPAl0TNuX6mx"
      },
      "source": [
        "\n",
        "# course_taxonomy\n",
        "\n",
        "poincare_emb_data = get_cleaned_taxonomy(categories)\n",
        "poincare_val = get_cleaned_taxonomy(val_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aedZzkBsqEeK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "051b36d7-c529-49b9-865b-0c4d94e45987"
      },
      "source": [
        "poincare_emb_data[2]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'energy LIGHT REFLECT'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZN6jdKp0uhO3"
      },
      "source": [
        "\n",
        "import sys\n",
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from matplotlib import pyplot as plt\n",
        "from torch.nn.modules.loss import HingeEmbeddingLoss\n",
        "from random import randint\n",
        "\n",
        "from tqdm import tqdm\n",
        "import geoopt\n",
        "import time\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from matplotlib import pyplot as plt\n",
        "from torch.nn.modules.loss import HingeEmbeddingLoss\n",
        "from random import randint\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import time\n",
        "import argparse\n",
        "cos = nn.CosineSimilarity(dim=0, eps=1e-6)\n",
        "# Neural Classifierwork\n",
        "class MulticlassClassifier(nn.Module):\n",
        "    def __init__(self,bert_model_path):\n",
        "        super(MulticlassClassifier,self).__init__()\n",
        "        self.bert = BertModel.from_pretrained(bert_model_path,output_hidden_states=False,output_attentions=False)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.fc1 = nn.Linear(768, 384)\n",
        "        self.fc2 = nn.Linear(384, 1024)\n",
        "\n",
        "    def forward(self,tokens,masks):\n",
        "        _, pooled_output = self.bert(tokens, attention_mask=masks)\n",
        "        x = self.fc1(pooled_output)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "class MyHingeLoss(torch.nn.Module):\n",
        "    def __init__(self, margin):\n",
        "        super(MyHingeLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "    # def forward_val(self, output, target):\n",
        "    #     cos = nn.CosineSimilarity(dim=0, eps=1e-6)\n",
        "    #     loss = 0\n",
        "    #     num_compare = 4\n",
        "    #     count = 0\n",
        "    #     for i in range(len(output)):\n",
        "    #         v_image = output[i]\n",
        "    #         t_label = target[i]\n",
        "    #         for j in range(num_compare):\n",
        "    #             if j != i:\n",
        "    #                 count += 1\n",
        "    #                 t_j = target[j]\n",
        "    #                 loss += torch.relu( self.margin - cos(t_label, v_image) + cos(t_j, v_image) )\n",
        "    #     return loss / count\n",
        "\n",
        "    def forward(self, output, target):\n",
        "        loss=0\n",
        "        for i in range(len(output)):\n",
        "            v_image = F.normalize(output[i],p=2,dim=0)\n",
        "            t_label = F.normalize(target[i],p=2,dim=0)\n",
        "            j = randint(0, len(output)-1)\n",
        "            while j == i:\n",
        "                j = randint(0, len(output)-1)\n",
        "            t_j = F.normalize(target[j],p=2,dim=0)\n",
        "            loss+= torch.relu( self.margin - cos(t_label, v_image) + cos(t_j, v_image) )\n",
        "        return loss / len(output)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1C2etny7les",
        "outputId": "924744c9-76a6-431f-be9e-dfb8077d8c81"
      },
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "# Loads BertForSequenceClassification, the pretrained BERT model with a single \n",
        "model_multi_class = BertForSequenceClassification.from_pretrained(\n",
        "    \"model_save_categorized_reduced_QC\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = 420,   \n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = True, # Whether the model returns all hidden-states.\n",
        ")\n",
        "# model.load_state_dict(torch.load('model_save_categorized_reduced_nov/model_weights'))\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model_multi_class.cuda()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=420, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHYyjxMDIx2U"
      },
      "source": [
        "from transformers import BertModel, AdamW, BertConfig\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4178_yLFMWmx"
      },
      "source": [
        "test_features = test_features.values\n",
        "labels = test_labels.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZpmBJuIC2nM"
      },
      "source": [
        "\n",
        "test_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0hxcNFx2w8r",
        "outputId": "e727d9b3-5eb5-4fb4-9b2e-ba695389f820"
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "kf = KFold(n_splits=5, shuffle=True)\n",
        "kf.split(test_features)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<generator object _BaseKFold.split at 0x7f57bf2a6a50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRZ54gFokNh9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbb7205e-aab8-40e2-9b1d-abb401f3e871"
      },
      "source": [
        "labels"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([349, 282, 299, ..., 194, 137,  23])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ohj1x7frQJ1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afbc5860-0f04-4a66-f81e-9d69f06df519"
      },
      "source": [
        "len(list(set(labels)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "352"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kjinn0gXkNuP"
      },
      "source": [
        "\n",
        "# course_taxonomy\n",
        "test_labels = list(set(test_data[\"QCLabel\"].values))\n",
        "emb_data_test = get_cleaned_taxonomy(test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dorQwznCeMpR"
      },
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "sent_model = SentenceTransformer('bert-large-nli-stsb-mean-tokens')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sL2dQ-jAoIop",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b533340-c3c4-4f42-8334-0edf8f6d92a3"
      },
      "source": [
        "import gensim.downloader as api\n",
        "wv = api.load('glove-wiki-gigaword-300')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 376.1/376.1MB downloaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9keiJWnRX1z4"
      },
      "source": [
        "\n",
        "# course_taxonomy\n",
        "test_labels = list(set(test_data[\"QCLabel\"].values))\n",
        "poincare_emb_data = get_cleaned_taxonomy(test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDa8TZDjX1z5",
        "outputId": "d13b896f-7f78-4cee-fe52-aaab6cf5eeaa"
      },
      "source": [
        "taxonomy_vectors_glove = []\n",
        "for feature in poincare_emb_data:\n",
        "  token_embeddings = []\n",
        "  for token in feature.split(\" \"):\n",
        "    token=token.lower()\n",
        "    if token in wv:\n",
        "      token_embeddings.append(wv[token])\n",
        "  token_emb  = np.vstack(token_embeddings)\n",
        "  taxonomy_vectors_glove.append(np.mean(token_emb,axis=0))\n",
        "taxonomy_vectors_glove = np.vstack(taxonomy_vectors_glove)\n",
        "taxonomy_vectors_glove.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(352, 300)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 223
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wl0RJ3SSW4i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cf0b691-2cb5-4c1d-d376-1012cb1aadb9"
      },
      "source": [
        "# taxonomy_vectors = []\"\"\n",
        "taxonomy_vectors = sent_model.encode(emb_data_test)\n",
        "taxonomy_vectors = np.vstack(taxonomy_vectors)\n",
        "taxonomy_vectors.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(352, 1024)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 187
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uv8cTot-aME6",
        "outputId": "cc3c985b-397c-46ea-e875-a5f9fb10e32e"
      },
      "source": [
        "taxonomy_vectors[0].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(300,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 186
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nso39n1N_po_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "feaa64d2-4ede-40b7-cc4b-a0acaabad6ee"
      },
      "source": [
        "model = MulticlassClassifier('bert-base-uncased')\n",
        "model.load_state_dict(torch.load('model_euclidean_SENT_BERT_cos_QC/model_weights'))\n",
        "model.cuda()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MulticlassClassifier(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (fc1): Linear(in_features=768, out_features=384, bias=True)\n",
              "  (fc2): Linear(in_features=384, out_features=1024, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uh3i6-6YYAil"
      },
      "source": [
        "class MulticlassClassifier(nn.Module):\n",
        "    def __init__(self,bert_model_path):\n",
        "        super(MulticlassClassifier,self).__init__()\n",
        "        self.bert = BertModel.from_pretrained(bert_model_path,output_hidden_states=False,output_attentions=False)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.fc1 = nn.Linear(768, 384)\n",
        "        self.fc2 = nn.Linear(384, 300)\n",
        "\n",
        "    def forward(self,tokens,masks):\n",
        "        _, pooled_output = self.bert(tokens, attention_mask=masks)\n",
        "        x = self.fc1(pooled_output)\n",
        "        x = self.fc2(x)\n",
        "        return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5sgZMaNYFgC",
        "outputId": "c49cc2a8-7831-40e5-f681-d2a0b720ad21"
      },
      "source": [
        "from transformers import BertModel, AdamW, BertConfig\n",
        "\n",
        "# Loads BertModel, the pretrained BERT model with a single \n",
        "model_glove = MulticlassClassifier('bert-base-uncased')\n",
        "model_glove.load_state_dict(torch.load('model_euclidean_glove_QC/model_weights'))\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model_glove.cuda()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MulticlassClassifier(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (fc1): Linear(in_features=768, out_features=384, bias=True)\n",
              "  (fc2): Linear(in_features=384, out_features=300, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 205
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qe4qYkV2C4fX"
      },
      "source": [
        "input_ids = []\n",
        "attention_masks = []\n",
        "for sent in train_features:\n",
        "\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 128,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        truncation=True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "# labels = torch.tensor(labels)\n",
        "\n",
        "# Set the batch size.  \n",
        "batch_size = 32  \n",
        "# test_poincare_tensor = torch.tensor(taxonomy_vectors,dtype=torch.float)\n",
        "\n",
        "# Create the DataLoader.\n",
        "# prediction_data = TensorDataset(test_input_ids, test_attention_masks, test_poincare_tensor)\n",
        "# prediction_sampler = SequentialSampler(prediction_data)\n",
        "# prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlgiwrmLOwbO"
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "class_emb = {cls:[] for cls in list(set(train_data[\"QCLabel\"].values))}\n",
        "for index,label in enumerate(list(set(train_data[\"QCLabel\"].values))):\n",
        "  sample_indices_for_label = np.where(train_labels == label)[0][:3]\n",
        "  input_ids_for_class = input_ids[sample_indices_for_label]\n",
        "  attention_masks_class = attention_masks[sample_indices_for_label]\n",
        "  input_ids_for_class = input_ids_for_class.to(device)\n",
        "  attention_masks_class = attention_masks_class.to(device)\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    outputs = model_multi_class(input_ids_for_class,attention_masks_class)\n",
        "  # print(len(sample_indices_for_label),torch.mean(torch.cat((outputs[1][-2][:,0,:],outputs[1][-3][:,0,:],outputs[1][-4][:,0,:],outputs[1][-5][:,0,:]),dim=1),dim=0).shape,input_ids_for_class.shape)\n",
        "  class_emb[label] = torch.cat((outputs[1][-2][0][0],outputs[1][-3][0][0],outputs[1][-4][0][0],outputs[1][-5][0][0]),dim=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgjlpYeVPyeP",
        "outputId": "ed75c56d-2083-42d9-a6ca-32a22a3752a8"
      },
      "source": [
        "class_values = [item[1] for item in class_emb.items()]\n",
        "class_prototype_embeddings = torch.stack(class_values,dim=0)\n",
        "class_prototype_embeddings.to('cuda')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0122, -0.8191,  0.3060,  ..., -0.1863, -0.3371,  0.4865],\n",
              "        [ 0.0775,  0.0075, -0.2368,  ..., -0.3233, -0.4790,  0.3708],\n",
              "        [ 0.3962, -0.2155, -0.3501,  ..., -0.1602, -0.3938,  0.4521],\n",
              "        ...,\n",
              "        [ 0.8396, -0.8144, -0.9089,  ..., -0.2618, -0.3365,  0.4882],\n",
              "        [ 0.0194, -0.3093,  0.3774,  ..., -0.1982, -0.1186,  0.4908],\n",
              "        [-0.2893, -0.8658,  0.3535,  ...,  0.0372, -0.1535,  0.3684]],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcOhfGZUCyie"
      },
      "source": [
        "def get_recall_at_k(labels,predictions,indices):\n",
        "    labels=test_data['QCLabel'].values\n",
        "    LE= LabelEncoder()\n",
        "    LE.fit_transform(pd.concat([train_data['QCLabel'],test_data['QCLabel']]))\n",
        "\n",
        "    labels = LE.transform(labels)\n",
        "    labels=labels[indices[0]]\n",
        "    final_predictions = []\n",
        "    for prediction in predictions:\n",
        "      final_predictions.append(LE.transform(prediction))\n",
        "    y_true = np.array(labels)\n",
        "    y_true = tf.identity(y_true)\n",
        "    y_pred = np.array(final_predictions)\n",
        "    y_pred = tf.identity(y_pred)\n",
        "    print(y_pred.shape,y_true.shape)\n",
        "    k = 20\n",
        "    recall, update_recall = tf.compat.v1.metrics.recall_at_top_k(y_true, y_pred, k=20)\n",
        "    precision, update_precision = tf.compat.v1.metrics.precision_at_top_k(y_true, y_pred, k=20)\n",
        "\n",
        "    tmp_rank = tf.nn.top_k(y_pred, 20)\n",
        "    stream_vars = [i for i in tf.local_variables()]\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(tf.local_variables_initializer())\n",
        "        # print(\"precision\",sess.run(update_precision))\n",
        "        # print(\"precision\",s|ess.run(precision))\n",
        "\n",
        "        # print(\"update_recall: \",sess.run(update_recall ))\n",
        "        # print(\"recall\",sess.run(recall))\n",
        "\n",
        "        # print(\"STREAM_VARS: \",(sess.run(stream_vars)))\n",
        "        # print(\"TMP_RANK: \",sess.run(tmp_rank))\n",
        "        return sess.run(update_recall )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JsyUOOgDiRI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBot2nm9DiRJ"
      },
      "source": [
        "from sklearn .preprocessing import LabelEncoder\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpxT6BPNDiRK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJneekKaCLJZ"
      },
      "source": [
        "def get_predictions(input_ids,attention_masks,test_poincare_tensor,indices,class_prototype_embeddings,class_keys,test_poincare_tensor_glove):\n",
        "\n",
        "  print('Predicting labels for {:,} test sentences...'.format(len(test_input_ids)))\n",
        "\n",
        "  # Put model in evaluation mode\n",
        "  model.eval()\n",
        "  cos = torch.nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "\n",
        "  input_ids = input_ids.to('cuda')\n",
        "  attention_masks = attention_masks.to('cuda')\n",
        "  test_poincare_tensor = test_poincare_tensor.to('cuda')\n",
        "  test_poincare_tensor_glove = test_poincare_tensor_glove.to('cuda')\n",
        "  # Tracking variables \n",
        "  predictions , true_labels = [], []\n",
        "  for input_id,attention_mask in zip(input_ids, attention_masks):\n",
        "    with torch.no_grad():\n",
        "      outputs = model(input_id.reshape(1,-1),attention_mask.reshape(1,-1))\n",
        "      \n",
        "    distances = cos(outputs,test_poincare_tensor)#torch.topk(cos(outputs,test_poincare_tensor),20,largest=True)\n",
        "    distances,index = torch.topk(distances,20,largest=True)\n",
        "    predictions.append(test_labels[index.cpu().numpy()])\n",
        "  model_multi_class.eval()\n",
        "  cos = torch.nn.CosineSimilarity(dim=-1, eps=1e-6)\n",
        "  class_prototype_embeddings = class_prototype_embeddings.to('cuda')\n",
        "    # Tracking variables1\n",
        "  multi_predictions , true_labels = [], []\n",
        "  for input_id,attention_mask in zip(input_ids, attention_masks):\n",
        "    with torch.no_grad():\n",
        "      outputs = model_glove(input_id.reshape(1,-1),attention_mask.reshape(1,-1))\n",
        "      \n",
        "    distances = cos(outputs,test_poincare_tensor_glove)\n",
        "    distances,indexed = torch.topk(distances,20,largest=True)\n",
        "    multi_predictions.append(test_labels[indexed.cpu().numpy()])\n",
        "\n",
        "\n",
        "  return get_recall_at_k(labels,predictions,indices),get_recall_at_k(labels,multi_predictions,indices)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsBXz5gVk7LN",
        "outputId": "3ec3685a-eef5-4daa-b797-f50c5474431e"
      },
      "source": [
        "test_poincare_tensor_glove.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([352, 300])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 236
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgy31zb48_aU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0fb1537-6112-4072-c1ec-4ab15f035975"
      },
      "source": [
        "r_k_sent_bert = []\n",
        "r_k_multi_class =[]\n",
        "for indices in kf.split(test_features):\n",
        "  test_input_ids = []\n",
        "  test_attention_masks = []\n",
        "  for sent in test_features[indices[0]]:\n",
        "\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 128,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        truncation=True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    test_input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    test_attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "\n",
        "  input_ids = torch.cat(test_input_ids, dim=0)\n",
        "  attention_masks = torch.cat(test_attention_masks, dim=0)\n",
        "  # labels = test_labels.values\n",
        "  # test_labels_tensor = torch.tensor(taxonomy_vectors)\n",
        "  test_poincare_tensor = torch.tensor(taxonomy_vectors,dtype=torch.float)\n",
        "  test_poincare_tensor_glove = torch.tensor(taxonomy_vectors_glove,dtype=torch.float)\n",
        "\n",
        "  batch_size = 32  \n",
        "\n",
        "  # prediction_data = TensorDataset(input_ids, attention_masks, test_labels_tensor,test_skill_labels_tensor)\n",
        "  # prediction_sampler = SequentialSampler(prediction_data)\n",
        "  # prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n",
        "  import tensorflow as tf\n",
        "  class_keys = [item[0] for item in class_emb.items()]\n",
        "  class_keys = np.array(class_keys)\n",
        "  r_k_bert,r_k_multi = get_predictions(input_ids,attention_masks,test_poincare_tensor,indices,class_prototype_embeddings,class_keys,test_poincare_tensor_glove)\n",
        "  print(\"r_k_bert\",r_k_bert)\n",
        "  print(\"r_k_multi\",r_k_multi)\n",
        "  r_k_sent_bert.append(r_k_bert)\n",
        "  r_k_multi_class.append(r_k_multi)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting labels for 1,120 test sentences...\n",
            "(1120, 20) (1120,)\n",
            "(1120, 20) (1120,)\n",
            "r_k_bert 0.8723214285714286\n",
            "r_k_multi 0.8589285714285714\n",
            "Predicting labels for 1,120 test sentences...\n",
            "(1120, 20) (1120,)\n",
            "r_k_bert 0.8803571428571428\n",
            "r_k_multi 0.8580357142857142\n",
            "Predicting labels for 1,120 test sentences...\n",
            "(1120, 20) (1120,)\n",
            "(1120, 20) (1120,)\n",
            "r_k_bert 0.8821428571428571\n",
            "r_k_multi 0.86875\n",
            "Predicting labels for 1,120 test sentences...\n",
            "(1120, 20) (1120,)\n",
            "(1120, 20) (1120,)\n",
            "r_k_bert 0.8794642857142857\n",
            "r_k_multi 0.8660714285714286\n",
            "Predicting labels for 1,120 test sentences...\n",
            "(1120, 20) (1120,)\n",
            "(1120, 20) (1120,)\n",
            "r_k_bert 0.8714285714285714\n",
            "r_k_multi 0.8553571428571428\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_oMb7gEOJL0",
        "outputId": "44c8b09d-e7b9-4b4b-b043-d8f18551c384"
      },
      "source": [
        "from scipy import stats\n",
        "stats.ttest_rel(r_k_multi_class,r_k_sent_bert)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Ttest_relResult(statistic=-9.076506967077343, pvalue=0.0008168164830242052)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 241
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BvtXkhdDdVPI",
        "outputId": "82a5845c-c458-4ce4-8845-a613792f4505"
      },
      "source": [
        "r_k_multi_class"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.8589285714285714,\n",
              " 0.8580357142857142,\n",
              " 0.86875,\n",
              " 0.8660714285714286,\n",
              " 0.8553571428571428]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 242
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "veD3fO7ndTto",
        "outputId": "b4469ea7-c9be-4131-d4cf-2cca68757ef1"
      },
      "source": [
        "r_k_sent_bert"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.8723214285714286,\n",
              " 0.8803571428571428,\n",
              " 0.8821428571428571,\n",
              " 0.8794642857142857,\n",
              " 0.8714285714285714]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 243
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNdlve8AJcCO"
      },
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "test_poincare_tensor = torch.tensor(taxonomy_vectors,dtype=torch.float)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6aMBHkAQZjT"
      },
      "source": [
        "cos = torch.nn.CosineSimilarity(dim=0, eps=1e-6)\n",
        "def dist_without_grad( u, v):\n",
        "  sqdist = torch.sum((u - v) ** 2, dim=-1)\n",
        "  squnorm = torch.sum(u ** 2, dim=-1)\n",
        "  sqvnorm = torch.sum(v ** 2, dim=-1)\n",
        "  x = 1 + 2 * sqdist / ((1 - squnorm) * (1 - sqvnorm)) + 1e-7\n",
        "  z = torch.sqrt(x ** 2 - 1)\n",
        "  return torch.log(x + z)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oe0otXOPg7z0"
      },
      "source": [
        "test_labels = np.array(test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxxB6hEgXGsH"
      },
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "test_input_ids = []\n",
        "test_attention_masks = []\n",
        "for sent in test_features:\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 128,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        truncation=True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    test_input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    test_attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "test_input_ids = torch.cat(test_input_ids, dim=0)\n",
        "test_attention_masks = torch.cat(test_attention_masks, dim=0)\n",
        "# labels = torch.tensor(labels)\n",
        "\n",
        "# Set the batch size.  \n",
        "batch_size = 32  \n",
        "# test_poincare_tensor = torch.tensor(taxonomy_vectors,dtype=torch.float)\n",
        "\n",
        "# Create the DataLoader.\n",
        "# prediction_data = TensorDataset(test_input_ids, test_attention_masks, labels)\n",
        "# prediction_sampler = SequentialSampler(prediction_data)\n",
        "# prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFZJufpHW-9S",
        "outputId": "f5df049e-b338-4df6-a95c-bf92667b9cad"
      },
      "source": [
        "print('Predicting labels for {:,} test sentences...'.format(len(test_input_ids)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "cos = torch.nn.CosineSimilarity(dim=-1, eps=1e-6)\n",
        "\n",
        "test_input_ids = test_input_ids.to('cuda')\n",
        "test_attention_masks = test_attention_masks.to('cuda')\n",
        "class_prototype_embeddings = class_prototype_embeddings.to('cuda')\n",
        "# Tracking variables1\n",
        "predictions , true_labels = [], []\n",
        "for input_id,attention_mask in zip(test_input_ids, test_attention_masks):\n",
        "  with torch.no_grad():\n",
        "    outputs = model_multi_class(input_id.reshape(1,-1),attention_mask.reshape(1,-1))\n",
        "  # print(torch.mean(outputs[1][0].squeeze(),dim=0).shape)\n",
        "  distances = cos(torch.cat((outputs[1][-2][0][0], outputs[1][-3][0][0],outputs[1][-4][0][0],outputs[1][-5][0][0])),class_prototype_embeddings)\n",
        "  distances,indices = torch.topk(distances,20,largest=True)\n",
        "  predictions.append(class_keys[indices.cpu().numpy()])\n",
        "print(len(predictions))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting labels for 1,400 test sentences...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPCktQT9DVT4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b052a31c-1a2a-466d-802f-299972e3d921"
      },
      "source": [
        "# Prediction on test set\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(test_input_ids)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "cos = torch.nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "\n",
        "input_ids = test_input_ids.to('cuda')\n",
        "attention_masks = test_attention_masks.to('cuda')\n",
        "test_poincare_tensor = test_poincare_tensor.to('cuda')\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "for input_id,attention_mask in zip(input_ids, attention_masks):\n",
        "  with torch.no_grad():\n",
        "    outputs = model(input_id.reshape(1,-1),attention_mask.reshape(1,-1))\n",
        "    \n",
        "  distances = cos(outputs,test_poincare_tensor)#torch.topk(cos(outputs,test_poincare_tensor),20,largest=True)\n",
        "  distances,indices = torch.topk(distances,20,largest=True)\n",
        "  predictions.append(test_labels[indices.cpu().numpy()])\n",
        "print(len(predictions))\n",
        "  # max_distance =100000000000000\n",
        "  # label=None\n",
        "  # for index,test_poincare in enumerate(test_poincare_tensor):\n",
        "\n",
        "  #   distance = distanceTo(test_poincare, outputs)\n",
        "  #   if distance < max_distance:\n",
        "  #     max_distance = distance\n",
        "  #     label = index\n",
        "  # predictions.append(labels[label])\n",
        "    \n",
        "# Predict \n",
        "# for batch in prediction_dataloader:\n",
        "#   # Add batch to GPU\n",
        "#   batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "#   # Unpack the inputs from our dataloader\n",
        "#   b_input_ids, b_input_mask, b_labels = batch\n",
        "  \n",
        "#   # Telling the model not to compute or store gradients, saving memory and \n",
        "#   # speeding up prediction\n",
        "#   with torch.no_grad():\n",
        "#       # Forward pass, calculate logit predictions\n",
        "#       outputs = model(b_input_ids,b_input_mask)\n",
        "\n",
        "#   logits = outputs\n",
        "#   for logit in logits:\n",
        "#     max_similarity = 0\n",
        "\n",
        "\n",
        "#   # Move logits and labels to CPU\n",
        "#   logits = logits.detach().cpu().numpy()\n",
        "#   label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "#   # Store predictions and true labels\n",
        "#   predictions.append(logits)\n",
        "#   true_labels.append(label_ids)\n",
        "\n",
        "print('    DONE.')\n",
        "# predictions"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting labels for 1,400 test sentences...\n",
            "1400\n",
            "    DONE.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSbzmHGslxzg",
        "outputId": "2a0b11f3-f5bb-4cc6-b358-2c6cae67fcf5"
      },
      "source": [
        "# Prediction on test set\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(test_input_ids)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "cos = torch.nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "test_poincare_tensor_glove = torch.tensor(taxonomy_vectors_glove,dtype=torch.float)\n",
        "\n",
        "input_ids = test_input_ids.to('cuda')\n",
        "attention_masks = test_attention_masks.to('cuda')\n",
        "test_poincare_tensor = test_poincare_tensor_glove.to('cuda')\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "for input_id,attention_mask in zip(input_ids, attention_masks):\n",
        "  with torch.no_grad():\n",
        "    outputs = model_glove(input_id.reshape(1,-1),attention_mask.reshape(1,-1))\n",
        "    \n",
        "  distances = cos(outputs,test_poincare_tensor)\n",
        "  distances,indices = torch.topk(distances,20,largest=True)\n",
        "  predictions.append(test_labels[indices.cpu().numpy()])\n",
        "print(len(predictions))\n",
        "  \n",
        "print('    DONE.')\n",
        "# predictions"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting labels for 1,400 test sentences...\n",
            "1400\n",
            "    DONE.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5wj2gXYFkrQ"
      },
      "source": [
        "labels=test_data['QCLabel'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3lKinqqQV8S",
        "outputId": "98363022-e706-4d20-aad6-9e636f4b8f5d"
      },
      "source": [
        "LE.fit_transform(pd.concat([train_data['QCLabel'],test_data['QCLabel']]))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([399, 260, 293, ..., 231, 159,  28])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 227
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOt8hvs-CZfn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a8f0313-8603-450d-e01d-e931d82169fe"
      },
      "source": [
        "from sklearn .preprocessing import LabelEncoder\n",
        "# LE= LabelEncoder()\n",
        "labels = LE.transform(labels)\n",
        "labels"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([416, 336, 354, ..., 231, 159,  28])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 228
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adb4gTNgGKUP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff239758-747f-45ce-bab1-641fe3fb4691"
      },
      "source": [
        "labels"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([416, 336, 354, ..., 231, 159,  28])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 229
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azkCfK-bCoXd"
      },
      "source": [
        "final_predictions = []\n",
        "for prediction in predictions:\n",
        "  final_predictions.append(LE.transform(prediction))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQrlczKxMwzZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e0adaf4-460a-4b39-e0e1-69e0c526638b"
      },
      "source": [
        "len(final_predictions[-1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 231
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUk41t_9H8K3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d7e5d4a-3282-495f-ee17-5020f5793b34"
      },
      "source": [
        "import tensorflow as tf\n",
        "y_true = np.array(labels)\n",
        "y_true = tf.identity(y_true)\n",
        "y_pred = np.array(final_predictions)\n",
        "y_pred = tf.identity(y_pred)\n",
        "print(y_pred.shape,y_true.shape)\n",
        "k = 20\n",
        "recall, update_recall = tf.compat.v1.metrics.recall_at_top_k(y_true, y_pred, k=20)\n",
        "precision, update_precision = tf.compat.v1.metrics.precision_at_top_k(y_true, y_pred, k=20)\n",
        "\n",
        "tmp_rank = tf.nn.top_k(y_pred, 20)\n",
        "stream_vars = [i for i in tf.local_variables()]\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.local_variables_initializer())\n",
        "    # print(\"precision\",sess.run(update_precision))\n",
        "    # print(\"precision\",s|ess.run(precision))\n",
        "\n",
        "    print(\"update_recall: \",sess.run(update_recall ))\n",
        "    print(\"recall\",sess.run(recall))\n",
        "\n",
        "    print(\"STREAM_VARS: \",(sess.run(stream_vars)))\n",
        "    print(\"TMP_RANK: \",sess.run(tmp_rank))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1400, 20) (1400,)\n",
            "update_recall:  0.8614285714285714\n",
            "recall 0.8614285714285714\n",
            "STREAM_VARS:  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1206.0, 194.0, 0.0, 0.0]\n",
            "TMP_RANK:  TopKV2(values=array([[418, 417, 416, ..., 237,  98,  21],\n",
            "       [339, 338, 337, ...,  46,  36,  32],\n",
            "       [398, 395, 394, ..., 311, 286, 283],\n",
            "       ...,\n",
            "       [399, 376, 370, ..., 185, 171, 107],\n",
            "       [161, 160, 159, ..., 112, 111, 101],\n",
            "       [ 34,  31,  30, ...,   8,   4,   2]]), indices=array([[ 1,  8, 11, ..., 14, 19, 17],\n",
            "       [ 9, 11,  5, ..., 19,  7, 16],\n",
            "       [19, 16, 17, ..., 15, 12, 14],\n",
            "       ...,\n",
            "       [14, 17, 10, ..., 18, 13, 19],\n",
            "       [16,  7,  2, ...,  8, 10,  9],\n",
            "       [12,  3, 11, ...,  0,  2,  9]], dtype=int32))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Is-KTAENfB6C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f3531ed-aae7-4db1-a803-4d0c3659d176"
      },
      "source": [
        "import tensorflow as tf\n",
        "y_true = np.array(labels)\n",
        "y_true = tf.identity(y_true)\n",
        "y_pred = np.array(final_predictions)\n",
        "y_pred = tf.identity(y_pred)\n",
        "print(y_pred.shape,y_true.shape)\n",
        "k = 20\n",
        "recall, update_recall = tf.compat.v1.metrics.recall_at_top_k(y_true, y_pred, k=20)\n",
        "precision, update_precision = tf.compat.v1.metrics.precision_at_top_k(y_true, y_pred, k=20)\n",
        "\n",
        "tmp_rank = tf.nn.top_k(y_pred, 20)\n",
        "stream_vars = [i for i in tf.local_variables()]\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.local_variables_initializer())\n",
        "    # print(\"precision\",sess.run(update_precision))\n",
        "    # print(\"precision\",s|ess.run(precision))\n",
        "\n",
        "    print(\"update_recall: \",sess.run(update_recall ))\n",
        "    print(\"recall\",sess.run(recall))\n",
        "\n",
        "    print(\"STREAM_VARS: \",(sess.run(stream_vars)))\n",
        "    print(\"TMP_RANK: \",sess.run(tmp_rank))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4784, 20) (4784,)\n",
            "update_recall:  0.9487876254180602\n",
            "recall 0.9487876254180602\n",
            "STREAM_VARS:  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4539.0, 245.0, 0.0, 0.0]\n",
            "TMP_RANK:  TopKV2(values=array([[236, 133, 100, ...,  30,   9,   0],\n",
            "       [244, 243, 240, ..., 187, 183, 148],\n",
            "       [293, 247, 246, ..., 131, 116, 104],\n",
            "       ...,\n",
            "       [163, 114,  66, ...,  32,  30,  29],\n",
            "       [175, 174, 162, ...,   8,   6,   1],\n",
            "       [310, 273, 271, ...,  28,   5,   3]]), indices=array([[12, 19,  4, ..., 15,  5, 16],\n",
            "       [19,  7,  3, ..., 13, 10, 17],\n",
            "       [12,  3,  5, ..., 14,  0, 17],\n",
            "       ...,\n",
            "       [14, 11, 19, ...,  1, 17,  8],\n",
            "       [ 4, 17, 18, ...,  0, 11, 19],\n",
            "       [11, 10,  7, ..., 18, 15,  2]], dtype=int32))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jL6yAM4URvgw"
      },
      "source": [
        "data = pd.read_csv(\"what_you_learnt_lo_labelled.csv\",delimiter=\"|\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "tVXu1LFyX3Do",
        "outputId": "a9d9a486-18e8-43f0-d308-88011803aa34"
      },
      "source": [
        "data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>learning_objectives</th>\n",
              "      <th>taxonomy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A complete chemical equation represents the re...</td>\n",
              "      <td>science&gt;&gt;chemical reactions and equations</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A chemical equation is balanced so that the nu...</td>\n",
              "      <td>science&gt;&gt;chemical reactions and equations</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>In a combination reaction two or more substanc...</td>\n",
              "      <td>science&gt;&gt;chemical reactions and equations</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Decomposition reactions are opposite to combin...</td>\n",
              "      <td>science&gt;&gt;chemical reactions and equations</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Reactions in which heat is given out along wit...</td>\n",
              "      <td>science&gt;&gt;chemical reactions and equations</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>412</th>\n",
              "      <td>Pollutants are the substances which contaminat...</td>\n",
              "      <td>science&gt;&gt;pollution of air and water</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>413</th>\n",
              "      <td>Carbon monoxide nitrogen oxides carbon dioxide...</td>\n",
              "      <td>science&gt;&gt;pollution of air and water</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>414</th>\n",
              "      <td>Increasing levels of greenhouse gases like CO ...</td>\n",
              "      <td>science&gt;&gt;pollution of air and water</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>415</th>\n",
              "      <td>Water pollution is the contamination of water ...</td>\n",
              "      <td>science&gt;&gt;pollution of air and water</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>416</th>\n",
              "      <td>Water which is purified and fit for drinking i...</td>\n",
              "      <td>science&gt;&gt;pollution of air and water</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>417 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                   learning_objectives                                   taxonomy\n",
              "0    A complete chemical equation represents the re...  science>>chemical reactions and equations\n",
              "1    A chemical equation is balanced so that the nu...  science>>chemical reactions and equations\n",
              "2    In a combination reaction two or more substanc...  science>>chemical reactions and equations\n",
              "3    Decomposition reactions are opposite to combin...  science>>chemical reactions and equations\n",
              "4    Reactions in which heat is given out along wit...  science>>chemical reactions and equations\n",
              "..                                                 ...                                        ...\n",
              "412  Pollutants are the substances which contaminat...        science>>pollution of air and water\n",
              "413  Carbon monoxide nitrogen oxides carbon dioxide...        science>>pollution of air and water\n",
              "414  Increasing levels of greenhouse gases like CO ...        science>>pollution of air and water\n",
              "415  Water pollution is the contamination of water ...        science>>pollution of air and water\n",
              "416  Water which is purified and fit for drinking i...        science>>pollution of air and water\n",
              "\n",
              "[417 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kuRcg1QRvjz"
      },
      "source": [
        "lo_features =  data[\"learning_objectives\"].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQjD4cUbRvmy"
      },
      "source": [
        "test_input_ids = []\n",
        "test_attention_masks = []\n",
        "for sent in lo_features:\n",
        "\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 128,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        truncation=True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    test_input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    test_attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "test_input_ids = torch.cat(test_input_ids, dim=0)\n",
        "test_attention_masks = torch.cat(test_attention_masks, dim=0)\n",
        "# labels = torch.tensor(labels)\n",
        "\n",
        "# Set the batch size.  \n",
        "batch_size = 32  \n",
        "test_poincare_tensor = torch.tensor(taxonomy_vectors,dtype=torch.float)\n",
        "\n",
        "# Create the DataLoader.\n",
        "# prediction_data = TensorDataset(test_input_ids, test_attention_masks, test_poincare_tensor)\n",
        "# prediction_sampler = SequentialSampler(prediction_data)\n",
        "# prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfYU8RyuRvpf",
        "outputId": "2402ea3a-e9e8-495a-ed65-0498306b2c3b"
      },
      "source": [
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(test_input_ids)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "cos = torch.nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "\n",
        "input_ids = test_input_ids.to('cuda')\n",
        "attention_masks = test_attention_masks.to('cuda')\n",
        "test_poincare_tensor = test_poincare_tensor.to('cuda')\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "for input_id,attention_mask in zip(input_ids, attention_masks):\n",
        "  with torch.no_grad():\n",
        "    outputs = model(input_id.reshape(1,-1),attention_mask.reshape(1,-1))\n",
        "    \n",
        "  distances = cos(outputs,test_poincare_tensor)#torch.topk(cos(outputs,test_poincare_tensor),20,largest=True)\n",
        "  distances,indices = torch.topk(distances,5,largest=True)\n",
        "  predictions.append(test_labels[indices.cpu().numpy()])\n",
        "print(len(predictions))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting labels for 417 test sentences...\n",
            "417\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqDOGyveasex"
      },
      "source": [
        "labels=data['taxonomy'].apply(lambda x : x.strip()).values\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzLES9xGasfB",
        "outputId": "7e3a6a71-b228-4531-a23b-a8e2be0bed9a"
      },
      "source": [
        "from sklearn .preprocessing import LabelEncoder\n",
        "LE= LabelEncoder()\n",
        "all_labels = LE.fit_transform(list(set(train_labels.values)))\n",
        "labels = LE.transform(labels)\n",
        "labels"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 101,\n",
              "       101, 101, 101, 101, 101, 101, 101, 143, 143, 143, 143, 143, 143,\n",
              "       143, 143, 143, 143, 143, 143, 143, 143, 105, 105, 105, 105, 105,\n",
              "       105, 105, 105, 152, 152, 152, 152, 136, 136, 136, 136, 136, 136,\n",
              "       136, 136, 136, 136, 136, 114, 114, 114, 114, 114, 114, 114, 132,\n",
              "       132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132,\n",
              "       131, 131, 131, 131, 131, 131, 138, 138, 138, 138, 138, 138, 138,\n",
              "       138, 138, 138, 138, 138, 133, 133, 133, 133, 133, 133, 133, 118,\n",
              "       118, 118, 118, 118, 118, 118, 140, 140, 140, 140, 140, 140, 140,\n",
              "       140, 140, 140, 140, 140, 164, 151, 151, 151, 151, 151, 151, 167,\n",
              "       142, 142, 142, 142, 142, 142, 142, 142, 142, 142, 142, 142, 142,\n",
              "       135, 135, 135, 135, 135, 135, 135, 135, 135, 135, 103, 103, 103,\n",
              "       103, 103, 103, 103, 103, 103, 166, 166, 166, 166, 166, 166, 166,\n",
              "       169, 169, 169, 169, 169, 169, 169, 169, 169, 169, 169, 171, 171,\n",
              "       171, 171, 171, 171, 171, 171, 171, 116, 116, 116, 116, 116, 116,\n",
              "       145, 145, 145, 145, 145, 122, 122, 122, 122, 122, 122, 122, 122,\n",
              "       129, 129, 129, 129, 129, 129, 129, 129, 178, 178, 178, 178, 178,\n",
              "       178, 163, 163, 163, 163, 163, 163, 163, 163, 163, 163, 163, 163,\n",
              "       163, 177, 177, 177, 177, 177, 177, 177, 177, 177, 148, 148, 148,\n",
              "       148, 148, 148, 148, 134, 134, 134, 134, 134, 134, 134, 134, 134,\n",
              "       134, 134, 134, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115,\n",
              "       115, 115, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144,\n",
              "       144, 144, 168, 168, 168, 168, 168, 168, 168, 141, 141, 141, 141,\n",
              "       141, 141, 141, 141, 141, 110, 110, 110, 110, 110, 110, 111, 111,\n",
              "       111, 111, 111, 111, 111, 111, 111, 111, 111, 113, 113, 113, 113,\n",
              "       113, 106, 106, 106, 106, 106, 106, 106, 106, 156, 156, 156, 156,\n",
              "       156, 156, 156, 156, 156, 156, 156, 156, 156, 156, 156, 123, 123,\n",
              "       123, 123, 123, 123, 123, 123, 123, 125, 125, 125, 125, 125, 125,\n",
              "       125, 125, 125, 125, 125, 125, 125, 163, 163, 163, 163, 163, 163,\n",
              "       163, 163, 163, 108, 108, 108, 108, 161, 161, 161, 161, 161, 161,\n",
              "       161, 161, 137, 137, 137, 137, 137, 137, 137, 137, 165, 165, 165,\n",
              "       165, 165, 165, 165, 165, 165, 165, 165, 154, 154, 154, 154, 154,\n",
              "       154])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1YuRgmTasfC",
        "outputId": "750c79de-73a7-4f0f-d0d9-252e0bbdd1b3"
      },
      "source": [
        "len(labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "417"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJEfI0ZsasfD"
      },
      "source": [
        "final_predictions = []\n",
        "for prediction in predictions:\n",
        "    final_predictions.append(LE.transform(prediction))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7PXd3R-asfD",
        "outputId": "9af16358-4794-49a3-a922-667a7004c463"
      },
      "source": [
        "len(final_predictions[-4])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_uhiSt7SasfE",
        "outputId": "ae654493-21b0-4c0c-9906-c9104dd65bbe"
      },
      "source": [
        "import tensorflow as tf\n",
        "y_true = np.array(labels)\n",
        "y_true = tf.identity(y_true)\n",
        "y_pred = np.array(final_predictions)\n",
        "y_pred = tf.identity(y_pred)\n",
        "print(y_pred.shape,y_true.shape)\n",
        "k = 1\n",
        "recall, update_recall = tf.compat.v1.metrics.recall_at_top_k(y_true, y_pred, k=1)\n",
        "precision, update_precision = tf.compat.v1.metrics.precision_at_top_k(y_true, y_pred, k=1)\n",
        "\n",
        "tmp_rank = tf.nn.top_k(y_pred, 1)\n",
        "stream_vars = [i for i in tf.local_variables()]\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.local_variables_initializer())\n",
        "    # print(\"precision\",sess.run(update_precision))\n",
        "    # print(\"precision\",s|ess.run(precision))\n",
        "\n",
        "    print(\"update_recall: \",sess.run(update_recall ))\n",
        "    print(\"recall\",sess.run(recall))\n",
        "\n",
        "    print(\"STREAM_VARS: \",(sess.run(stream_vars)))\n",
        "    print(\"TMP_RANK: \",sess.run(tmp_rank))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(417, 1) (417,)\n",
            "update_recall:  0.6906474820143885\n",
            "recall 0.6906474820143885\n",
            "STREAM_VARS:  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 288.0, 129.0, 0.0, 0.0]\n",
            "TMP_RANK:  TopKV2(values=array([[109],\n",
            "       [109],\n",
            "       [109],\n",
            "       [109],\n",
            "       [109],\n",
            "       [109],\n",
            "       [109],\n",
            "       [ 18],\n",
            "       [109],\n",
            "       [109],\n",
            "       [109],\n",
            "       [109],\n",
            "       [101],\n",
            "       [101],\n",
            "       [101],\n",
            "       [108],\n",
            "       [101],\n",
            "       [101],\n",
            "       [135],\n",
            "       [101],\n",
            "       [152],\n",
            "       [141],\n",
            "       [141],\n",
            "       [101],\n",
            "       [141],\n",
            "       [101],\n",
            "       [141],\n",
            "       [141],\n",
            "       [141],\n",
            "       [141],\n",
            "       [108],\n",
            "       [141],\n",
            "       [141],\n",
            "       [101],\n",
            "       [148],\n",
            "       [105],\n",
            "       [  1],\n",
            "       [105],\n",
            "       [ 17],\n",
            "       [110],\n",
            "       [105],\n",
            "       [ 20],\n",
            "       [152],\n",
            "       [152],\n",
            "       [152],\n",
            "       [152],\n",
            "       [136],\n",
            "       [172],\n",
            "       [149],\n",
            "       [149],\n",
            "       [149],\n",
            "       [136],\n",
            "       [158],\n",
            "       [136],\n",
            "       [171],\n",
            "       [172],\n",
            "       [158],\n",
            "       [114],\n",
            "       [114],\n",
            "       [114],\n",
            "       [114],\n",
            "       [114],\n",
            "       [114],\n",
            "       [114],\n",
            "       [132],\n",
            "       [132],\n",
            "       [156],\n",
            "       [132],\n",
            "       [132],\n",
            "       [157],\n",
            "       [132],\n",
            "       [132],\n",
            "       [132],\n",
            "       [157],\n",
            "       [155],\n",
            "       [132],\n",
            "       [132],\n",
            "       [156],\n",
            "       [131],\n",
            "       [131],\n",
            "       [179],\n",
            "       [131],\n",
            "       [131],\n",
            "       [131],\n",
            "       [137],\n",
            "       [139],\n",
            "       [138],\n",
            "       [138],\n",
            "       [138],\n",
            "       [138],\n",
            "       [138],\n",
            "       [137],\n",
            "       [133],\n",
            "       [138],\n",
            "       [138],\n",
            "       [138],\n",
            "       [137],\n",
            "       [137],\n",
            "       [133],\n",
            "       [138],\n",
            "       [138],\n",
            "       [133],\n",
            "       [133],\n",
            "       [117],\n",
            "       [118],\n",
            "       [118],\n",
            "       [118],\n",
            "       [118],\n",
            "       [118],\n",
            "       [118],\n",
            "       [126],\n",
            "       [140],\n",
            "       [140],\n",
            "       [140],\n",
            "       [140],\n",
            "       [140],\n",
            "       [ 56],\n",
            "       [140],\n",
            "       [140],\n",
            "       [140],\n",
            "       [140],\n",
            "       [117],\n",
            "       [164],\n",
            "       [148],\n",
            "       [167],\n",
            "       [151],\n",
            "       [154],\n",
            "       [151],\n",
            "       [127],\n",
            "       [232],\n",
            "       [142],\n",
            "       [142],\n",
            "       [142],\n",
            "       [ 65],\n",
            "       [142],\n",
            "       [142],\n",
            "       [  7],\n",
            "       [135],\n",
            "       [142],\n",
            "       [142],\n",
            "       [142],\n",
            "       [ 76],\n",
            "       [ 76],\n",
            "       [135],\n",
            "       [159],\n",
            "       [135],\n",
            "       [135],\n",
            "       [135],\n",
            "       [ 12],\n",
            "       [135],\n",
            "       [152],\n",
            "       [103],\n",
            "       [135],\n",
            "       [103],\n",
            "       [103],\n",
            "       [166],\n",
            "       [103],\n",
            "       [103],\n",
            "       [103],\n",
            "       [103],\n",
            "       [103],\n",
            "       [103],\n",
            "       [166],\n",
            "       [166],\n",
            "       [166],\n",
            "       [166],\n",
            "       [166],\n",
            "       [152],\n",
            "       [103],\n",
            "       [106],\n",
            "       [169],\n",
            "       [169],\n",
            "       [106],\n",
            "       [169],\n",
            "       [169],\n",
            "       [169],\n",
            "       [169],\n",
            "       [106],\n",
            "       [169],\n",
            "       [106],\n",
            "       [171],\n",
            "       [171],\n",
            "       [171],\n",
            "       [171],\n",
            "       [171],\n",
            "       [171],\n",
            "       [171],\n",
            "       [171],\n",
            "       [171],\n",
            "       [116],\n",
            "       [116],\n",
            "       [116],\n",
            "       [116],\n",
            "       [116],\n",
            "       [116],\n",
            "       [147],\n",
            "       [147],\n",
            "       [145],\n",
            "       [147],\n",
            "       [145],\n",
            "       [122],\n",
            "       [122],\n",
            "       [125],\n",
            "       [122],\n",
            "       [ 63],\n",
            "       [109],\n",
            "       [ 65],\n",
            "       [122],\n",
            "       [129],\n",
            "       [129],\n",
            "       [129],\n",
            "       [123],\n",
            "       [129],\n",
            "       [129],\n",
            "       [129],\n",
            "       [129],\n",
            "       [178],\n",
            "       [178],\n",
            "       [178],\n",
            "       [178],\n",
            "       [178],\n",
            "       [178],\n",
            "       [163],\n",
            "       [163],\n",
            "       [163],\n",
            "       [163],\n",
            "       [163],\n",
            "       [ 73],\n",
            "       [163],\n",
            "       [163],\n",
            "       [163],\n",
            "       [163],\n",
            "       [163],\n",
            "       [163],\n",
            "       [163],\n",
            "       [183],\n",
            "       [177],\n",
            "       [177],\n",
            "       [177],\n",
            "       [177],\n",
            "       [177],\n",
            "       [177],\n",
            "       [177],\n",
            "       [183],\n",
            "       [148],\n",
            "       [102],\n",
            "       [175],\n",
            "       [176],\n",
            "       [148],\n",
            "       [167],\n",
            "       [167],\n",
            "       [115],\n",
            "       [230],\n",
            "       [230],\n",
            "       [115],\n",
            "       [115],\n",
            "       [115],\n",
            "       [ 21],\n",
            "       [115],\n",
            "       [115],\n",
            "       [134],\n",
            "       [163],\n",
            "       [134],\n",
            "       [134],\n",
            "       [115],\n",
            "       [115],\n",
            "       [160],\n",
            "       [115],\n",
            "       [115],\n",
            "       [115],\n",
            "       [115],\n",
            "       [115],\n",
            "       [115],\n",
            "       [115],\n",
            "       [115],\n",
            "       [144],\n",
            "       [176],\n",
            "       [144],\n",
            "       [144],\n",
            "       [144],\n",
            "       [144],\n",
            "       [144],\n",
            "       [144],\n",
            "       [144],\n",
            "       [144],\n",
            "       [144],\n",
            "       [160],\n",
            "       [160],\n",
            "       [168],\n",
            "       [120],\n",
            "       [168],\n",
            "       [168],\n",
            "       [120],\n",
            "       [168],\n",
            "       [127],\n",
            "       [141],\n",
            "       [141],\n",
            "       [141],\n",
            "       [141],\n",
            "       [141],\n",
            "       [101],\n",
            "       [101],\n",
            "       [141],\n",
            "       [141],\n",
            "       [110],\n",
            "       [164],\n",
            "       [110],\n",
            "       [110],\n",
            "       [110],\n",
            "       [110],\n",
            "       [111],\n",
            "       [105],\n",
            "       [111],\n",
            "       [111],\n",
            "       [108],\n",
            "       [111],\n",
            "       [111],\n",
            "       [110],\n",
            "       [110],\n",
            "       [ 21],\n",
            "       [111],\n",
            "       [113],\n",
            "       [113],\n",
            "       [113],\n",
            "       [113],\n",
            "       [124],\n",
            "       [106],\n",
            "       [106],\n",
            "       [106],\n",
            "       [169],\n",
            "       [169],\n",
            "       [106],\n",
            "       [169],\n",
            "       [106],\n",
            "       [132],\n",
            "       [156],\n",
            "       [156],\n",
            "       [132],\n",
            "       [156],\n",
            "       [156],\n",
            "       [132],\n",
            "       [156],\n",
            "       [156],\n",
            "       [156],\n",
            "       [156],\n",
            "       [132],\n",
            "       [156],\n",
            "       [132],\n",
            "       [156],\n",
            "       [123],\n",
            "       [123],\n",
            "       [123],\n",
            "       [147],\n",
            "       [123],\n",
            "       [123],\n",
            "       [123],\n",
            "       [123],\n",
            "       [123],\n",
            "       [125],\n",
            "       [125],\n",
            "       [125],\n",
            "       [125],\n",
            "       [125],\n",
            "       [125],\n",
            "       [125],\n",
            "       [125],\n",
            "       [125],\n",
            "       [125],\n",
            "       [125],\n",
            "       [125],\n",
            "       [125],\n",
            "       [163],\n",
            "       [163],\n",
            "       [163],\n",
            "       [163],\n",
            "       [163],\n",
            "       [163],\n",
            "       [163],\n",
            "       [163],\n",
            "       [163],\n",
            "       [ 56],\n",
            "       [108],\n",
            "       [117],\n",
            "       [ 56],\n",
            "       [161],\n",
            "       [161],\n",
            "       [161],\n",
            "       [108],\n",
            "       [161],\n",
            "       [161],\n",
            "       [161],\n",
            "       [161],\n",
            "       [137],\n",
            "       [137],\n",
            "       [137],\n",
            "       [137],\n",
            "       [137],\n",
            "       [138],\n",
            "       [137],\n",
            "       [138],\n",
            "       [165],\n",
            "       [165],\n",
            "       [ 64],\n",
            "       [165],\n",
            "       [165],\n",
            "       [165],\n",
            "       [165],\n",
            "       [165],\n",
            "       [165],\n",
            "       [165],\n",
            "       [165],\n",
            "       [154],\n",
            "       [154],\n",
            "       [154],\n",
            "       [154],\n",
            "       [154],\n",
            "       [173]]), indices=array([[0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0]], dtype=int32))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ihuJsXeCdN_k",
        "outputId": "f634986a-3d12-448a-efd0-a9657b0cd892"
      },
      "source": [
        "import tensorflow as tf\n",
        "y_true = np.array(labels)\n",
        "y_true = tf.identity(y_true)\n",
        "y_pred = np.array(final_predictions)\n",
        "y_pred = tf.identity(y_pred)\n",
        "print(y_pred.shape,y_true.shape)\n",
        "k = 2\n",
        "recall, update_recall = tf.compat.v1.metrics.recall_at_top_k(y_true, y_pred, k=2)\n",
        "precision, update_precision = tf.compat.v1.metrics.precision_at_top_k(y_true, y_pred, k=2)\n",
        "\n",
        "tmp_rank = tf.nn.top_k(y_pred, 2)\n",
        "stream_vars = [i for i in tf.local_variables()]\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.local_variables_initializer())\n",
        "    # print(\"precision\",sess.run(update_precision))\n",
        "    # print(\"precision\",s|ess.run(precision))\n",
        "\n",
        "    print(\"update_recall: \",sess.run(update_recall ))\n",
        "    print(\"recall\",sess.run(recall))\n",
        "\n",
        "    print(\"STREAM_VARS: \",(sess.run(stream_vars)))\n",
        "    print(\"TMP_RANK: \",sess.run(tmp_rank))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(417, 2) (417,)\n",
            "update_recall:  0.8513189448441247\n",
            "recall 0.8513189448441247\n",
            "STREAM_VARS:  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 355.0, 62.0, 0.0, 0.0]\n",
            "TMP_RANK:  TopKV2(values=array([[109,   2],\n",
            "       [109,   2],\n",
            "       [153, 109],\n",
            "       [109,   2],\n",
            "       [111, 109],\n",
            "       [109,   2],\n",
            "       [153, 109],\n",
            "       [ 18,  17],\n",
            "       [109,   2],\n",
            "       [109,   2],\n",
            "       [109, 108],\n",
            "       [109, 108],\n",
            "       [101,  20],\n",
            "       [141, 101],\n",
            "       [141, 101],\n",
            "       [108,  56],\n",
            "       [143, 101],\n",
            "       [101,  17],\n",
            "       [159, 135],\n",
            "       [101,  20],\n",
            "       [152, 141],\n",
            "       [143, 141],\n",
            "       [143, 141],\n",
            "       [141, 101],\n",
            "       [143, 141],\n",
            "       [141, 101],\n",
            "       [143, 141],\n",
            "       [143, 141],\n",
            "       [143, 141],\n",
            "       [143, 141],\n",
            "       [153, 108],\n",
            "       [143, 141],\n",
            "       [143, 141],\n",
            "       [141, 101],\n",
            "       [148, 105],\n",
            "       [105,  13],\n",
            "       [105,   1],\n",
            "       [105,  25],\n",
            "       [105,  17],\n",
            "       [148, 110],\n",
            "       [110, 105],\n",
            "       [ 20,  19],\n",
            "       [166, 152],\n",
            "       [166, 152],\n",
            "       [152,   3],\n",
            "       [152,   3],\n",
            "       [136, 116],\n",
            "       [172, 158],\n",
            "       [150, 149],\n",
            "       [150, 149],\n",
            "       [150, 149],\n",
            "       [169, 136],\n",
            "       [172, 158],\n",
            "       [172, 136],\n",
            "       [171, 136],\n",
            "       [172, 158],\n",
            "       [158, 136],\n",
            "       [136, 114],\n",
            "       [136, 114],\n",
            "       [114, 106],\n",
            "       [171, 114],\n",
            "       [169, 114],\n",
            "       [155, 114],\n",
            "       [155, 114],\n",
            "       [156, 132],\n",
            "       [156, 132],\n",
            "       [156, 132],\n",
            "       [157, 132],\n",
            "       [157, 132],\n",
            "       [157, 128],\n",
            "       [156, 132],\n",
            "       [156, 132],\n",
            "       [132, 131],\n",
            "       [157, 132],\n",
            "       [155, 132],\n",
            "       [156, 132],\n",
            "       [156, 132],\n",
            "       [156, 132],\n",
            "       [132, 131],\n",
            "       [131, 116],\n",
            "       [273, 179],\n",
            "       [132, 131],\n",
            "       [132, 131],\n",
            "       [131, 116],\n",
            "       [137, 133],\n",
            "       [139, 138],\n",
            "       [138, 137],\n",
            "       [138, 137],\n",
            "       [138, 137],\n",
            "       [138, 137],\n",
            "       [138, 137],\n",
            "       [137, 133],\n",
            "       [137, 133],\n",
            "       [138,  74],\n",
            "       [138,  74],\n",
            "       [138,  74],\n",
            "       [137, 133],\n",
            "       [137, 133],\n",
            "       [138, 133],\n",
            "       [138, 137],\n",
            "       [138, 133],\n",
            "       [138, 133],\n",
            "       [139, 133],\n",
            "       [118, 117],\n",
            "       [118, 117],\n",
            "       [118, 117],\n",
            "       [119, 118],\n",
            "       [118, 117],\n",
            "       [118,  79],\n",
            "       [118,  79],\n",
            "       [140, 126],\n",
            "       [140, 126],\n",
            "       [140, 126],\n",
            "       [140,  85],\n",
            "       [140, 126],\n",
            "       [140, 126],\n",
            "       [117,  56],\n",
            "       [140, 126],\n",
            "       [140,  85],\n",
            "       [140,  85],\n",
            "       [140, 117],\n",
            "       [119, 117],\n",
            "       [178, 164],\n",
            "       [167, 148],\n",
            "       [167, 151],\n",
            "       [151, 124],\n",
            "       [154,  21],\n",
            "       [151, 127],\n",
            "       [167, 127],\n",
            "       [232, 167],\n",
            "       [162, 142],\n",
            "       [142, 102],\n",
            "       [142, 102],\n",
            "       [178,  65],\n",
            "       [142, 102],\n",
            "       [154, 142],\n",
            "       [159,   7],\n",
            "       [142, 135],\n",
            "       [142, 135],\n",
            "       [142, 135],\n",
            "       [142, 102],\n",
            "       [130,  76],\n",
            "       [111,  76],\n",
            "       [159, 135],\n",
            "       [159, 135],\n",
            "       [159, 135],\n",
            "       [135,   8],\n",
            "       [135,  12],\n",
            "       [135,  12],\n",
            "       [135,  12],\n",
            "       [152, 103],\n",
            "       [135, 103],\n",
            "       [159, 135],\n",
            "       [109, 103],\n",
            "       [152, 103],\n",
            "       [166, 103],\n",
            "       [166, 103],\n",
            "       [152, 103],\n",
            "       [166, 103],\n",
            "       [166, 103],\n",
            "       [166, 103],\n",
            "       [135, 103],\n",
            "       [166,  67],\n",
            "       [166, 103],\n",
            "       [166, 103],\n",
            "       [166, 103],\n",
            "       [166, 103],\n",
            "       [166, 152],\n",
            "       [103,  11],\n",
            "       [169, 106],\n",
            "       [169, 106],\n",
            "       [169, 106],\n",
            "       [169, 106],\n",
            "       [169, 106],\n",
            "       [169, 106],\n",
            "       [169, 106],\n",
            "       [169, 106],\n",
            "       [169, 106],\n",
            "       [169, 106],\n",
            "       [132, 106],\n",
            "       [171, 106],\n",
            "       [171, 106],\n",
            "       [171, 104],\n",
            "       [171, 104],\n",
            "       [171, 106],\n",
            "       [171, 106],\n",
            "       [171, 104],\n",
            "       [171, 104],\n",
            "       [171, 106],\n",
            "       [131, 116],\n",
            "       [131, 116],\n",
            "       [169, 116],\n",
            "       [170, 116],\n",
            "       [170, 116],\n",
            "       [170, 116],\n",
            "       [147, 145],\n",
            "       [147, 145],\n",
            "       [145,  61],\n",
            "       [147,  61],\n",
            "       [147, 145],\n",
            "       [129, 122],\n",
            "       [129, 122],\n",
            "       [125,  57],\n",
            "       [122,  57],\n",
            "       [ 63,  60],\n",
            "       [109, 107],\n",
            "       [ 65,  59],\n",
            "       [123, 122],\n",
            "       [129, 122],\n",
            "       [129, 123],\n",
            "       [129, 123],\n",
            "       [123, 122],\n",
            "       [129, 122],\n",
            "       [129, 122],\n",
            "       [129, 123],\n",
            "       [129,  58],\n",
            "       [178,  65],\n",
            "       [178,  65],\n",
            "       [178, 164],\n",
            "       [178,  65],\n",
            "       [178, 164],\n",
            "       [178, 164],\n",
            "       [163, 100],\n",
            "       [163, 100],\n",
            "       [171, 163],\n",
            "       [171, 163],\n",
            "       [171, 163],\n",
            "       [ 73,  58],\n",
            "       [163,  68],\n",
            "       [163,  77],\n",
            "       [163, 145],\n",
            "       [163, 100],\n",
            "       [163, 100],\n",
            "       [163, 100],\n",
            "       [163, 100],\n",
            "       [183, 177],\n",
            "       [183, 177],\n",
            "       [177, 144],\n",
            "       [183, 177],\n",
            "       [183, 177],\n",
            "       [183, 177],\n",
            "       [183, 177],\n",
            "       [183, 177],\n",
            "       [183, 177],\n",
            "       [178, 148],\n",
            "       [154, 102],\n",
            "       [175, 174],\n",
            "       [176, 161],\n",
            "       [167, 148],\n",
            "       [167, 148],\n",
            "       [167, 148],\n",
            "       [134, 115],\n",
            "       [230, 215],\n",
            "       [230, 215],\n",
            "       [134, 115],\n",
            "       [134, 115],\n",
            "       [134, 115],\n",
            "       [219,  21],\n",
            "       [134, 115],\n",
            "       [134, 115],\n",
            "       [134, 115],\n",
            "       [163, 148],\n",
            "       [134, 115],\n",
            "       [207, 134],\n",
            "       [128, 115],\n",
            "       [134, 115],\n",
            "       [160, 115],\n",
            "       [134, 115],\n",
            "       [134, 115],\n",
            "       [134, 115],\n",
            "       [167, 115],\n",
            "       [134, 115],\n",
            "       [160, 115],\n",
            "       [134, 115],\n",
            "       [134, 115],\n",
            "       [144, 113],\n",
            "       [176, 170],\n",
            "       [144, 113],\n",
            "       [144, 116],\n",
            "       [177, 144],\n",
            "       [177, 144],\n",
            "       [144, 113],\n",
            "       [151, 144],\n",
            "       [177, 144],\n",
            "       [177, 144],\n",
            "       [144, 128],\n",
            "       [160, 148],\n",
            "       [160, 148],\n",
            "       [168, 120],\n",
            "       [168, 120],\n",
            "       [168, 120],\n",
            "       [168, 120],\n",
            "       [168, 120],\n",
            "       [168, 127],\n",
            "       [168, 127],\n",
            "       [143, 141],\n",
            "       [143, 141],\n",
            "       [143, 141],\n",
            "       [143, 141],\n",
            "       [143, 141],\n",
            "       [141, 101],\n",
            "       [141, 101],\n",
            "       [143, 141],\n",
            "       [162, 141],\n",
            "       [164, 110],\n",
            "       [164, 148],\n",
            "       [164, 110],\n",
            "       [111, 110],\n",
            "       [110, 105],\n",
            "       [234, 110],\n",
            "       [153, 111],\n",
            "       [105, 102],\n",
            "       [111,  65],\n",
            "       [111, 110],\n",
            "       [119, 108],\n",
            "       [111, 110],\n",
            "       [111,  76],\n",
            "       [111, 110],\n",
            "       [164, 110],\n",
            "       [154,  21],\n",
            "       [111, 110],\n",
            "       [216, 113],\n",
            "       [124, 113],\n",
            "       [216, 113],\n",
            "       [216, 113],\n",
            "       [124, 113],\n",
            "       [169, 106],\n",
            "       [169, 106],\n",
            "       [169, 106],\n",
            "       [169, 106],\n",
            "       [169, 106],\n",
            "       [169, 106],\n",
            "       [169, 106],\n",
            "       [169, 106],\n",
            "       [156, 132],\n",
            "       [156, 132],\n",
            "       [156, 132],\n",
            "       [157, 132],\n",
            "       [156, 132],\n",
            "       [156, 132],\n",
            "       [156, 132],\n",
            "       [156, 132],\n",
            "       [156, 132],\n",
            "       [156, 132],\n",
            "       [156, 132],\n",
            "       [156, 132],\n",
            "       [156, 132],\n",
            "       [156, 132],\n",
            "       [156, 132],\n",
            "       [123,  57],\n",
            "       [123,  57],\n",
            "       [123,  57],\n",
            "       [147, 145],\n",
            "       [123, 122],\n",
            "       [123,  57],\n",
            "       [123,  57],\n",
            "       [123, 122],\n",
            "       [123, 102],\n",
            "       [125, 123],\n",
            "       [125, 123],\n",
            "       [125,  57],\n",
            "       [125, 123],\n",
            "       [125, 123],\n",
            "       [125, 123],\n",
            "       [125,  57],\n",
            "       [125, 123],\n",
            "       [125, 123],\n",
            "       [125, 123],\n",
            "       [125, 123],\n",
            "       [125,  57],\n",
            "       [125, 123],\n",
            "       [163, 100],\n",
            "       [171, 163],\n",
            "       [171, 163],\n",
            "       [171, 163],\n",
            "       [163, 145],\n",
            "       [163, 100],\n",
            "       [171, 163],\n",
            "       [163, 100],\n",
            "       [163, 148],\n",
            "       [108,  56],\n",
            "       [108,  56],\n",
            "       [117,  56],\n",
            "       [108,  56],\n",
            "       [161, 142],\n",
            "       [161, 142],\n",
            "       [161, 108],\n",
            "       [119, 108],\n",
            "       [161, 108],\n",
            "       [164, 161],\n",
            "       [161, 107],\n",
            "       [161, 107],\n",
            "       [139, 137],\n",
            "       [138, 137],\n",
            "       [139, 137],\n",
            "       [138, 137],\n",
            "       [138, 137],\n",
            "       [138, 137],\n",
            "       [139, 137],\n",
            "       [138, 133],\n",
            "       [242, 165],\n",
            "       [242, 165],\n",
            "       [165,  64],\n",
            "       [165, 133],\n",
            "       [242, 165],\n",
            "       [165, 133],\n",
            "       [242, 165],\n",
            "       [165, 129],\n",
            "       [242, 165],\n",
            "       [242, 165],\n",
            "       [165,  82],\n",
            "       [154,  21],\n",
            "       [173, 154],\n",
            "       [154, 151],\n",
            "       [154,  21],\n",
            "       [173, 154],\n",
            "       [173, 154]]), indices=array([[0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1]], dtype=int32))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "macOxKfyfdFa",
        "outputId": "3cb8768d-86f6-4c44-9283-39e0d2b05c5e"
      },
      "source": [
        "import tensorflow as tf\n",
        "y_true = np.array(labels)\n",
        "y_true = tf.identity(y_true)\n",
        "y_pred = np.array(final_predictions)\n",
        "y_pred = tf.identity(y_pred)\n",
        "print(y_pred.shape,y_true.shape)\n",
        "k = 3\n",
        "recall, update_recall = tf.compat.v1.metrics.recall_at_top_k(y_true, y_pred, k=3)\n",
        "precision, update_precision = tf.compat.v1.metrics.precision_at_top_k(y_true, y_pred, k=3)\n",
        "\n",
        "tmp_rank = tf.nn.top_k(y_pred, 3)\n",
        "stream_vars = [i for i in tf.local_variables()]\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.local_variables_initializer())\n",
        "    # print(\"precision\",sess.run(update_precision))\n",
        "    # print(\"precision\",s|ess.run(precision))\n",
        "\n",
        "    print(\"update_recall: \",sess.run(update_recall ))\n",
        "    print(\"recall\",sess.run(recall))\n",
        "\n",
        "    print(\"STREAM_VARS: \",(sess.run(stream_vars)))\n",
        "    print(\"TMP_RANK: \",sess.run(tmp_rank))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(417, 3) (417,)\n",
            "update_recall:  0.8992805755395683\n",
            "recall 0.8992805755395683\n",
            "STREAM_VARS:  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 375.0, 42.0, 0.0, 0.0]\n",
            "TMP_RANK:  TopKV2(values=array([[109,   3,   2],\n",
            "       [109,  11,   2],\n",
            "       [153, 109,   2],\n",
            "       ...,\n",
            "       [154, 111,  21],\n",
            "       [175, 173, 154],\n",
            "       [173, 167, 154]]), indices=array([[0, 2, 1],\n",
            "       [0, 2, 1],\n",
            "       [1, 0, 2],\n",
            "       ...,\n",
            "       [0, 2, 1],\n",
            "       [2, 1, 0],\n",
            "       [0, 2, 1]], dtype=int32))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wp5F3C6LRvtA",
        "outputId": "2e28d345-f304-4e43-f708-e928eed60913"
      },
      "source": [
        "import tensorflow as tf\n",
        "y_true = np.array(labels)\n",
        "y_true = tf.identity(y_true)\n",
        "y_pred = np.array(final_predictions)\n",
        "y_pred = tf.identity(y_pred)\n",
        "print(y_pred.shape,y_true.shape)\n",
        "k = 4\n",
        "recall, update_recall = tf.compat.v1.metrics.recall_at_top_k(y_true, y_pred, k=4)\n",
        "precision, update_precision = tf.compat.v1.metrics.precision_at_top_k(y_true, y_pred, k=4)\n",
        "\n",
        "tmp_rank = tf.nn.top_k(y_pred, 4)\n",
        "stream_vars = [i for i in tf.local_variables()]\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.local_variables_initializer())\n",
        "    # print(\"precision\",sess.run(update_precision))\n",
        "    # print(\"precision\",s|ess.run(precision))\n",
        "\n",
        "    print(\"update_recall: \",sess.run(update_recall ))\n",
        "    print(\"recall\",sess.run(recall))\n",
        "\n",
        "    print(\"STREAM_VARS: \",(sess.run(stream_vars)))\n",
        "    print(\"TMP_RANK: \",sess.run(tmp_rank))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(417, 4) (417,)\n",
            "update_recall:  0.9424460431654677\n",
            "recall 0.9424460431654677\n",
            "STREAM_VARS:  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 393.0, 24.0, 0.0, 0.0]\n",
            "TMP_RANK:  TopKV2(values=array([[109,   4,   3,   2],\n",
            "       [109,  11,   4,   2],\n",
            "       [153, 109,   4,   2],\n",
            "       ...,\n",
            "       [154, 151, 111,  21],\n",
            "       [175, 173, 167, 154],\n",
            "       [175, 173, 167, 154]]), indices=array([[0, 3, 2, 1],\n",
            "       [0, 2, 3, 1],\n",
            "       [1, 0, 3, 2],\n",
            "       ...,\n",
            "       [0, 3, 2, 1],\n",
            "       [2, 1, 3, 0],\n",
            "       [3, 0, 2, 1]], dtype=int32))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4EYtX2SRvyB",
        "outputId": "93d411b5-3719-4a0d-9680-7638248cf4d3"
      },
      "source": [
        "import tensorflow as tf\n",
        "y_true = np.array(labels)\n",
        "y_true = tf.identity(y_true)\n",
        "y_pred = np.array(final_predictions)\n",
        "y_pred = tf.identity(y_pred)\n",
        "print(y_pred.shape,y_true.shape)\n",
        "k = 5\n",
        "recall, update_recall = tf.compat.v1.metrics.recall_at_top_k(y_true, y_pred, k=5)\n",
        "precision, update_precision = tf.compat.v1.metrics.precision_at_top_k(y_true, y_pred, k=5)\n",
        "\n",
        "tmp_rank = tf.nn.top_k(y_pred, 4)\n",
        "stream_vars = [i for i in tf.local_variables()]\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.local_variables_initializer())\n",
        "    print(\"precision\",sess.run(update_precision))\n",
        "    print(\"precision\",sess.run(precision))\n",
        "\n",
        "    print(\"update_recall: \",sess.run(update_recall ))\n",
        "    print(\"recall\",sess.run(recall))\n",
        "\n",
        "    print(\"STREAM_VARS: \",(sess.run(stream_vars)))\n",
        "    print(\"TMP_RANK: \",sess.run(tmp_rank))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(417, 5) (417,)\n",
            "precision 0.1908872901678657\n",
            "precision 0.1908872901678657\n",
            "update_recall:  0.9544364508393285\n",
            "recall 0.9544364508393285\n",
            "STREAM_VARS:  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 398.0, 19.0, 398.0, 1687.0]\n",
            "TMP_RANK:  TopKV2(values=array([[109,  15,   4,   3],\n",
            "       [109,  11,   4,   2],\n",
            "       [153, 109,   4,   2],\n",
            "       ...,\n",
            "       [154, 151, 148, 111],\n",
            "       [175, 173, 167, 154],\n",
            "       [175, 174, 173, 167]]), indices=array([[0, 4, 3, 2],\n",
            "       [0, 2, 3, 1],\n",
            "       [1, 0, 3, 2],\n",
            "       ...,\n",
            "       [0, 3, 4, 2],\n",
            "       [2, 1, 3, 0],\n",
            "       [3, 4, 0, 2]], dtype=int32))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nb9D-RQMfpsW"
      },
      "source": [
        "### other baselines on unseen set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jo6uiquXftFL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogNFKF6sftJX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzo_t16LOYQg"
      },
      "source": [
        "data = pd.read_csv(\"science_learning_objectives.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "CCCOEZEzR1_h",
        "outputId": "b6deaa67-88b8-4ed4-e50c-4a96a00cd242"
      },
      "source": [
        "data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>learning_objectives</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A complete chemical equation represents the re...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A chemical equation is balanced so that the nu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>In a combination reaction two or more substanc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Decomposition reactions are opposite to combin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Reactions in which heat is given out along wit...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1316</th>\n",
              "      <td>Pollutants are the substances which contaminat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1317</th>\n",
              "      <td>Carbon monoxide nitrogen oxides carbon dioxide...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1318</th>\n",
              "      <td>Increasing levels of greenhouse gases like CO ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1319</th>\n",
              "      <td>Water pollution is the contamination of water ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1320</th>\n",
              "      <td>Water which is purified and fit for drinking i...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1321 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                    learning_objectives\n",
              "0     A complete chemical equation represents the re...\n",
              "1     A chemical equation is balanced so that the nu...\n",
              "2     In a combination reaction two or more substanc...\n",
              "3     Decomposition reactions are opposite to combin...\n",
              "4     Reactions in which heat is given out along wit...\n",
              "...                                                 ...\n",
              "1316  Pollutants are the substances which contaminat...\n",
              "1317  Carbon monoxide nitrogen oxides carbon dioxide...\n",
              "1318  Increasing levels of greenhouse gases like CO ...\n",
              "1319  Water pollution is the contamination of water ...\n",
              "1320  Water which is purified and fit for drinking i...\n",
              "\n",
              "[1321 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPwOVt2MXPsK"
      },
      "source": [
        "lo_features = data[\"learning_objectives\"].values\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGKvfo-jXVXR"
      },
      "source": [
        "test_input_ids = []\n",
        "test_attention_masks = []\n",
        "for sent in lo_features:\n",
        "\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 128,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        truncation=True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    test_input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    test_attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "test_input_ids = torch.cat(test_input_ids, dim=0)\n",
        "test_attention_masks = torch.cat(test_attention_masks, dim=0)\n",
        "# labels = torch.tensor(labels)\n",
        "\n",
        "# Set the batch size.  \n",
        "batch_size = 32  \n",
        "test_poincare_tensor = torch.tensor(taxonomy_vectors,dtype=torch.float)\n",
        "\n",
        "# Create the DataLoader.\n",
        "# prediction_data = TensorDataset(test_input_ids, test_attention_masks, test_poincare_tensor)\n",
        "# prediction_sampler = SequentialSampler(prediction_data)\n",
        "# prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qj7_UH1_W7yK",
        "outputId": "fdb52667-4e50-4710-d2fa-1823025a1fe2"
      },
      "source": [
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(test_input_ids)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "cos = torch.nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "\n",
        "input_ids = test_input_ids.to('cuda')\n",
        "attention_masks = test_attention_masks.to('cuda')\n",
        "test_poincare_tensor = test_poincare_tensor.to('cuda')\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "for input_id,attention_mask in zip(input_ids, attention_masks):\n",
        "  with torch.no_grad():\n",
        "    outputs = model(input_id.reshape(1,-1),attention_mask.reshape(1,-1))\n",
        "    \n",
        "  distances = cos(outputs,test_poincare_tensor)#torch.topk(cos(outputs,test_poincare_tensor),20,largest=True)\n",
        "  distances,indices = torch.topk(distances,5,largest=True)\n",
        "  predictions.append(test_labels[indices.cpu().numpy()])\n",
        "print(len(predictions))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting labels for 1,321 test sentences...\n",
            "1321\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_8oXT1VXcJe",
        "outputId": "b6ea92b4-81f1-47b3-d8ad-2462a6c7bc8f"
      },
      "source": [
        "predictions[1302]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['science>>light', 'science>>light, shadows and reflections',\n",
              "       'science>>light - reflection and refraction',\n",
              "       'science>>human eye and colourful world', 'science'], dtype='<U116')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "nSLzMSHfXc52",
        "outputId": "d278785b-9e78-4b0b-da5d-474a993d4388"
      },
      "source": [
        "lo_features[1303]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Splitting of light into its constituent colours is known as dispersion.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awniXXkBapJS"
      },
      "source": [
        "final_result = {'lo':[],'label1':[],'label2':[],'label3':[],'label4':[],'label5':[]}\n",
        "for lo,labels in zip(lo_features,predictions):\n",
        "  final_result['lo'].append(lo)\n",
        "  final_result['label1'].append(labels[0])\n",
        "  final_result['label2'].append(labels[1])\n",
        "  final_result['label3'].append(labels[2])\n",
        "  final_result['label4'].append(labels[3])\n",
        "  final_result['label5'].append(labels[4])\n",
        "final_result = pd.DataFrame(final_result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 738
        },
        "id": "CU8dkp8sbhB0",
        "outputId": "2c0d337c-6692-4cd3-e354-58ed99ff0461"
      },
      "source": [
        "final_result"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>lo</th>\n",
              "      <th>label1</th>\n",
              "      <th>label2</th>\n",
              "      <th>label3</th>\n",
              "      <th>label4</th>\n",
              "      <th>label5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A complete chemical equation represents the re...</td>\n",
              "      <td>science&gt;&gt;chemical reactions and equations</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;chemical kinetics</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;classification ...</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;coordination co...</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;thermodynamics</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A chemical equation is balanced so that the nu...</td>\n",
              "      <td>science&gt;&gt;chemical reactions and equations</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;chemical kinetics</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;structure of atom</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;coordination co...</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;chemical bondin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>In a combination reaction two or more substanc...</td>\n",
              "      <td>science&gt;&gt;chemical reactions and equations</td>\n",
              "      <td>science&gt;&gt;physical and chemical changes</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;chemical kinetics</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;coordination co...</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;chemical bondin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Decomposition reactions are opposite to combin...</td>\n",
              "      <td>science&gt;&gt;chemical reactions and equations</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;chemical kinetics</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;coordination co...</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;classification ...</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part ii&gt;&gt;organic chemis...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Reactions in which heat is given out along wit...</td>\n",
              "      <td>science&gt;&gt;chemical reactions and equations</td>\n",
              "      <td>science&gt;&gt;combustion and flame</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;thermodynamics</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;chemical kinetics</td>\n",
              "      <td>physics&gt;&gt;physics : part - ii&gt;&gt;thermal properti...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1316</th>\n",
              "      <td>Pollutants are the substances which contaminat...</td>\n",
              "      <td>science&gt;&gt;pollution of air and water</td>\n",
              "      <td>science&gt;&gt;wastewater story</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part ii&gt;&gt;environmental ...</td>\n",
              "      <td>science&gt;&gt;sustainable management of natural res...</td>\n",
              "      <td>science&gt;&gt;our environment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1317</th>\n",
              "      <td>Carbon monoxide nitrogen oxides carbon dioxide...</td>\n",
              "      <td>science&gt;&gt;pollution of air and water</td>\n",
              "      <td>science&gt;&gt;our environment</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part ii&gt;&gt;environmental ...</td>\n",
              "      <td>science&gt;&gt;natural resources</td>\n",
              "      <td>science&gt;&gt;garbage in, garbage out</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1318</th>\n",
              "      <td>Increasing levels of greenhouse gases like CO ...</td>\n",
              "      <td>science&gt;&gt;pollution of air and water</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part ii&gt;&gt;environmental ...</td>\n",
              "      <td>science&gt;&gt;combustion and flame</td>\n",
              "      <td>science&gt;&gt;our environment</td>\n",
              "      <td>science&gt;&gt;natural resources</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1319</th>\n",
              "      <td>Water pollution is the contamination of water ...</td>\n",
              "      <td>science&gt;&gt;pollution of air and water</td>\n",
              "      <td>science&gt;&gt;wastewater story</td>\n",
              "      <td>science&gt;&gt;water: a precious resource</td>\n",
              "      <td>science&gt;&gt;sustainable management of natural res...</td>\n",
              "      <td>political science&gt;&gt;political science : contemp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1320</th>\n",
              "      <td>Water which is purified and fit for drinking i...</td>\n",
              "      <td>science&gt;&gt;wastewater story</td>\n",
              "      <td>science&gt;&gt;pollution of air and water</td>\n",
              "      <td>science&gt;&gt;sustainable management of natural res...</td>\n",
              "      <td>science&gt;&gt;water: a precious resource</td>\n",
              "      <td>science&gt;&gt;water</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1321 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                     lo  ...                                             label5\n",
              "0     A complete chemical equation represents the re...  ...      chemistry>>chemistry : part i>>thermodynamics\n",
              "1     A chemical equation is balanced so that the nu...  ...  chemistry>>chemistry : part i>>chemical bondin...\n",
              "2     In a combination reaction two or more substanc...  ...  chemistry>>chemistry : part i>>chemical bondin...\n",
              "3     Decomposition reactions are opposite to combin...  ...  chemistry>>chemistry : part ii>>organic chemis...\n",
              "4     Reactions in which heat is given out along wit...  ...  physics>>physics : part - ii>>thermal properti...\n",
              "...                                                 ...  ...                                                ...\n",
              "1316  Pollutants are the substances which contaminat...  ...                           science>>our environment\n",
              "1317  Carbon monoxide nitrogen oxides carbon dioxide...  ...                   science>>garbage in, garbage out\n",
              "1318  Increasing levels of greenhouse gases like CO ...  ...                         science>>natural resources\n",
              "1319  Water pollution is the contamination of water ...  ...  political science>>political science : contemp...\n",
              "1320  Water which is purified and fit for drinking i...  ...                                     science>>water\n",
              "\n",
              "[1321 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9ONWDk3bitg"
      },
      "source": [
        "final_result.to_csv(\"lo_with_hierarchy_labeled.csv\",index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2ZqAEVBNbqdS",
        "outputId": "46d54c3c-21b4-4494-b504-e59e7e4cf41d"
      },
      "source": [
        "final_result.head(20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>lo</th>\n",
              "      <th>label1</th>\n",
              "      <th>label2</th>\n",
              "      <th>label3</th>\n",
              "      <th>label4</th>\n",
              "      <th>label5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A complete chemical equation represents the re...</td>\n",
              "      <td>science&gt;&gt;chemical reactions and equations</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;chemical kinetics</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;classification ...</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;coordination co...</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;thermodynamics</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A chemical equation is balanced so that the nu...</td>\n",
              "      <td>science&gt;&gt;chemical reactions and equations</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;chemical kinetics</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;structure of atom</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;coordination co...</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;chemical bondin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>In a combination reaction two or more substanc...</td>\n",
              "      <td>science&gt;&gt;chemical reactions and equations</td>\n",
              "      <td>science&gt;&gt;physical and chemical changes</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;chemical kinetics</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;coordination co...</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;chemical bondin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Decomposition reactions are opposite to combin...</td>\n",
              "      <td>science&gt;&gt;chemical reactions and equations</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;chemical kinetics</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;coordination co...</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;classification ...</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part ii&gt;&gt;organic chemis...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Reactions in which heat is given out along wit...</td>\n",
              "      <td>science&gt;&gt;chemical reactions and equations</td>\n",
              "      <td>science&gt;&gt;combustion and flame</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;thermodynamics</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;chemical kinetics</td>\n",
              "      <td>physics&gt;&gt;physics : part - ii&gt;&gt;thermal properti...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Reactions in which energy is absorbed are know...</td>\n",
              "      <td>science&gt;&gt;chemical reactions and equations</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;chemical kinetics</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;thermodynamics</td>\n",
              "      <td>science&gt;&gt;physical and chemical changes</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;states of matter</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>When an element displaces another element from...</td>\n",
              "      <td>science&gt;&gt;chemical reactions and equations</td>\n",
              "      <td>science&gt;&gt;physical and chemical changes</td>\n",
              "      <td>science&gt;&gt;periodic classification of elements</td>\n",
              "      <td>science&gt;&gt;metals and non-metal</td>\n",
              "      <td>science&gt;&gt;atoms and molecules</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Precipitation reactions produce insoluble salts.</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part ii&gt;&gt;amines</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part ii&gt;&gt;aldehydes, ket...</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part ii&gt;&gt;the p-block el...</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;solutions</td>\n",
              "      <td>chemistry</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Two different atoms or groups of atoms (ions) ...</td>\n",
              "      <td>science&gt;&gt;chemical reactions and equations</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;chemical kinetics</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;chemical bondin...</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;coordination co...</td>\n",
              "      <td>science&gt;&gt;physical and chemical changes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Reactions also involve the gain or loss of oxy...</td>\n",
              "      <td>science&gt;&gt;chemical reactions and equations</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;chemical kinetics</td>\n",
              "      <td>science&gt;&gt;physical and chemical changes</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;thermodynamics</td>\n",
              "      <td>science&gt;&gt;carbon and its compounds</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Oxidation is the gain of oxygen or loss of hyd...</td>\n",
              "      <td>science&gt;&gt;chemical reactions and equations</td>\n",
              "      <td>science&gt;&gt;chemical effects of electric current</td>\n",
              "      <td>science&gt;&gt;physical and chemical changes</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;thermodynamics</td>\n",
              "      <td>chemistry</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Reduction is the loss of oxygen or gain of hyd...</td>\n",
              "      <td>science&gt;&gt;chemical reactions and equations</td>\n",
              "      <td>science&gt;&gt;chemical effects of electric current</td>\n",
              "      <td>science&gt;&gt;carbon and its compounds</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part ii&gt;&gt;hydrogen</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part ii&gt;&gt;hydrocarbons</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Acid-base indicators are dyes or mixtures of d...</td>\n",
              "      <td>science&gt;&gt;acids, bases and salts</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part ii&gt;&gt;chemistry in e...</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part ii&gt;&gt;biomolecules</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part ii&gt;&gt;aldehydes, ket...</td>\n",
              "      <td>chemistry</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>When an acid reacts with a metal hydrogen gas ...</td>\n",
              "      <td>science&gt;&gt;acids, bases and salts</td>\n",
              "      <td>science&gt;&gt;materials : metals and non-metals</td>\n",
              "      <td>science&gt;&gt;metals and non-metal</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part ii&gt;&gt;amines</td>\n",
              "      <td>science&gt;&gt;chemical effects of electric current</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>When a base reacts with a metal along with the...</td>\n",
              "      <td>science&gt;&gt;acids, bases and salts</td>\n",
              "      <td>science&gt;&gt;materials : metals and non-metals</td>\n",
              "      <td>science&gt;&gt;metals and non-metal</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part ii&gt;&gt;the p-block el...</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part ii&gt;&gt;amines</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Acidic and basic solutions in water conduct el...</td>\n",
              "      <td>science&gt;&gt;chemical effects of electric current</td>\n",
              "      <td>physical science&gt;&gt;physical science (physics)&gt;&gt;...</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;electrochemistry</td>\n",
              "      <td>science&gt;&gt;acids, bases and salts</td>\n",
              "      <td>chemistry</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Mixing concentrated acids or bases with water ...</td>\n",
              "      <td>science&gt;&gt;acids, bases and salts</td>\n",
              "      <td>science&gt;&gt;metals and non-metal</td>\n",
              "      <td>science&gt;&gt;materials : metals and non-metals</td>\n",
              "      <td>science&gt;&gt;chemical effects of electric current</td>\n",
              "      <td>science&gt;&gt;separation of substances</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>Acids and bases neutralise each other to form ...</td>\n",
              "      <td>science&gt;&gt;acids, bases and salts</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part ii&gt;&gt;aldehydes, ket...</td>\n",
              "      <td>science&gt;&gt;carbon and its compounds</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part ii&gt;&gt;chemistry in e...</td>\n",
              "      <td>science&gt;&gt;why do we fall ill?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>Water of crystallisation is the fixed number o...</td>\n",
              "      <td>science&gt;&gt;is matter around us pure</td>\n",
              "      <td>science&gt;&gt;separation of substances</td>\n",
              "      <td>science&gt;&gt;physical and chemical changes</td>\n",
              "      <td>science&gt;&gt;matter in our surroundings</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;surface chemistry</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>Salts have various uses in everyday life and i...</td>\n",
              "      <td>science&gt;&gt;acids, bases and salts</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part ii&gt;&gt;chemistry in e...</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part ii&gt;&gt;the p-block el...</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;surface chemistry</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part ii&gt;&gt;the s-block el...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   lo  ...                                             label5\n",
              "0   A complete chemical equation represents the re...  ...      chemistry>>chemistry : part i>>thermodynamics\n",
              "1   A chemical equation is balanced so that the nu...  ...  chemistry>>chemistry : part i>>chemical bondin...\n",
              "2   In a combination reaction two or more substanc...  ...  chemistry>>chemistry : part i>>chemical bondin...\n",
              "3   Decomposition reactions are opposite to combin...  ...  chemistry>>chemistry : part ii>>organic chemis...\n",
              "4   Reactions in which heat is given out along wit...  ...  physics>>physics : part - ii>>thermal properti...\n",
              "5   Reactions in which energy is absorbed are know...  ...    chemistry>>chemistry : part i>>states of matter\n",
              "6   When an element displaces another element from...  ...                       science>>atoms and molecules\n",
              "7    Precipitation reactions produce insoluble salts.  ...                                          chemistry\n",
              "8   Two different atoms or groups of atoms (ions) ...  ...             science>>physical and chemical changes\n",
              "9   Reactions also involve the gain or loss of oxy...  ...                  science>>carbon and its compounds\n",
              "10  Oxidation is the gain of oxygen or loss of hyd...  ...                                          chemistry\n",
              "11  Reduction is the loss of oxygen or gain of hyd...  ...       chemistry>>chemistry : part ii>>hydrocarbons\n",
              "12  Acid-base indicators are dyes or mixtures of d...  ...                                          chemistry\n",
              "13  When an acid reacts with a metal hydrogen gas ...  ...      science>>chemical effects of electric current\n",
              "14  When a base reacts with a metal along with the...  ...             chemistry>>chemistry : part ii>>amines\n",
              "15  Acidic and basic solutions in water conduct el...  ...                                          chemistry\n",
              "16  Mixing concentrated acids or bases with water ...  ...                  science>>separation of substances\n",
              "17  Acids and bases neutralise each other to form ...  ...                       science>>why do we fall ill?\n",
              "18  Water of crystallisation is the fixed number o...  ...   chemistry>>chemistry : part i>>surface chemistry\n",
              "19  Salts have various uses in everyday life and i...  ...  chemistry>>chemistry : part ii>>the s-block el...\n",
              "\n",
              "[20 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U69eoLqfk4EO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}