{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ecml_pkdd_statistical_significance.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2f97e2f6ca9d4899aafabc33e8532757": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e93cd617776c40d9b090a803562bba2e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f947042a77f6448190b7912cdfc5c4cd",
              "IPY_MODEL_96c769b1867e4ed6a870dcea1c702939"
            ]
          }
        },
        "e93cd617776c40d9b090a803562bba2e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f947042a77f6448190b7912cdfc5c4cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_000fbe4318f443759c320396523ff53a",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 433,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 433,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ac794ce27ed64b3ab066359de42904f9"
          }
        },
        "96c769b1867e4ed6a870dcea1c702939": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0dce79f02e7549f9b63f380f21b26613",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 433/433 [08:01&lt;00:00, 1.11s/B]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3ca11b4639fe46a1913dec15dd3b5a91"
          }
        },
        "000fbe4318f443759c320396523ff53a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ac794ce27ed64b3ab066359de42904f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0dce79f02e7549f9b63f380f21b26613": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3ca11b4639fe46a1913dec15dd3b5a91": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0bc788dafb1b40939fd7f0ab0471975f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a6c28d44519d40c58b1688d417b43db6",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_7ad214226fde4f1da534ae1ea60370e1",
              "IPY_MODEL_a309538ebe8f41f88916efb18b40c4a9"
            ]
          }
        },
        "a6c28d44519d40c58b1688d417b43db6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7ad214226fde4f1da534ae1ea60370e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b31c466b077745fcaacbf39f9f18082a",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 440473133,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 440473133,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1e5f889ae68e413a93f7100bbfe7e4ea"
          }
        },
        "a309538ebe8f41f88916efb18b40c4a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a9a1a512ca0b46b99933265593657cdf",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 440M/440M [00:11&lt;00:00, 39.3MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ba1ba717d9614c5aab1bf789bc5551e1"
          }
        },
        "b31c466b077745fcaacbf39f9f18082a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1e5f889ae68e413a93f7100bbfe7e4ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a9a1a512ca0b46b99933265593657cdf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ba1ba717d9614c5aab1bf789bc5551e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "sR9av2JU3kf6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c65f5ec3-590d-4239-ce8b-b7befec574c0"
      },
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "import torch\n",
        "import logging\n",
        "logging.basicConfig(level=logging.ERROR)\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJCr2Bi89Zw5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10c01bfc-03b2-4808-e5fb-9a128ce17b09"
      },
      "source": [
        "!pip install tensorflow==1.13.1\n",
        "! pip install tensorflow-hub==0.7.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.13.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/29/6b4f1e02417c3a1ccc85380f093556ffd0b35dc354078074c5195c8447f2/tensorflow-1.13.1-cp37-cp37m-manylinux1_x86_64.whl (92.6MB)\n",
            "\u001b[K     |████████████████████████████████| 92.6MB 48kB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.1.0)\n",
            "Collecting keras-applications>=1.0.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (0.10.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (0.8.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (3.12.4)\n",
            "Collecting tensorflow-estimator<1.14.0rc0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/48/13f49fc3fa0fdf916aa1419013bb8f2ad09674c275b4046d5ee669a46873/tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 54.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.19.5)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.1.2)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (0.3.3)\n",
            "Collecting tensorboard<1.14.0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/39/bdd75b08a6fba41f098b6cb091b9e8c7a80e1b4d679a581a0ccd17b10373/tensorboard-1.13.1-py3-none-any.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 45.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (0.36.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.32.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.6->tensorflow==1.13.1) (2.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.6.1->tensorflow==1.13.1) (54.0.0)\n",
            "Collecting mock>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/5c/03/b7e605db4a57c0f6fba744b11ef3ddf4ddebcada35022927a2b5fc623fdf/mock-4.0.3-py3-none-any.whl\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.3.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.4.1)\n",
            "Installing collected packages: keras-applications, mock, tensorflow-estimator, tensorboard, tensorflow\n",
            "  Found existing installation: tensorflow-estimator 2.4.0\n",
            "    Uninstalling tensorflow-estimator-2.4.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
            "  Found existing installation: tensorboard 2.4.1\n",
            "    Uninstalling tensorboard-2.4.1:\n",
            "      Successfully uninstalled tensorboard-2.4.1\n",
            "  Found existing installation: tensorflow 2.4.1\n",
            "    Uninstalling tensorflow-2.4.1:\n",
            "      Successfully uninstalled tensorflow-2.4.1\n",
            "Successfully installed keras-applications-1.0.8 mock-4.0.3 tensorboard-1.13.1 tensorflow-1.13.1 tensorflow-estimator-1.13.0\n",
            "Collecting tensorflow-hub==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/00/0e/a91780d07592b1abf9c91344ce459472cc19db3b67fdf3a61dca6ebb2f5c/tensorflow_hub-0.7.0-py2.py3-none-any.whl (89kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 5.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-hub==0.7.0) (1.19.5)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-hub==0.7.0) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-hub==0.7.0) (3.12.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.4.0->tensorflow-hub==0.7.0) (54.0.0)\n",
            "Installing collected packages: tensorflow-hub\n",
            "  Found existing installation: tensorflow-hub 0.11.0\n",
            "    Uninstalling tensorflow-hub-0.11.0:\n",
            "      Successfully uninstalled tensorflow-hub-0.11.0\n",
            "Successfully installed tensorflow-hub-0.7.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "da-zaZJUGMFU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9543a97-bda2-480a-ad65-d08a791118c4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slZnjEXG5_-Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4d13f39-7d70-4344-a3f2-15a2b33a6dee"
      },
      "source": [
        "!pip install sentence-transformers==0.2.6.1\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentence-transformers==0.2.6.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b9/46/b7d6c37d92d1bd65319220beabe4df845434930e3f30e42d3cfaecb74dc4/sentence-transformers-0.2.6.1.tar.gz (55kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 5.3MB/s \n",
            "\u001b[?25hCollecting transformers>=2.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f9/54/5ca07ec9569d2f232f3166de5457b63943882f7950ddfcc887732fc7fb23/transformers-4.3.3-py3-none-any.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 10.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers==0.2.6.1) (4.41.1)\n",
            "Requirement already satisfied: torch>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers==0.2.6.1) (1.8.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers==0.2.6.1) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers==0.2.6.1) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers==0.2.6.1) (1.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers==0.2.6.1) (3.2.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers>=2.8.0->sentence-transformers==0.2.6.1) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers>=2.8.0->sentence-transformers==0.2.6.1) (3.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=2.8.0->sentence-transformers==0.2.6.1) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers>=2.8.0->sentence-transformers==0.2.6.1) (20.9)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 47.8MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/23/2ddc317b2121117bf34dd00f5b0de194158f2a44ee2bf5e47c7166878a97/tokenizers-0.10.1-cp37-cp37m-manylinux2010_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 51.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=2.8.0->sentence-transformers==0.2.6.1) (2019.12.20)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0.1->sentence-transformers==0.2.6.1) (3.7.4.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers==0.2.6.1) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers==0.2.6.1) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=2.8.0->sentence-transformers==0.2.6.1) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=2.8.0->sentence-transformers==0.2.6.1) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=2.8.0->sentence-transformers==0.2.6.1) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=2.8.0->sentence-transformers==0.2.6.1) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers>=2.8.0->sentence-transformers==0.2.6.1) (3.4.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers>=2.8.0->sentence-transformers==0.2.6.1) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=2.8.0->sentence-transformers==0.2.6.1) (7.1.2)\n",
            "Building wheels for collected packages: sentence-transformers, sacremoses\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-0.2.6.1-cp37-none-any.whl size=74031 sha256=a6010984ae2947379e43391d3094bebd54ce02194cc6dec45e10ce02cdccbeef\n",
            "  Stored in directory: /root/.cache/pip/wheels/d7/fa/17/2b081a8cd8b0a86753fb0e9826b3cc19f0207062c0b2da7008\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp37-none-any.whl size=893262 sha256=b9a12e2c99f3e830e4329a515190e792e7518d53dec73f605a195d8e4a5cafb4\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sentence-transformers sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, transformers, sentence-transformers\n",
            "Successfully installed sacremoses-0.0.43 sentence-transformers-0.2.6.1 tokenizers-0.10.1 transformers-4.3.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzKeqoCs3kgA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec7b5a53-c122-4cec-ce3c-d47ce8d4d6dd"
      },
      "source": [
        "!pip install transformers==2.8.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers==2.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/78/92cedda05552398352ed9784908b834ee32a0bd071a9b32de287327370b7/transformers-2.8.0-py3-none-any.whl (563kB)\n",
            "\r\u001b[K     |▋                               | 10kB 20.9MB/s eta 0:00:01\r\u001b[K     |█▏                              | 20kB 13.6MB/s eta 0:00:01\r\u001b[K     |█▊                              | 30kB 12.8MB/s eta 0:00:01\r\u001b[K     |██▎                             | 40kB 12.1MB/s eta 0:00:01\r\u001b[K     |███                             | 51kB 8.3MB/s eta 0:00:01\r\u001b[K     |███▌                            | 61kB 8.7MB/s eta 0:00:01\r\u001b[K     |████                            | 71kB 8.7MB/s eta 0:00:01\r\u001b[K     |████▋                           | 81kB 9.5MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 92kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 102kB 7.9MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 112kB 7.9MB/s eta 0:00:01\r\u001b[K     |███████                         | 122kB 7.9MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 133kB 7.9MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 143kB 7.9MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 153kB 7.9MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 163kB 7.9MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 174kB 7.9MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 184kB 7.9MB/s eta 0:00:01\r\u001b[K     |███████████                     | 194kB 7.9MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 204kB 7.9MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 215kB 7.9MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 225kB 7.9MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 235kB 7.9MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 245kB 7.9MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 256kB 7.9MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 266kB 7.9MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 276kB 7.9MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 286kB 7.9MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 296kB 7.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 307kB 7.9MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 317kB 7.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 327kB 7.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 337kB 7.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 348kB 7.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 358kB 7.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 368kB 7.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 378kB 7.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 389kB 7.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 399kB 7.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 409kB 7.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 419kB 7.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 430kB 7.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 440kB 7.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 450kB 7.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 460kB 7.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 471kB 7.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 481kB 7.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 491kB 7.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 501kB 7.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 512kB 7.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 522kB 7.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 532kB 7.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 542kB 7.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 552kB 7.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 563kB 7.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 573kB 7.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (4.41.1)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 16.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (0.0.43)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (3.0.12)\n",
            "Collecting tokenizers==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d6/e3/5e49e9a83fb605aaa34a1c1173e607302fecae529428c28696fb18f1c2c9/tokenizers-0.5.2-cp37-cp37m-manylinux1_x86_64.whl (5.6MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6MB 14.8MB/s \n",
            "\u001b[?25hCollecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a2/2e/c867b1794dab90a7131d0d294cdad9c218aa9cadfac1a330106280458aaf/boto3-1.17.26-py2.py3-none-any.whl (130kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 52.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (2019.12.20)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.8.0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.8.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.8.0) (1.0.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.8.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.8.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.8.0) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.8.0) (2.10)\n",
            "Collecting botocore<1.21.0,>=1.20.26\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/f6/e3d30857321e90d5e91d67056c0a21ae8109813f4bc425204f1c5d28e457/botocore-1.20.26-py2.py3-none-any.whl (7.3MB)\n",
            "\u001b[K     |████████████████████████████████| 7.3MB 42.1MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Collecting s3transfer<0.4.0,>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/43/4b4a1b26eb03a429a4c37ca7fdf369d938bd60018fc194e94b8379b0c77c/s3transfer-0.3.4-py2.py3-none-any.whl (69kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 10.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.26->boto3->transformers==2.8.0) (2.8.1)\n",
            "\u001b[31mERROR: botocore 1.20.26 has requirement urllib3<1.27,>=1.25.4, but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: sentencepiece, tokenizers, jmespath, botocore, s3transfer, boto3, transformers\n",
            "  Found existing installation: tokenizers 0.10.1\n",
            "    Uninstalling tokenizers-0.10.1:\n",
            "      Successfully uninstalled tokenizers-0.10.1\n",
            "  Found existing installation: transformers 4.3.3\n",
            "    Uninstalling transformers-4.3.3:\n",
            "      Successfully uninstalled transformers-4.3.3\n",
            "Successfully installed boto3-1.17.26 botocore-1.20.26 jmespath-0.10.0 s3transfer-0.3.4 sentencepiece-0.1.95 tokenizers-0.5.2 transformers-2.8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZFv4UU8sLTM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8250e69-5644-429d-af5d-c0d9f6ae7526"
      },
      "source": [
        "!pip install git+https://github.com/geoopt/geoopt.git\n",
        "! pip install git+https://github.com/ferrine/hyrnn.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/geoopt/geoopt.git\n",
            "  Cloning https://github.com/geoopt/geoopt.git to /tmp/pip-req-build-suubo323\n",
            "  Running command git clone -q https://github.com/geoopt/geoopt.git /tmp/pip-req-build-suubo323\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from geoopt==0.3.1) (1.8.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from geoopt==0.3.1) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4.0->geoopt==0.3.1) (3.7.4.3)\n",
            "Building wheels for collected packages: geoopt\n",
            "  Building wheel for geoopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for geoopt: filename=geoopt-0.3.1-cp37-none-any.whl size=76168 sha256=ecc4369cc1cb1d28bdebd394f49d43d866096061c77e1f9f713a10ff2ff54676\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-sffrlkod/wheels/10/df/30/e0d857f034c142ca5f38af048b62aae3da773b272553e5dd21\n",
            "Successfully built geoopt\n",
            "Installing collected packages: geoopt\n",
            "Successfully installed geoopt-0.3.1\n",
            "Collecting git+https://github.com/ferrine/hyrnn.git\n",
            "  Cloning https://github.com/ferrine/hyrnn.git to /tmp/pip-req-build-wz50zz1c\n",
            "  Running command git clone -q https://github.com/ferrine/hyrnn.git /tmp/pip-req-build-wz50zz1c\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from hyrnn==0.0.0) (1.8.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from hyrnn==0.0.0) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0.0->hyrnn==0.0.0) (3.7.4.3)\n",
            "Building wheels for collected packages: hyrnn\n",
            "  Building wheel for hyrnn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hyrnn: filename=hyrnn-0.0.0-cp37-none-any.whl size=13955 sha256=679afeefaaa73aeaadf4bd6842ad65ef219b501d1d6374ecd2f96b0f4eb96d1e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-7gkhmcgk/wheels/24/c3/64/cc0e9d25d466081dc154a2a8843157f54d845b916b4ba66418\n",
            "Successfully built hyrnn\n",
            "Installing collected packages: hyrnn\n",
            "Successfully installed hyrnn-0.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsADhaO93kgD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "outputId": "eb277ff0-00a4-4ac3-8ca5-d07572f4b7ba"
      },
      "source": [
        "import pandas as pd\n",
        "train_data = pd.read_csv(\"train_taxonomy_prediction.csv\")\n",
        "val_data = pd.read_csv(\"validation_taxonomy_prediction.csv\")\n",
        "test_data = pd.read_csv(\"test_taxonomy_prediction.csv\")\n",
        "\n",
        "train_data\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>board_syllabus</th>\n",
              "      <th>question_answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>social science&gt;&gt;geography : our environment&gt;&gt;w...</td>\n",
              "      <td>Identify the different processes involved in ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>social science&gt;&gt;history : our pasts - iii&gt;&gt;wea...</td>\n",
              "      <td>The word &amp;#39;Chintz&amp;#39; comes from which la...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>social science&gt;&gt;civics : social and political ...</td>\n",
              "      <td>Which disease is responsible for the deaths o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>social science&gt;&gt;civics : social and political ...</td>\n",
              "      <td>Patients usually have to wait for hours in the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>science&gt;&gt;components of food</td>\n",
              "      <td>How can deficiency diseases be prevented? Def...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40890</th>\n",
              "      <td>science&gt;&gt;synthetic fibres and plastics</td>\n",
              "      <td>Name the monomer of polyester. Ester</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40891</th>\n",
              "      <td>physics&gt;&gt;physics : part - ii&gt;&gt;ray optics and o...</td>\n",
              "      <td>A convex lens and a concave lens, each having...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40892</th>\n",
              "      <td>social science&gt;&gt;history : india and the contem...</td>\n",
              "      <td>Peasants who opposed collectivisation and res...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40893</th>\n",
              "      <td>science&gt;&gt;garbage in, garbage out</td>\n",
              "      <td>Cancer is one of the fatal diseases causeddue...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40894</th>\n",
              "      <td>science&gt;&gt;some natural phenomena</td>\n",
              "      <td>Fill in the blanks : (a) The process of trans...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>40895 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          board_syllabus                                    question_answer\n",
              "0      social science>>geography : our environment>>w...   Identify the different processes involved in ...\n",
              "1      social science>>history : our pasts - iii>>wea...   The word &#39;Chintz&#39; comes from which la...\n",
              "2      social science>>civics : social and political ...   Which disease is responsible for the deaths o...\n",
              "3      social science>>civics : social and political ...  Patients usually have to wait for hours in the...\n",
              "4                            science>>components of food   How can deficiency diseases be prevented? Def...\n",
              "...                                                  ...                                                ...\n",
              "40890             science>>synthetic fibres and plastics               Name the monomer of polyester. Ester\n",
              "40891  physics>>physics : part - ii>>ray optics and o...   A convex lens and a concave lens, each having...\n",
              "40892  social science>>history : india and the contem...   Peasants who opposed collectivisation and res...\n",
              "40893                   science>>garbage in, garbage out   Cancer is one of the fatal diseases causeddue...\n",
              "40894                    science>>some natural phenomena   Fill in the blanks : (a) The process of trans...\n",
              "\n",
              "[40895 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGAnZbmnGrVI"
      },
      "source": [
        "!cp -r \"/content/drive/My Drive/research_lo_content_taxonomy_classification/model_euclidean_SENT_BERT_cos_1/\" /content/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bocSFvkT1pOG"
      },
      "source": [
        "!cp -r \"/content/drive/My Drive/research_lo_content_taxonomy_classification/model_save_categorized_reduced_freeze/\" /content/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oqlcl1XTmkz2"
      },
      "source": [
        "!cp -r \"/content/drive/My Drive/research_lo_content_taxonomy_classification/model_euclidean_USE_cos_final/\" /content/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AyvDLcAPmk2O"
      },
      "source": [
        "# for i in range(100000000000000000000000):\n",
        "#   j=i"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQhO6qqt6lge"
      },
      "source": [
        "from google.colab import files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIrS5sxE3kgk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8565582b-dadb-4ec4-ef46-9172bf877d56"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('model_euclidean_SENT_BERT_cos_1', do_lower_case=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading BERT tokenizer...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijn1nIpByb3e"
      },
      "source": [
        "train_features = train_data[\"question_answer\"]\n",
        "test_features = test_data[\"question_answer\"]\n",
        "train_labels = train_data[\"board_syllabus\"]\n",
        "test_labels = test_data[\"board_syllabus\"]\n",
        "val_features = val_data[\"question_answer\"]\n",
        "val_labels = val_data[\"board_syllabus\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkyM7gqv3khI"
      },
      "source": [
        "\n",
        "question_answer = train_features.values\n",
        "categories = train_labels.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FN1zRXMOXwLK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c20fac34-dbc4-4b50-c016-68be1a0a001e"
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "!pip install inflection\n",
        "\n",
        "from bokeh.io import output_file, output_notebook, show\n",
        "from bokeh.plotting import figure\n",
        "from bokeh.transform import linear_cmap\n",
        "from bokeh.util.hex import hexbin\n",
        "from bokeh.models import HoverTool\n",
        "from bokeh import colors\n",
        "import inflection\n",
        "\n",
        "from nltk.stem import PorterStemmer \n",
        "ps = PorterStemmer()\n",
        "from gzip import open as gopen\n",
        "from pandas.core.common import flatten\n",
        "import gensim.models.poincare as poincare\n",
        "def get_cleaned_taxonomy(taxonomy):\n",
        "  cleaned_taxonomy = []\n",
        "  for value in taxonomy:\n",
        "      value = ' '.join(value.split(\">>\"))\n",
        "      # taxonomy_words = [inflection.singularize(val)  for token in value for val in token.split(\" \") if val.isalpha()]\n",
        "      cleaned_taxonomy.append( value )\n",
        "  return cleaned_taxonomy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting inflection\n",
            "  Downloading https://files.pythonhosted.org/packages/59/91/aa6bde563e0085a02a435aa99b49ef75b0a4b062635e606dab23ce18d720/inflection-0.5.1-py2.py3-none-any.whl\n",
            "Installing collected packages: inflection\n",
            "Successfully installed inflection-0.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPAl0TNuX6mx"
      },
      "source": [
        "\n",
        "# course_taxonomy\n",
        "\n",
        "poincare_emb_data = get_cleaned_taxonomy(categories)\n",
        "poincare_val = get_cleaned_taxonomy(val_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aedZzkBsqEeK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3d3062a5-437a-4f0b-e875-1e9ed7102f8c"
      },
      "source": [
        "poincare_emb_data[2]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'social science civics : social and political life - ii role of the government in health'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZN6jdKp0uhO3"
      },
      "source": [
        "\n",
        "import sys\n",
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from matplotlib import pyplot as plt\n",
        "from torch.nn.modules.loss import HingeEmbeddingLoss\n",
        "from random import randint\n",
        "\n",
        "from tqdm import tqdm\n",
        "import geoopt\n",
        "import time\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from matplotlib import pyplot as plt\n",
        "from torch.nn.modules.loss import HingeEmbeddingLoss\n",
        "from random import randint\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import time\n",
        "import argparse\n",
        "cos = nn.CosineSimilarity(dim=0, eps=1e-6)\n",
        "# Neural Classifierwork\n",
        "class MulticlassClassifier(nn.Module):\n",
        "    def __init__(self,bert_model_path):\n",
        "        super(MulticlassClassifier,self).__init__()\n",
        "        self.bert = BertModel.from_pretrained(bert_model_path,output_hidden_states=False,output_attentions=False)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.fc1 = nn.Linear(768, 384)\n",
        "        self.fc2 = nn.Linear(384, 1024)\n",
        "\n",
        "    def forward(self,tokens,masks):\n",
        "        _, pooled_output = self.bert(tokens, attention_mask=masks)\n",
        "        x = self.fc1(pooled_output)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "class MyHingeLoss(torch.nn.Module):\n",
        "    def __init__(self, margin):\n",
        "        super(MyHingeLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "    # def forward_val(self, output, target):\n",
        "    #     cos = nn.CosineSimilarity(dim=0, eps=1e-6)\n",
        "    #     loss = 0\n",
        "    #     num_compare = 4\n",
        "    #     count = 0\n",
        "    #     for i in range(len(output)):\n",
        "    #         v_image = output[i]\n",
        "    #         t_label = target[i]\n",
        "    #         for j in range(num_compare):\n",
        "    #             if j != i:\n",
        "    #                 count += 1\n",
        "    #                 t_j = target[j]\n",
        "    #                 loss += torch.relu( self.margin - cos(t_label, v_image) + cos(t_j, v_image) )\n",
        "    #     return loss / count\n",
        "\n",
        "    def forward(self, output, target):\n",
        "        loss=0\n",
        "        for i in range(len(output)):\n",
        "            v_image = F.normalize(output[i],p=2,dim=0)\n",
        "            t_label = F.normalize(target[i],p=2,dim=0)\n",
        "            j = randint(0, len(output)-1)\n",
        "            while j == i:\n",
        "                j = randint(0, len(output)-1)\n",
        "            t_j = F.normalize(target[j],p=2,dim=0)\n",
        "            loss+= torch.relu( self.margin - cos(t_label, v_image) + cos(t_j, v_image) )\n",
        "        return loss / len(output)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1C2etny7les",
        "outputId": "db226557-2e7b-4f36-da6c-4e71ca1a3634"
      },
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "# Loads BertForSequenceClassification, the pretrained BERT model with a single \n",
        "model_multi_class = BertForSequenceClassification.from_pretrained(\n",
        "    \"model_save_categorized_reduced_nov\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = 312,   \n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = True, # Whether the model returns all hidden-states.\n",
        ")\n",
        "# model.load_state_dict(torch.load('model_save_categorized_reduced_nov/model_weights'))\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model_multi_class.cuda()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=312, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHYyjxMDIx2U"
      },
      "source": [
        "from transformers import BertModel, AdamW, BertConfig\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4178_yLFMWmx"
      },
      "source": [
        "test_features = test_features.values\n",
        "labels = test_labels.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZpmBJuIC2nM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5105bd5-185b-467b-e315-836670e7d93d"
      },
      "source": [
        "test_features"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([' (a) Describe a simple experiment (or activity) to show that the polarity of emf induced in a coil is always such that it tends to produce a current which opposes the change of magnetic flux that produces it. (b) The current flowing through an inductor of self inductance L is continuously increasing. Plot a graph showing the variation of (i) Magnetic flux versus the current (ii) Induced emf versus dI/dt (iii) Magnetic potential energy stored versus the current. /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-priority:99; mso-style-qformat:yes; mso-style-parent:\"\"; mso-padding-alt:0in 5.4pt 0in 5.4pt; mso-para-margin-top:0in; mso-para-margin-right:0in; mso-para-margin-bottom:10.0pt; mso-para-margin-left:0in; line-height:115%; mso-pagination:widow-orphan; font-size:11.0pt; font-family:\"Calibri\",\"sans-serif\"; mso-ascii-font-family:Calibri; mso-ascii-theme-font:minor-latin; mso-hansi-font-family:Calibri; mso-hansi-theme-font:minor-latin; mso-bidi-font-family:\"Times New Roman\"; mso-bidi-theme-font:minor-bidi;} v:* {behavior:url(#default#VML);} o:* {behavior:url(#default#VML);} w:* {behavior:url(#default#VML);} .shape {behavior:url(#default#VML);} (a) Lenz law: According to Lenz&#39;s law, the polarity of the induced emf is such that it opposes a change in magnetic flux responsible for its production. Activity: When the north pole of a bar magnet is pushed towards the coil, the amount of magnetic flux linked with the coil increases. Current is induced in the coil in such a direction that it opposes the increase in magnetic flux. This is possible only when the current induced in the coil is in anti-clockwise direction, with respect to an observer. The magnetic moment associated with this induced emf has north polarity, towards the north pole of the approaching bar magnet. Similarly, when the north pole of the bar magnet is moved away from the coil, the magnetic flux linked with the coil decreases. To counter this decrease in magnetic flux, current is induced in the coil in clockwise direction so that its south pole faces the receding north pole of the bar magnet. This would result in an attractive force which opposes the motion of the magnet and the corresponding decrease in magnetic flux. (b)',\n",
              "       ' What is the average weather in a place over many years called? Climate',\n",
              "       ' Which of the following is correct for the characteristics considered at the higher level of classification? dependent on the characteristic of the lower level',\n",
              "       ...,\n",
              "       ' Program formatting has more effect when a consistent style is followed.',\n",
              "       'Cryoscopic constant is related to depression in freezing point .',\n",
              "       ' The elements present in same period have same number of shells.'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0hxcNFx2w8r",
        "outputId": "910dd65f-10d9-4d50-9232-fe9af78a5ea3"
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "kf = KFold(n_splits=5, shuffle=True)\n",
        "kf.split(test_features)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<generator object _BaseKFold.split at 0x7fb90ed07f50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRZ54gFokNh9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf814a25-120d-4c33-a892-05b24df4b803"
      },
      "source": [
        "labels"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['physics>>physics : part - ii',\n",
              "       'social science>>geography : the earth our habitat>>india : climate, vegetation and wildlife',\n",
              "       'science>>diversity in living organisms', ...,\n",
              "       'computer science[c++]>>programming methodology',\n",
              "       'chemistry>>chemistry : part i>>solutions',\n",
              "       'science>>periodic classification of elements'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ohj1x7frQJ1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94f733d7-f4ad-421c-971f-8a12e1f89679"
      },
      "source": [
        "len(list(set(labels)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "312"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVWhJ4o0YxZ1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f33ed231-fc25-4836-c724-af8b9f74b4f2"
      },
      "source": [
        "len(list(set(train_data[\"board_syllabus\"].values)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "312"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kjinn0gXkNuP"
      },
      "source": [
        "\n",
        "# course_taxonomy\n",
        "test_labels = list(set(test_data[\"board_syllabus\"].values))\n",
        "emb_data_test = get_cleaned_taxonomy(test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dorQwznCeMpR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c54ca89-da72-4a66-c639-08509150390b"
      },
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "sent_model = SentenceTransformer('bert-large-nli-stsb-mean-tokens')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1.24G/1.24G [01:04<00:00, 19.3MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wl0RJ3SSW4i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa19aae5-f1c0-4abf-99cf-7346f4e8ef71"
      },
      "source": [
        "# taxonomy_vectors = []\"\"\n",
        "taxonomy_vectors = sent_model.encode(emb_data_test)\n",
        "taxonomy_vectors = np.vstack(taxonomy_vectors)\n",
        "taxonomy_vectors.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(312, 1024)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nso39n1N_po_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "2f97e2f6ca9d4899aafabc33e8532757",
            "e93cd617776c40d9b090a803562bba2e",
            "f947042a77f6448190b7912cdfc5c4cd",
            "96c769b1867e4ed6a870dcea1c702939",
            "000fbe4318f443759c320396523ff53a",
            "ac794ce27ed64b3ab066359de42904f9",
            "0dce79f02e7549f9b63f380f21b26613",
            "3ca11b4639fe46a1913dec15dd3b5a91",
            "0bc788dafb1b40939fd7f0ab0471975f",
            "a6c28d44519d40c58b1688d417b43db6",
            "7ad214226fde4f1da534ae1ea60370e1",
            "a309538ebe8f41f88916efb18b40c4a9",
            "b31c466b077745fcaacbf39f9f18082a",
            "1e5f889ae68e413a93f7100bbfe7e4ea",
            "a9a1a512ca0b46b99933265593657cdf",
            "ba1ba717d9614c5aab1bf789bc5551e1"
          ]
        },
        "outputId": "0db6c723-bafb-4570-e8aa-e9604b939bee"
      },
      "source": [
        "model = MulticlassClassifier('bert-base-uncased')\n",
        "model.load_state_dict(torch.load('model_euclidean_SENT_BERT_cos_1/model_weights'))\n",
        "model.cuda()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2f97e2f6ca9d4899aafabc33e8532757",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0bc788dafb1b40939fd7f0ab0471975f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MulticlassClassifier(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (fc1): Linear(in_features=768, out_features=384, bias=True)\n",
              "  (fc2): Linear(in_features=384, out_features=1024, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qe4qYkV2C4fX"
      },
      "source": [
        "input_ids = []\n",
        "attention_masks = []\n",
        "for sent in train_features:\n",
        "\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 128,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        truncation=True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "# labels = torch.tensor(labels)\n",
        "\n",
        "# Set the batch size.  \n",
        "batch_size = 32  \n",
        "# test_poincare_tensor = torch.tensor(taxonomy_vectors,dtype=torch.float)\n",
        "\n",
        "# Create the DataLoader.\n",
        "# prediction_data = TensorDataset(test_input_ids, test_attention_masks, test_poincare_tensor)\n",
        "# prediction_sampler = SequentialSampler(prediction_data)\n",
        "# prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlgiwrmLOwbO"
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "class_emb = {cls:[] for cls in list(set(train_data[\"board_syllabus\"].values))}\n",
        "for index,label in enumerate(list(set(train_data[\"board_syllabus\"].values))):\n",
        "  sample_indices_for_label = np.where(train_labels == label)[0][:3]\n",
        "  input_ids_for_class = input_ids[sample_indices_for_label]\n",
        "  attention_masks_class = attention_masks[sample_indices_for_label]\n",
        "  input_ids_for_class = input_ids_for_class.to(device)\n",
        "  attention_masks_class = attention_masks_class.to(device)\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    outputs = model_multi_class(input_ids_for_class,attention_masks_class)\n",
        "  # print(len(sample_indices_for_label),torch.mean(torch.cat((outputs[1][-2][:,0,:],outputs[1][-3][:,0,:],outputs[1][-4][:,0,:],outputs[1][-5][:,0,:]),dim=1),dim=0).shape,input_ids_for_class.shape)\n",
        "  class_emb[label] = torch.cat((outputs[1][-2][0][0],outputs[1][-3][0][0],outputs[1][-4][0][0],outputs[1][-5][0][0]),dim=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgjlpYeVPyeP",
        "outputId": "5307a8ca-cc00-4a18-9752-127ebf6293c5"
      },
      "source": [
        "class_values = [item[1] for item in class_emb.items()]\n",
        "class_prototype_embeddings = torch.stack(class_values,dim=0)\n",
        "class_prototype_embeddings.to('cuda')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.3494,  0.0839,  0.8508,  ..., -0.7199,  0.0337,  0.6656],\n",
              "        [-0.1233,  0.1221,  0.1806,  ..., -0.2270, -0.8302,  0.5190],\n",
              "        [-0.3744, -1.1714, -0.0588,  ..., -0.8741, -0.3291,  0.5664],\n",
              "        ...,\n",
              "        [ 0.2620, -0.3328, -0.1897,  ..., -0.4095, -0.1519,  0.1082],\n",
              "        [-0.0238,  0.8392,  0.3977,  ...,  0.0530, -0.3944,  0.6401],\n",
              "        [ 0.4721,  0.2779, -0.5011,  ..., -0.5818, -0.1891,  0.4532]],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcOhfGZUCyie"
      },
      "source": [
        "def get_recall_at_k(labels,predictions,indices):\n",
        "    labels=test_data['board_syllabus'].values\n",
        "    LE= LabelEncoder()\n",
        "    labels = LE.fit_transform(labels)\n",
        "    labels=labels[indices[0]]\n",
        "    final_predictions = []\n",
        "    for prediction in predictions:\n",
        "      final_predictions.append(LE.transform(prediction))\n",
        "    y_true = np.array(labels)\n",
        "    y_true = tf.identity(y_true)\n",
        "    y_pred = np.array(final_predictions)\n",
        "    y_pred = tf.identity(y_pred)\n",
        "    print(y_pred.shape,y_true.shape)\n",
        "    k = 20\n",
        "    recall, update_recall = tf.compat.v1.metrics.recall_at_top_k(y_true, y_pred, k=20)\n",
        "    precision, update_precision = tf.compat.v1.metrics.precision_at_top_k(y_true, y_pred, k=20)\n",
        "\n",
        "    tmp_rank = tf.nn.top_k(y_pred, 20)\n",
        "    stream_vars = [i for i in tf.local_variables()]\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(tf.local_variables_initializer())\n",
        "        # print(\"precision\",sess.run(update_precision))\n",
        "        # print(\"precision\",s|ess.run(precision))\n",
        "\n",
        "        # print(\"update_recall: \",sess.run(update_recall ))\n",
        "        # print(\"recall\",sess.run(recall))\n",
        "\n",
        "        # print(\"STREAM_VARS: \",(sess.run(stream_vars)))\n",
        "        # print(\"TMP_RANK: \",sess.run(tmp_rank))\n",
        "        return sess.run(update_recall )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JsyUOOgDiRI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBot2nm9DiRJ"
      },
      "source": [
        "from sklearn .preprocessing import LabelEncoder\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpxT6BPNDiRK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJneekKaCLJZ"
      },
      "source": [
        "def get_predictions(input_ids,attention_masks,test_poincare_tensor,indices,class_prototype_embeddings,class_keys):\n",
        "\n",
        "  print('Predicting labels for {:,} test sentences...'.format(len(test_input_ids)))\n",
        "\n",
        "  # Put model in evaluation mode\n",
        "  model.eval()\n",
        "  cos = torch.nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "\n",
        "  input_ids = input_ids.to('cuda')\n",
        "  attention_masks = attention_masks.to('cuda')\n",
        "  test_poincare_tensor = test_poincare_tensor.to('cuda')\n",
        "  # Tracking variables \n",
        "  predictions , true_labels = [], []\n",
        "  for input_id,attention_mask in zip(input_ids, attention_masks):\n",
        "    with torch.no_grad():\n",
        "      outputs = model(input_id.reshape(1,-1),attention_mask.reshape(1,-1))\n",
        "      \n",
        "    distances = cos(outputs,test_poincare_tensor)#torch.topk(cos(outputs,test_poincare_tensor),20,largest=True)\n",
        "    distances,index = torch.topk(distances,20,largest=True)\n",
        "    predictions.append(test_labels[index.cpu().numpy()])\n",
        "  model_multi_class.eval()\n",
        "  cos = torch.nn.CosineSimilarity(dim=-1, eps=1e-6)\n",
        "  class_prototype_embeddings = class_prototype_embeddings.to('cuda')\n",
        "    # Tracking variables1\n",
        "  multi_predictions , true_labels = [], []\n",
        "  for input_id,attention_mask in zip(input_ids, attention_masks):\n",
        "    with torch.no_grad():\n",
        "      outputs = model_multi_class(input_id.reshape(1,-1),attention_mask.reshape(1,-1))\n",
        "    # print(torch.mean(outputs[1][0].squeeze(),dim=0).shape)\n",
        "    distances = cos(torch.cat((outputs[1][-2][0][0], outputs[1][-3][0][0],outputs[1][-4][0][0],outputs[1][-5][0][0])),class_prototype_embeddings)\n",
        "    distances,indexes = torch.topk(distances,20,largest=True)\n",
        "    multi_predictions.append(class_keys[indexes.cpu().numpy()])\n",
        "\n",
        "\n",
        "  return get_recall_at_k(labels,predictions,indices),get_recall_at_k(labels,multi_predictions,indices)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgy31zb48_aU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c5b18bb-c1a7-4312-95f5-5ae9419dd5ad"
      },
      "source": [
        "r_k_sent_bert = []\n",
        "r_k_multi_class =[]\n",
        "for indices in kf.split(test_features):\n",
        "  test_input_ids = []\n",
        "  test_attention_masks = []\n",
        "  for sent in test_features[indices[0]]:\n",
        "\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 128,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        truncation=True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    test_input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    test_attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "\n",
        "  input_ids = torch.cat(test_input_ids, dim=0)\n",
        "  attention_masks = torch.cat(test_attention_masks, dim=0)\n",
        "  # labels = test_labels.values\n",
        "  # test_labels_tensor = torch.tensor(taxonomy_vectors)\n",
        "  test_poincare_tensor = torch.tensor(taxonomy_vectors,dtype=torch.float)\n",
        "  batch_size = 32  \n",
        "\n",
        "  # prediction_data = TensorDataset(input_ids, attention_masks, test_labels_tensor,test_skill_labels_tensor)\n",
        "  # prediction_sampler = SequentialSampler(prediction_data)\n",
        "  # prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n",
        "  import tensorflow as tf\n",
        "  class_keys = [item[0] for item in class_emb.items()]\n",
        "  class_keys = np.array(class_keys)\n",
        "  r_k_bert,r_k_multi = get_predictions(input_ids,attention_masks,test_poincare_tensor,indices,class_prototype_embeddings,class_keys)\n",
        "  print(\"r_k_bert\",r_k_bert)\n",
        "  print(\"r_k_multi\",r_k_multi)\n",
        "  r_k_sent_bert.append(r_k_bert)\n",
        "  r_k_multi_class.append(r_k_multi)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting labels for 3,827 test sentences...\n",
            "(3827, 20) (3827,)\n",
            "(3827, 20) (3827,)\n",
            "r_k_bert 0.966553436111837\n",
            "r_k_multi 0.947478442644369\n",
            "Predicting labels for 3,827 test sentences...\n",
            "(3827, 20) (3827,)\n",
            "(3827, 20) (3827,)\n",
            "r_k_bert 0.964724327149203\n",
            "r_k_multi 0.9493075516070029\n",
            "Predicting labels for 3,827 test sentences...\n",
            "(3827, 20) (3827,)\n",
            "(3827, 20) (3827,)\n",
            "r_k_bert 0.9636791220276979\n",
            "r_k_multi 0.951397961850013\n",
            "Predicting labels for 3,827 test sentences...\n",
            "(3827, 20) (3827,)\n",
            "(3827, 20) (3827,)\n",
            "r_k_bert 0.9644630258688267\n",
            "r_k_multi 0.9453880324013587\n",
            "Predicting labels for 3,828 test sentences...\n",
            "(3828, 20) (3828,)\n",
            "(3828, 20) (3828,)\n",
            "r_k_bert 0.9660397074190178\n",
            "r_k_multi 0.9503657262277951\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_oMb7gEOJL0",
        "outputId": "4b4fc6a9-ca03-42c3-9526-2b21377877c8"
      },
      "source": [
        "from scipy import stats\n",
        "stats.ttest_rel(r_k_multi_class,r_k_sent_bert)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Ttest_relResult(statistic=-12.746330812219284, pvalue=0.0002182713827503407)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BvtXkhdDdVPI",
        "outputId": "8a1d6b7e-08e2-47f7-cb52-83d31188ec81"
      },
      "source": [
        "r_k_multi_class"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.947478442644369,\n",
              " 0.9493075516070029,\n",
              " 0.951397961850013,\n",
              " 0.9453880324013587,\n",
              " 0.9503657262277951]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "veD3fO7ndTto",
        "outputId": "0d6d19e7-2d01-4c8c-c9a6-a0d879ad8a35"
      },
      "source": [
        "r_k_sent_bert"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.966553436111837,\n",
              " 0.964724327149203,\n",
              " 0.9636791220276979,\n",
              " 0.9644630258688267,\n",
              " 0.9660397074190178]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNdlve8AJcCO"
      },
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "test_poincare_tensor = torch.tensor(taxonomy_vectors,dtype=torch.float)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6aMBHkAQZjT"
      },
      "source": [
        "cos = torch.nn.CosineSimilarity(dim=0, eps=1e-6)\n",
        "def dist_without_grad( u, v):\n",
        "  sqdist = torch.sum((u - v) ** 2, dim=-1)\n",
        "  squnorm = torch.sum(u ** 2, dim=-1)\n",
        "  sqvnorm = torch.sum(v ** 2, dim=-1)\n",
        "  x = 1 + 2 * sqdist / ((1 - squnorm) * (1 - sqvnorm)) + 1e-7\n",
        "  z = torch.sqrt(x ** 2 - 1)\n",
        "  return torch.log(x + z)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oe0otXOPg7z0"
      },
      "source": [
        "test_labels = np.array(test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxxB6hEgXGsH",
        "outputId": "c7bf6951-be35-4e80-b266-4af7ea2067ca"
      },
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "test_input_ids = []\n",
        "test_attention_masks = []\n",
        "for sent in test_features:\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 128,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        truncation=True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    test_input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    test_attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "test_input_ids = torch.cat(test_input_ids, dim=0)\n",
        "test_attention_masks = torch.cat(test_attention_masks, dim=0)\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "# Set the batch size.  \n",
        "batch_size = 32  \n",
        "# test_poincare_tensor = torch.tensor(taxonomy_vectors,dtype=torch.float)\n",
        "\n",
        "# Create the DataLoader.\n",
        "# prediction_data = TensorDataset(test_input_ids, test_attention_masks, labels)\n",
        "# prediction_sampler = SequentialSampler(prediction_data)\n",
        "# prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFZJufpHW-9S",
        "outputId": "064ade68-45b6-429e-d627-72b93c3f7796"
      },
      "source": [
        "print('Predicting labels for {:,} test sentences...'.format(len(test_input_ids)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "cos = torch.nn.CosineSimilarity(dim=-1, eps=1e-6)\n",
        "\n",
        "test_input_ids = test_input_ids.to('cuda')\n",
        "test_attention_masks = test_attention_masks.to('cuda')\n",
        "class_prototype_embeddings = class_prototype_embeddings.to('cuda')\n",
        "# Tracking variables1\n",
        "predictions , true_labels = [], []\n",
        "for input_id,attention_mask in zip(test_input_ids, test_attention_masks):\n",
        "  with torch.no_grad():\n",
        "    outputs = model_multi_class(input_id.reshape(1,-1),attention_mask.reshape(1,-1))\n",
        "  # print(torch.mean(outputs[1][0].squeeze(),dim=0).shape)\n",
        "  distances = cos(torch.cat((outputs[1][-2][0][0], outputs[1][-3][0][0],outputs[1][-4][0][0],outputs[1][-5][0][0])),class_prototype_embeddings)\n",
        "  distances,indices = torch.topk(distances,20,largest=True)\n",
        "  predictions.append(class_keys[indices.cpu().numpy()])\n",
        "print(len(predictions))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting labels for 4,784 test sentences...\n",
            "4784\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPCktQT9DVT4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d3eddb5-0538-4bc9-9e0f-a172e21df58c"
      },
      "source": [
        "# Prediction on test set\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(test_input_ids)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "cos = torch.nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "\n",
        "input_ids = test_input_ids.to('cuda')\n",
        "attention_masks = test_attention_masks.to('cuda')\n",
        "test_poincare_tensor = test_poincare_tensor.to('cuda')\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "for input_id,attention_mask in zip(input_ids, attention_masks):\n",
        "  with torch.no_grad():\n",
        "    outputs = model(input_id.reshape(1,-1),attention_mask.reshape(1,-1))\n",
        "    \n",
        "  distances = cos(outputs,test_poincare_tensor)#torch.topk(cos(outputs,test_poincare_tensor),20,largest=True)\n",
        "  distances,indices = torch.topk(distances,20,largest=True)\n",
        "  predictions.append(test_labels[indices.cpu().numpy()])\n",
        "print(len(predictions))\n",
        "  # max_distance =100000000000000\n",
        "  # label=None\n",
        "  # for index,test_poincare in enumerate(test_poincare_tensor):\n",
        "\n",
        "  #   distance = distanceTo(test_poincare, outputs)\n",
        "  #   if distance < max_distance:\n",
        "  #     max_distance = distance\n",
        "  #     label = index\n",
        "  # predictions.append(labels[label])\n",
        "    \n",
        "# Predict \n",
        "# for batch in prediction_dataloader:\n",
        "#   # Add batch to GPU\n",
        "#   batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "#   # Unpack the inputs from our dataloader\n",
        "#   b_input_ids, b_input_mask, b_labels = batch\n",
        "  \n",
        "#   # Telling the model not to compute or store gradients, saving memory and \n",
        "#   # speeding up prediction\n",
        "#   with torch.no_grad():\n",
        "#       # Forward pass, calculate logit predictions\n",
        "#       outputs = model(b_input_ids,b_input_mask)\n",
        "\n",
        "#   logits = outputs\n",
        "#   for logit in logits:\n",
        "#     max_similarity = 0\n",
        "\n",
        "\n",
        "#   # Move logits and labels to CPU\n",
        "#   logits = logits.detach().cpu().numpy()\n",
        "#   label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "#   # Store predictions and true labels\n",
        "#   predictions.append(logits)\n",
        "#   true_labels.append(label_ids)\n",
        "\n",
        "print('    DONE.')\n",
        "# predictions"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting labels for 4,784 test sentences...\n",
            "4784\n",
            "    DONE.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5wj2gXYFkrQ"
      },
      "source": [
        "labels=test_data['board_syllabus'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOt8hvs-CZfn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a66adb9b-bb2f-4c95-943c-1d69fc692050"
      },
      "source": [
        "from sklearn .preprocessing import LabelEncoder\n",
        "LE= LabelEncoder()\n",
        "labels = LE.fit_transform(labels)\n",
        "labels"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 66, 237, 116, ...,  49,   8, 152])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adb4gTNgGKUP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31947ec1-c805-4481-ea4f-ab465727f1d2"
      },
      "source": [
        "labels"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 66, 237, 116, ...,  49,   8, 152])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azkCfK-bCoXd"
      },
      "source": [
        "final_predictions = []\n",
        "for prediction in predictions:\n",
        "  final_predictions.append(LE.transform(prediction))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQrlczKxMwzZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e51239b8-4230-4336-ef93-f951662d6aec"
      },
      "source": [
        "len(final_predictions[-1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUk41t_9H8K3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c9a8048-4179-4f07-b66a-614576b4a18a"
      },
      "source": [
        "import tensorflow as tf\n",
        "y_true = np.array(labels)\n",
        "y_true = tf.identity(y_true)\n",
        "y_pred = np.array(final_predictions)\n",
        "y_pred = tf.identity(y_pred)\n",
        "print(y_pred.shape,y_true.shape)\n",
        "k = 20\n",
        "recall, update_recall = tf.compat.v1.metrics.recall_at_top_k(y_true, y_pred, k=20)\n",
        "precision, update_precision = tf.compat.v1.metrics.precision_at_top_k(y_true, y_pred, k=20)\n",
        "\n",
        "tmp_rank = tf.nn.top_k(y_pred, 20)\n",
        "stream_vars = [i for i in tf.local_variables()]\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.local_variables_initializer())\n",
        "    # print(\"precision\",sess.run(update_precision))\n",
        "    # print(\"precision\",s|ess.run(precision))\n",
        "\n",
        "    print(\"update_recall: \",sess.run(update_recall ))\n",
        "    print(\"recall\",sess.run(recall))\n",
        "\n",
        "    print(\"STREAM_VARS: \",(sess.run(stream_vars)))\n",
        "    print(\"TMP_RANK: \",sess.run(tmp_rank))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4784, 20) (4784,)\n",
            "update_recall:  0.9650919732441472\n",
            "recall 0.9650919732441472\n",
            "STREAM_VARS:  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4617.0, 167.0, 0.0, 0.0]\n",
            "TMP_RANK:  TopKV2(values=array([[301, 166, 118, ...,  67,  66,   0],\n",
            "       [251, 248, 246, ..., 221, 176,  88],\n",
            "       [171, 170, 162, ..., 103, 102, 100],\n",
            "       ...,\n",
            "       [114,  55,  54, ...,  33,  32,  29],\n",
            "       [158, 109,  76, ...,   6,   2,   0],\n",
            "       [203, 179, 169, ...,  13,  11,   3]]), indices=array([[13, 19,  9, ...,  2,  0, 14],\n",
            "       [13, 14, 12, ...,  3,  1, 18],\n",
            "       [ 8, 17,  0, ...,  9,  7,  4],\n",
            "       ...,\n",
            "       [12,  5,  8, ..., 16, 10,  6],\n",
            "       [17, 12,  6, ...,  9,  2, 10],\n",
            "       [10,  4,  8, ..., 17, 16,  3]], dtype=int32))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Is-KTAENfB6C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f3531ed-aae7-4db1-a803-4d0c3659d176"
      },
      "source": [
        "import tensorflow as tf\n",
        "y_true = np.array(labels)\n",
        "y_true = tf.identity(y_true)\n",
        "y_pred = np.array(final_predictions)\n",
        "y_pred = tf.identity(y_pred)\n",
        "print(y_pred.shape,y_true.shape)\n",
        "k = 20\n",
        "recall, update_recall = tf.compat.v1.metrics.recall_at_top_k(y_true, y_pred, k=20)\n",
        "precision, update_precision = tf.compat.v1.metrics.precision_at_top_k(y_true, y_pred, k=20)\n",
        "\n",
        "tmp_rank = tf.nn.top_k(y_pred, 20)\n",
        "stream_vars = [i for i in tf.local_variables()]\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.local_variables_initializer())\n",
        "    # print(\"precision\",sess.run(update_precision))\n",
        "    # print(\"precision\",s|ess.run(precision))\n",
        "\n",
        "    print(\"update_recall: \",sess.run(update_recall ))\n",
        "    print(\"recall\",sess.run(recall))\n",
        "\n",
        "    print(\"STREAM_VARS: \",(sess.run(stream_vars)))\n",
        "    print(\"TMP_RANK: \",sess.run(tmp_rank))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4784, 20) (4784,)\n",
            "update_recall:  0.9487876254180602\n",
            "recall 0.9487876254180602\n",
            "STREAM_VARS:  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4539.0, 245.0, 0.0, 0.0]\n",
            "TMP_RANK:  TopKV2(values=array([[236, 133, 100, ...,  30,   9,   0],\n",
            "       [244, 243, 240, ..., 187, 183, 148],\n",
            "       [293, 247, 246, ..., 131, 116, 104],\n",
            "       ...,\n",
            "       [163, 114,  66, ...,  32,  30,  29],\n",
            "       [175, 174, 162, ...,   8,   6,   1],\n",
            "       [310, 273, 271, ...,  28,   5,   3]]), indices=array([[12, 19,  4, ..., 15,  5, 16],\n",
            "       [19,  7,  3, ..., 13, 10, 17],\n",
            "       [12,  3,  5, ..., 14,  0, 17],\n",
            "       ...,\n",
            "       [14, 11, 19, ...,  1, 17,  8],\n",
            "       [ 4, 17, 18, ...,  0, 11, 19],\n",
            "       [11, 10,  7, ..., 18, 15,  2]], dtype=int32))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jL6yAM4URvgw"
      },
      "source": [
        "data = pd.read_csv(\"what_you_learnt_lo_labelled.csv\",delimiter=\"|\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "tVXu1LFyX3Do",
        "outputId": "a9d9a486-18e8-43f0-d308-88011803aa34"
      },
      "source": [
        "data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>learning_objectives</th>\n",
              "      <th>taxonomy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A complete chemical equation represents the re...</td>\n",
              "      <td>science&gt;&gt;chemical reactions and equations</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A chemical equation is balanced so that the nu...</td>\n",
              "      <td>science&gt;&gt;chemical reactions and equations</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>In a combination reaction two or more substanc...</td>\n",
              "      <td>science&gt;&gt;chemical reactions and equations</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Decomposition reactions are opposite to combin...</td>\n",
              "      <td>science&gt;&gt;chemical reactions and equations</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Reactions in which heat is given out along wit...</td>\n",
              "      <td>science&gt;&gt;chemical reactions and equations</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>412</th>\n",
              "      <td>Pollutants are the substances which contaminat...</td>\n",
              "      <td>science&gt;&gt;pollution of air and water</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>413</th>\n",
              "      <td>Carbon monoxide nitrogen oxides carbon dioxide...</td>\n",
              "      <td>science&gt;&gt;pollution of air and water</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>414</th>\n",
              "      <td>Increasing levels of greenhouse gases like CO ...</td>\n",
              "      <td>science&gt;&gt;pollution of air and water</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>415</th>\n",
              "      <td>Water pollution is the contamination of water ...</td>\n",
              "      <td>science&gt;&gt;pollution of air and water</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>416</th>\n",
              "      <td>Water which is purified and fit for drinking i...</td>\n",
              "      <td>science&gt;&gt;pollution of air and water</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>417 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                   learning_objectives                                   taxonomy\n",
              "0    A complete chemical equation represents the re...  science>>chemical reactions and equations\n",
              "1    A chemical equation is balanced so that the nu...  science>>chemical reactions and equations\n",
              "2    In a combination reaction two or more substanc...  science>>chemical reactions and equations\n",
              "3    Decomposition reactions are opposite to combin...  science>>chemical reactions and equations\n",
              "4    Reactions in which heat is given out along wit...  science>>chemical reactions and equations\n",
              "..                                                 ...                                        ...\n",
              "412  Pollutants are the substances which contaminat...        science>>pollution of air and water\n",
              "413  Carbon monoxide nitrogen oxides carbon dioxide...        science>>pollution of air and water\n",
              "414  Increasing levels of greenhouse gases like CO ...        science>>pollution of air and water\n",
              "415  Water pollution is the contamination of water ...        science>>pollution of air and water\n",
              "416  Water which is purified and fit for drinking i...        science>>pollution of air and water\n",
              "\n",
              "[417 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kuRcg1QRvjz"
      },
      "source": [
        "lo_features =  data[\"learning_objectives\"].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQjD4cUbRvmy"
      },
      "source": [
        "test_input_ids = []\n",
        "test_attention_masks = []\n",
        "for sent in lo_features:\n",
        "\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 128,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        truncation=True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    test_input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    test_attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "test_input_ids = torch.cat(test_input_ids, dim=0)\n",
        "test_attention_masks = torch.cat(test_attention_masks, dim=0)\n",
        "# labels = torch.tensor(labels)\n",
        "\n",
        "# Set the batch size.  \n",
        "batch_size = 32  \n",
        "test_poincare_tensor = torch.tensor(taxonomy_vectors,dtype=torch.float)\n",
        "\n",
        "# Create the DataLoader.\n",
        "# prediction_data = TensorDataset(test_input_ids, test_attention_masks, test_poincare_tensor)\n",
        "# prediction_sampler = SequentialSampler(prediction_data)\n",
        "# prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfYU8RyuRvpf",
        "outputId": "2402ea3a-e9e8-495a-ed65-0498306b2c3b"
      },
      "source": [
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(test_input_ids)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "cos = torch.nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "\n",
        "input_ids = test_input_ids.to('cuda')\n",
        "attention_masks = test_attention_masks.to('cuda')\n",
        "test_poincare_tensor = test_poincare_tensor.to('cuda')\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "for input_id,attention_mask in zip(input_ids, attention_masks):\n",
        "  with torch.no_grad():\n",
        "    outputs = model(input_id.reshape(1,-1),attention_mask.reshape(1,-1))\n",
        "    \n",
        "  distances = cos(outputs,test_poincare_tensor)#torch.topk(cos(outputs,test_poincare_tensor),20,largest=True)\n",
        "  distances,indices = torch.topk(distances,5,largest=True)\n",
        "  predictions.append(test_labels[indices.cpu().numpy()])\n",
        "print(len(predictions))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting labels for 417 test sentences...\n",
            "417\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqDOGyveasex"
      },
      "source": [
        "labels=data['taxonomy'].apply(lambda x : x.strip()).values\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzLES9xGasfB",
        "outputId": "7e3a6a71-b228-4531-a23b-a8e2be0bed9a"
      },
      "source": [
        "from sklearn .preprocessing import LabelEncoder\n",
        "LE= LabelEncoder()\n",
        "all_labels = LE.fit_transform(list(set(train_labels.values)))\n",
        "labels = LE.transform(labels)\n",
        "labels"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 101,\n",
              "       101, 101, 101, 101, 101, 101, 101, 143, 143, 143, 143, 143, 143,\n",
              "       143, 143, 143, 143, 143, 143, 143, 143, 105, 105, 105, 105, 105,\n",
              "       105, 105, 105, 152, 152, 152, 152, 136, 136, 136, 136, 136, 136,\n",
              "       136, 136, 136, 136, 136, 114, 114, 114, 114, 114, 114, 114, 132,\n",
              "       132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132,\n",
              "       131, 131, 131, 131, 131, 131, 138, 138, 138, 138, 138, 138, 138,\n",
              "       138, 138, 138, 138, 138, 133, 133, 133, 133, 133, 133, 133, 118,\n",
              "       118, 118, 118, 118, 118, 118, 140, 140, 140, 140, 140, 140, 140,\n",
              "       140, 140, 140, 140, 140, 164, 151, 151, 151, 151, 151, 151, 167,\n",
              "       142, 142, 142, 142, 142, 142, 142, 142, 142, 142, 142, 142, 142,\n",
              "       135, 135, 135, 135, 135, 135, 135, 135, 135, 135, 103, 103, 103,\n",
              "       103, 103, 103, 103, 103, 103, 166, 166, 166, 166, 166, 166, 166,\n",
              "       169, 169, 169, 169, 169, 169, 169, 169, 169, 169, 169, 171, 171,\n",
              "       171, 171, 171, 171, 171, 171, 171, 116, 116, 116, 116, 116, 116,\n",
              "       145, 145, 145, 145, 145, 122, 122, 122, 122, 122, 122, 122, 122,\n",
              "       129, 129, 129, 129, 129, 129, 129, 129, 178, 178, 178, 178, 178,\n",
              "       178, 163, 163, 163, 163, 163, 163, 163, 163, 163, 163, 163, 163,\n",
              "       163, 177, 177, 177, 177, 177, 177, 177, 177, 177, 148, 148, 148,\n",
              "       148, 148, 148, 148, 134, 134, 134, 134, 134, 134, 134, 134, 134,\n",
              "       134, 134, 134, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115,\n",
              "       115, 115, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144,\n",
              "       144, 144, 168, 168, 168, 168, 168, 168, 168, 141, 141, 141, 141,\n",
              "       141, 141, 141, 141, 141, 110, 110, 110, 110, 110, 110, 111, 111,\n",
              "       111, 111, 111, 111, 111, 111, 111, 111, 111, 113, 113, 113, 113,\n",
              "       113, 106, 106, 106, 106, 106, 106, 106, 106, 156, 156, 156, 156,\n",
              "       156, 156, 156, 156, 156, 156, 156, 156, 156, 156, 156, 123, 123,\n",
              "       123, 123, 123, 123, 123, 123, 123, 125, 125, 125, 125, 125, 125,\n",
              "       125, 125, 125, 125, 125, 125, 125, 163, 163, 163, 163, 163, 163,\n",
              "       163, 163, 163, 108, 108, 108, 108, 161, 161, 161, 161, 161, 161,\n",
              "       161, 161, 137, 137, 137, 137, 137, 137, 137, 137, 165, 165, 165,\n",
              "       165, 165, 165, 165, 165, 165, 165, 165, 154, 154, 154, 154, 154,\n",
              "       154])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1YuRgmTasfC",
        "outputId": "750c79de-73a7-4f0f-d0d9-252e0bbdd1b3"
      },
      "source": [
        "len(labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "417"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJEfI0ZsasfD"
      },
      "source": [
        "final_predictions = []\n",
        "for prediction in predictions:\n",
        "    final_predictions.append(LE.transform(prediction))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7PXd3R-asfD",
        "outputId": "9af16358-4794-49a3-a922-667a7004c463"
      },
      "source": [
        "len(final_predictions[-4])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_uhiSt7SasfE",
        "outputId": "ae654493-21b0-4c0c-9906-c9104dd65bbe"
      },
      "source": [
        "import tensorflow as tf\n",
        "y_true = np.array(labels)\n",
        "y_true = tf.identity(y_true)\n",
        "y_pred = np.array(final_predictions)\n",
        "y_pred = tf.identity(y_pred)\n",
        "print(y_pred.shape,y_true.shape)\n",
        "k = 1\n",
        "recall, update_recall = tf.compat.v1.metrics.recall_at_top_k(y_true, y_pred, k=1)\n",
        "precision, update_precision = tf.compat.v1.metrics.precision_at_top_k(y_true, y_pred, k=1)\n",
        "\n",
        "tmp_rank = tf.nn.top_k(y_pred, 1)\n",
        "stream_vars = [i for i in tf.local_variables()]\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.local_variables_initializer())\n",
        "    # print(\"precision\",sess.run(update_precision))\n",
        "    # print(\"precision\",s|ess.run(precision))\n",
        "\n",
        "    print(\"update_recall: \",sess.run(update_recall ))\n",
        "    print(\"recall\",sess.run(recall))\n",
        "\n",
        "    print(\"STREAM_VARS: \",(sess.run(stream_vars)))\n",
        "    print(\"TMP_RANK: \",sess.run(tmp_rank))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(417, 1) (417,)\n",
            "update_recall:  0.6906474820143885\n",
            "recall 0.6906474820143885\n",
            "STREAM_VARS:  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 288.0, 129.0, 0.0, 0.0]\n",
            "TMP_RANK:  TopKV2(values=array([[109],\n",
            "       [109],\n",
            "       [109],\n",
            "       [109],\n",
            "       [109],\n",
            "       [109],\n",
            "       [109],\n",
            "       [ 18],\n",
            "       [109],\n",
            "       [109],\n",
            "       [109],\n",
            "       [109],\n",
            "       [101],\n",
            "       [101],\n",
            "       [101],\n",
            "       [108],\n",
            "       [101],\n",
            "       [101],\n",
            "       [135],\n",
            "       [101],\n",
            "       [152],\n",
            "       [141],\n",
            "       [141],\n",
            "       [101],\n",
            "       [141],\n",
            "       [101],\n",
            "       [141],\n",
            "       [141],\n",
            "       [141],\n",
            "       [141],\n",
            "       [108],\n",
            "       [141],\n",
            "       [141],\n",
            "       [101],\n",
            "       [148],\n",
            "       [105],\n",
            "       [  1],\n",
            "       [105],\n",
            "       [ 17],\n",
            "       [110],\n",
            "       [105],\n",
            "       [ 20],\n",
            "       [152],\n",
            "       [152],\n",
            "       [152],\n",
            "       [152],\n",
            "       [136],\n",
            "       [172],\n",
            "       [149],\n",
            "       [149],\n",
            "       [149],\n",
            "       [136],\n",
            "       [158],\n",
            "       [136],\n",
            "       [171],\n",
            "       [172],\n",
            "       [158],\n",
            "       [114],\n",
            "       [114],\n",
            "       [114],\n",
            "       [114],\n",
            "       [114],\n",
            "       [114],\n",
            "       [114],\n",
            "       [132],\n",
            "       [132],\n",
            "       [156],\n",
            "       [132],\n",
            "       [132],\n",
            "       [157],\n",
            "       [132],\n",
            "       [132],\n",
            "       [132],\n",
            "       [157],\n",
            "       [155],\n",
            "       [132],\n",
            "       [132],\n",
            "       [156],\n",
            "       [131],\n",
            "       [131],\n",
            "       [179],\n",
            "       [131],\n",
            "       [131],\n",
            "       [131],\n",
            "       [137],\n",
            "       [139],\n",
            "       [138],\n",
            "       [138],\n",
            "       [138],\n",
            "       [138],\n",
            "       [138],\n",
            "       [137],\n",
            "       [133],\n",
            "       [138],\n",
            "       [138],\n",
            "       [138],\n",
            "       [137],\n",
            "       [137],\n",
            "       [133],\n",
            "       [138],\n",
            "       [138],\n",
            "       [133],\n",
            "       [133],\n",
            "       [117],\n",
            "       [118],\n",
            "       [118],\n",
            "       [118],\n",
            "       [118],\n",
            "       [118],\n",
            "       [118],\n",
            "       [126],\n",
            "       [140],\n",
            "       [140],\n",
            "       [140],\n",
            "       [140],\n",
            "       [140],\n",
            "       [ 56],\n",
            "       [140],\n",
            "       [140],\n",
            "       [140],\n",
            "       [140],\n",
            "       [117],\n",
            "       [164],\n",
            "       [148],\n",
            "       [167],\n",
            "       [151],\n",
            "       [154],\n",
            "       [151],\n",
            "       [127],\n",
            "       [232],\n",
            "       [142],\n",
            "       [142],\n",
            "       [142],\n",
            "       [ 65],\n",
            "       [142],\n",
            "       [142],\n",
            "       [  7],\n",
            "       [135],\n",
            "       [142],\n",
            "       [142],\n",
            "       [142],\n",
            "       [ 76],\n",
            "       [ 76],\n",
            "       [135],\n",
            "       [159],\n",
            "       [135],\n",
            "       [135],\n",
            "       [135],\n",
            "       [ 12],\n",
            "       [135],\n",
            "       [152],\n",
            "       [103],\n",
            "       [135],\n",
            "       [103],\n",
            "       [103],\n",
            "       [166],\n",
            "       [103],\n",
            "       [103],\n",
            "       [103],\n",
            "       [103],\n",
            "       [103],\n",
            "       [103],\n",
            "       [166],\n",
            "       [166],\n",
            "       [166],\n",
            "       [166],\n",
            "       [166],\n",
            "       [152],\n",
            "       [103],\n",
            "       [106],\n",
            "       [169],\n",
            "       [169],\n",
            "       [106],\n",
            "       [169],\n",
            "       [169],\n",
            "       [169],\n",
            "       [169],\n",
            "       [106],\n",
            "       [169],\n",
            "       [106],\n",
            "       [171],\n",
            "       [171],\n",
            "       [171],\n",
            "       [171],\n",
            "       [171],\n",
            "       [171],\n",
            "       [171],\n",
            "       [171],\n",
            "       [171],\n",
            "       [116],\n",
            "       [116],\n",
            "       [116],\n",
            "       [116],\n",
            "       [116],\n",
            "       [116],\n",
            "       [147],\n",
            "       [147],\n",
            "       [145],\n",
            "       [147],\n",
            "       [145],\n",
            "       [122],\n",
            "       [122],\n",
            "       [125],\n",
            "       [122],\n",
            "       [ 63],\n",
            "       [109],\n",
            "       [ 65],\n",
            "       [122],\n",
            "       [129],\n",
            "       [129],\n",
            "       [129],\n",
            "       [123],\n",
            "       [129],\n",
            "       [129],\n",
            "       [129],\n",
            "       [129],\n",
            "       [178],\n",
            "       [178],\n",
            "       [178],\n",
            "       [178],\n",
            "       [178],\n",
            "       [178],\n",
            "       [163],\n",
            "       [163],\n",
            "       [163],\n",
            "       [163],\n",
            "       [163],\n",
            "       [ 73],\n",
            "       [163],\n",
            "       [163],\n",
            "       [163],\n",
            "       [163],\n",
            "       [163],\n",
            "       [163],\n",
            "       [163],\n",
            "       [183],\n",
            "       [177],\n",
            "       [177],\n",
            "       [177],\n",
            "       [177],\n",
            "       [177],\n",
            "       [177],\n",
            "       [177],\n",
            "       [183],\n",
            "       [148],\n",
            "       [102],\n",
            "       [175],\n",
            "       [176],\n",
            "       [148],\n",
            "       [167],\n",
            "       [167],\n",
            "       [115],\n",
            "       [230],\n",
            "       [230],\n",
            "       [115],\n",
            "       [115],\n",
            "       [115],\n",
            "       [ 21],\n",
            "       [115],\n",
            "       [115],\n",
            "       [134],\n",
            "       [163],\n",
            "       [134],\n",
            "       [134],\n",
            "       [115],\n",
            "       [115],\n",
            "       [160],\n",
            "       [115],\n",
            "       [115],\n",
            "       [115],\n",
            "       [115],\n",
            "       [115],\n",
            "       [115],\n",
            "       [115],\n",
            "       [115],\n",
            "       [144],\n",
            "       [176],\n",
            "       [144],\n",
            "       [144],\n",
            "       [144],\n",
            "       [144],\n",
            "       [144],\n",
            "       [144],\n",
            "       [144],\n",
            "       [144],\n",
            "       [144],\n",
            "       [160],\n",
            "       [160],\n",
            "       [168],\n",
            "       [120],\n",
            "       [168],\n",
            "       [168],\n",
            "       [120],\n",
            "       [168],\n",
            "       [127],\n",
            "       [141],\n",
            "       [141],\n",
            "       [141],\n",
            "       [141],\n",
            "       [141],\n",
            "       [101],\n",
            "       [101],\n",
            "       [141],\n",
            "       [141],\n",
            "       [110],\n",
            "       [164],\n",
            "       [110],\n",
            "       [110],\n",
            "       [110],\n",
            "       [110],\n",
            "       [111],\n",
            "       [105],\n",
            "       [111],\n",
            "       [111],\n",
            "       [108],\n",
            "       [111],\n",
            "       [111],\n",
            "       [110],\n",
            "       [110],\n",
            "       [ 21],\n",
            "       [111],\n",
            "       [113],\n",
            "       [113],\n",
            "       [113],\n",
            "       [113],\n",
            "       [124],\n",
            "       [106],\n",
            "       [106],\n",
            "       [106],\n",
            "       [169],\n",
            "       [169],\n",
            "       [106],\n",
            "       [169],\n",
            "       [106],\n",
            "       [132],\n",
            "       [156],\n",
            "       [156],\n",
            "       [132],\n",
            "       [156],\n",
            "       [156],\n",
            "       [132],\n",
            "       [156],\n",
            "       [156],\n",
            "       [156],\n",
            "       [156],\n",
            "       [132],\n",
            "       [156],\n",
            "       [132],\n",
            "       [156],\n",
            "       [123],\n",
            "       [123],\n",
            "       [123],\n",
            "       [147],\n",
            "       [123],\n",
            "       [123],\n",
            "       [123],\n",
            "       [123],\n",
            "       [123],\n",
            "       [125],\n",
            "       [125],\n",
            "       [125],\n",
            "       [125],\n",
            "       [125],\n",
            "       [125],\n",
            "       [125],\n",
            "       [125],\n",
            "       [125],\n",
            "       [125],\n",
            "       [125],\n",
            "       [125],\n",
            "       [125],\n",
            "       [163],\n",
            "       [163],\n",
            "       [163],\n",
            "       [163],\n",
            "       [163],\n",
            "       [163],\n",
            "       [163],\n",
            "       [163],\n",
            "       [163],\n",
            "       [ 56],\n",
            "       [108],\n",
            "       [117],\n",
            "       [ 56],\n",
            "       [161],\n",
            "       [161],\n",
            "       [161],\n",
            "       [108],\n",
            "       [161],\n",
            "       [161],\n",
            "       [161],\n",
            "       [161],\n",
            "       [137],\n",
            "       [137],\n",
            "       [137],\n",
            "       [137],\n",
            "       [137],\n",
            "       [138],\n",
            "       [137],\n",
            "       [138],\n",
            "       [165],\n",
            "       [165],\n",
            "       [ 64],\n",
            "       [165],\n",
            "       [165],\n",
            "       [165],\n",
            "       [165],\n",
            "       [165],\n",
            "       [165],\n",
            "       [165],\n",
            "       [165],\n",
            "       [154],\n",
            "       [154],\n",
            "       [154],\n",
            "       [154],\n",
            "       [154],\n",
            "       [173]]), indices=array([[0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0]], dtype=int32))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ihuJsXeCdN_k",
        "outputId": "f634986a-3d12-448a-efd0-a9657b0cd892"
      },
      "source": [
        "import tensorflow as tf\n",
        "y_true = np.array(labels)\n",
        "y_true = tf.identity(y_true)\n",
        "y_pred = np.array(final_predictions)\n",
        "y_pred = tf.identity(y_pred)\n",
        "print(y_pred.shape,y_true.shape)\n",
        "k = 2\n",
        "recall, update_recall = tf.compat.v1.metrics.recall_at_top_k(y_true, y_pred, k=2)\n",
        "precision, update_precision = tf.compat.v1.metrics.precision_at_top_k(y_true, y_pred, k=2)\n",
        "\n",
        "tmp_rank = tf.nn.top_k(y_pred, 2)\n",
        "stream_vars = [i for i in tf.local_variables()]\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.local_variables_initializer())\n",
        "    # print(\"precision\",sess.run(update_precision))\n",
        "    # print(\"precision\",s|ess.run(precision))\n",
        "\n",
        "    print(\"update_recall: \",sess.run(update_recall ))\n",
        "    print(\"recall\",sess.run(recall))\n",
        "\n",
        "    print(\"STREAM_VARS: \",(sess.run(stream_vars)))\n",
        "    print(\"TMP_RANK: \",sess.run(tmp_rank))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(417, 2) (417,)\n",
            "update_recall:  0.8513189448441247\n",
            "recall 0.8513189448441247\n",
            "STREAM_VARS:  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 355.0, 62.0, 0.0, 0.0]\n",
            "TMP_RANK:  TopKV2(values=array([[109,   2],\n",
            "       [109,   2],\n",
            "       [153, 109],\n",
            "       [109,   2],\n",
            "       [111, 109],\n",
            "       [109,   2],\n",
            "       [153, 109],\n",
            "       [ 18,  17],\n",
            "       [109,   2],\n",
            "       [109,   2],\n",
            "       [109, 108],\n",
            "       [109, 108],\n",
            "       [101,  20],\n",
            "       [141, 101],\n",
            "       [141, 101],\n",
            "       [108,  56],\n",
            "       [143, 101],\n",
            "       [101,  17],\n",
            "       [159, 135],\n",
            "       [101,  20],\n",
            "       [152, 141],\n",
            "       [143, 141],\n",
            "       [143, 141],\n",
            "       [141, 101],\n",
            "       [143, 141],\n",
            "       [141, 101],\n",
            "       [143, 141],\n",
            "       [143, 141],\n",
            "       [143, 141],\n",
            "       [143, 141],\n",
            "       [153, 108],\n",
            "       [143, 141],\n",
            "       [143, 141],\n",
            "       [141, 101],\n",
            "       [148, 105],\n",
            "       [105,  13],\n",
            "       [105,   1],\n",
            "       [105,  25],\n",
            "       [105,  17],\n",
            "       [148, 110],\n",
            "       [110, 105],\n",
            "       [ 20,  19],\n",
            "       [166, 152],\n",
            "       [166, 152],\n",
            "       [152,   3],\n",
            "       [152,   3],\n",
            "       [136, 116],\n",
            "       [172, 158],\n",
            "       [150, 149],\n",
            "       [150, 149],\n",
            "       [150, 149],\n",
            "       [169, 136],\n",
            "       [172, 158],\n",
            "       [172, 136],\n",
            "       [171, 136],\n",
            "       [172, 158],\n",
            "       [158, 136],\n",
            "       [136, 114],\n",
            "       [136, 114],\n",
            "       [114, 106],\n",
            "       [171, 114],\n",
            "       [169, 114],\n",
            "       [155, 114],\n",
            "       [155, 114],\n",
            "       [156, 132],\n",
            "       [156, 132],\n",
            "       [156, 132],\n",
            "       [157, 132],\n",
            "       [157, 132],\n",
            "       [157, 128],\n",
            "       [156, 132],\n",
            "       [156, 132],\n",
            "       [132, 131],\n",
            "       [157, 132],\n",
            "       [155, 132],\n",
            "       [156, 132],\n",
            "       [156, 132],\n",
            "       [156, 132],\n",
            "       [132, 131],\n",
            "       [131, 116],\n",
            "       [273, 179],\n",
            "       [132, 131],\n",
            "       [132, 131],\n",
            "       [131, 116],\n",
            "       [137, 133],\n",
            "       [139, 138],\n",
            "       [138, 137],\n",
            "       [138, 137],\n",
            "       [138, 137],\n",
            "       [138, 137],\n",
            "       [138, 137],\n",
            "       [137, 133],\n",
            "       [137, 133],\n",
            "       [138,  74],\n",
            "       [138,  74],\n",
            "       [138,  74],\n",
            "       [137, 133],\n",
            "       [137, 133],\n",
            "       [138, 133],\n",
            "       [138, 137],\n",
            "       [138, 133],\n",
            "       [138, 133],\n",
            "       [139, 133],\n",
            "       [118, 117],\n",
            "       [118, 117],\n",
            "       [118, 117],\n",
            "       [119, 118],\n",
            "       [118, 117],\n",
            "       [118,  79],\n",
            "       [118,  79],\n",
            "       [140, 126],\n",
            "       [140, 126],\n",
            "       [140, 126],\n",
            "       [140,  85],\n",
            "       [140, 126],\n",
            "       [140, 126],\n",
            "       [117,  56],\n",
            "       [140, 126],\n",
            "       [140,  85],\n",
            "       [140,  85],\n",
            "       [140, 117],\n",
            "       [119, 117],\n",
            "       [178, 164],\n",
            "       [167, 148],\n",
            "       [167, 151],\n",
            "       [151, 124],\n",
            "       [154,  21],\n",
            "       [151, 127],\n",
            "       [167, 127],\n",
            "       [232, 167],\n",
            "       [162, 142],\n",
            "       [142, 102],\n",
            "       [142, 102],\n",
            "       [178,  65],\n",
            "       [142, 102],\n",
            "       [154, 142],\n",
            "       [159,   7],\n",
            "       [142, 135],\n",
            "       [142, 135],\n",
            "       [142, 135],\n",
            "       [142, 102],\n",
            "       [130,  76],\n",
            "       [111,  76],\n",
            "       [159, 135],\n",
            "       [159, 135],\n",
            "       [159, 135],\n",
            "       [135,   8],\n",
            "       [135,  12],\n",
            "       [135,  12],\n",
            "       [135,  12],\n",
            "       [152, 103],\n",
            "       [135, 103],\n",
            "       [159, 135],\n",
            "       [109, 103],\n",
            "       [152, 103],\n",
            "       [166, 103],\n",
            "       [166, 103],\n",
            "       [152, 103],\n",
            "       [166, 103],\n",
            "       [166, 103],\n",
            "       [166, 103],\n",
            "       [135, 103],\n",
            "       [166,  67],\n",
            "       [166, 103],\n",
            "       [166, 103],\n",
            "       [166, 103],\n",
            "       [166, 103],\n",
            "       [166, 152],\n",
            "       [103,  11],\n",
            "       [169, 106],\n",
            "       [169, 106],\n",
            "       [169, 106],\n",
            "       [169, 106],\n",
            "       [169, 106],\n",
            "       [169, 106],\n",
            "       [169, 106],\n",
            "       [169, 106],\n",
            "       [169, 106],\n",
            "       [169, 106],\n",
            "       [132, 106],\n",
            "       [171, 106],\n",
            "       [171, 106],\n",
            "       [171, 104],\n",
            "       [171, 104],\n",
            "       [171, 106],\n",
            "       [171, 106],\n",
            "       [171, 104],\n",
            "       [171, 104],\n",
            "       [171, 106],\n",
            "       [131, 116],\n",
            "       [131, 116],\n",
            "       [169, 116],\n",
            "       [170, 116],\n",
            "       [170, 116],\n",
            "       [170, 116],\n",
            "       [147, 145],\n",
            "       [147, 145],\n",
            "       [145,  61],\n",
            "       [147,  61],\n",
            "       [147, 145],\n",
            "       [129, 122],\n",
            "       [129, 122],\n",
            "       [125,  57],\n",
            "       [122,  57],\n",
            "       [ 63,  60],\n",
            "       [109, 107],\n",
            "       [ 65,  59],\n",
            "       [123, 122],\n",
            "       [129, 122],\n",
            "       [129, 123],\n",
            "       [129, 123],\n",
            "       [123, 122],\n",
            "       [129, 122],\n",
            "       [129, 122],\n",
            "       [129, 123],\n",
            "       [129,  58],\n",
            "       [178,  65],\n",
            "       [178,  65],\n",
            "       [178, 164],\n",
            "       [178,  65],\n",
            "       [178, 164],\n",
            "       [178, 164],\n",
            "       [163, 100],\n",
            "       [163, 100],\n",
            "       [171, 163],\n",
            "       [171, 163],\n",
            "       [171, 163],\n",
            "       [ 73,  58],\n",
            "       [163,  68],\n",
            "       [163,  77],\n",
            "       [163, 145],\n",
            "       [163, 100],\n",
            "       [163, 100],\n",
            "       [163, 100],\n",
            "       [163, 100],\n",
            "       [183, 177],\n",
            "       [183, 177],\n",
            "       [177, 144],\n",
            "       [183, 177],\n",
            "       [183, 177],\n",
            "       [183, 177],\n",
            "       [183, 177],\n",
            "       [183, 177],\n",
            "       [183, 177],\n",
            "       [178, 148],\n",
            "       [154, 102],\n",
            "       [175, 174],\n",
            "       [176, 161],\n",
            "       [167, 148],\n",
            "       [167, 148],\n",
            "       [167, 148],\n",
            "       [134, 115],\n",
            "       [230, 215],\n",
            "       [230, 215],\n",
            "       [134, 115],\n",
            "       [134, 115],\n",
            "       [134, 115],\n",
            "       [219,  21],\n",
            "       [134, 115],\n",
            "       [134, 115],\n",
            "       [134, 115],\n",
            "       [163, 148],\n",
            "       [134, 115],\n",
            "       [207, 134],\n",
            "       [128, 115],\n",
            "       [134, 115],\n",
            "       [160, 115],\n",
            "       [134, 115],\n",
            "       [134, 115],\n",
            "       [134, 115],\n",
            "       [167, 115],\n",
            "       [134, 115],\n",
            "       [160, 115],\n",
            "       [134, 115],\n",
            "       [134, 115],\n",
            "       [144, 113],\n",
            "       [176, 170],\n",
            "       [144, 113],\n",
            "       [144, 116],\n",
            "       [177, 144],\n",
            "       [177, 144],\n",
            "       [144, 113],\n",
            "       [151, 144],\n",
            "       [177, 144],\n",
            "       [177, 144],\n",
            "       [144, 128],\n",
            "       [160, 148],\n",
            "       [160, 148],\n",
            "       [168, 120],\n",
            "       [168, 120],\n",
            "       [168, 120],\n",
            "       [168, 120],\n",
            "       [168, 120],\n",
            "       [168, 127],\n",
            "       [168, 127],\n",
            "       [143, 141],\n",
            "       [143, 141],\n",
            "       [143, 141],\n",
            "       [143, 141],\n",
            "       [143, 141],\n",
            "       [141, 101],\n",
            "       [141, 101],\n",
            "       [143, 141],\n",
            "       [162, 141],\n",
            "       [164, 110],\n",
            "       [164, 148],\n",
            "       [164, 110],\n",
            "       [111, 110],\n",
            "       [110, 105],\n",
            "       [234, 110],\n",
            "       [153, 111],\n",
            "       [105, 102],\n",
            "       [111,  65],\n",
            "       [111, 110],\n",
            "       [119, 108],\n",
            "       [111, 110],\n",
            "       [111,  76],\n",
            "       [111, 110],\n",
            "       [164, 110],\n",
            "       [154,  21],\n",
            "       [111, 110],\n",
            "       [216, 113],\n",
            "       [124, 113],\n",
            "       [216, 113],\n",
            "       [216, 113],\n",
            "       [124, 113],\n",
            "       [169, 106],\n",
            "       [169, 106],\n",
            "       [169, 106],\n",
            "       [169, 106],\n",
            "       [169, 106],\n",
            "       [169, 106],\n",
            "       [169, 106],\n",
            "       [169, 106],\n",
            "       [156, 132],\n",
            "       [156, 132],\n",
            "       [156, 132],\n",
            "       [157, 132],\n",
            "       [156, 132],\n",
            "       [156, 132],\n",
            "       [156, 132],\n",
            "       [156, 132],\n",
            "       [156, 132],\n",
            "       [156, 132],\n",
            "       [156, 132],\n",
            "       [156, 132],\n",
            "       [156, 132],\n",
            "       [156, 132],\n",
            "       [156, 132],\n",
            "       [123,  57],\n",
            "       [123,  57],\n",
            "       [123,  57],\n",
            "       [147, 145],\n",
            "       [123, 122],\n",
            "       [123,  57],\n",
            "       [123,  57],\n",
            "       [123, 122],\n",
            "       [123, 102],\n",
            "       [125, 123],\n",
            "       [125, 123],\n",
            "       [125,  57],\n",
            "       [125, 123],\n",
            "       [125, 123],\n",
            "       [125, 123],\n",
            "       [125,  57],\n",
            "       [125, 123],\n",
            "       [125, 123],\n",
            "       [125, 123],\n",
            "       [125, 123],\n",
            "       [125,  57],\n",
            "       [125, 123],\n",
            "       [163, 100],\n",
            "       [171, 163],\n",
            "       [171, 163],\n",
            "       [171, 163],\n",
            "       [163, 145],\n",
            "       [163, 100],\n",
            "       [171, 163],\n",
            "       [163, 100],\n",
            "       [163, 148],\n",
            "       [108,  56],\n",
            "       [108,  56],\n",
            "       [117,  56],\n",
            "       [108,  56],\n",
            "       [161, 142],\n",
            "       [161, 142],\n",
            "       [161, 108],\n",
            "       [119, 108],\n",
            "       [161, 108],\n",
            "       [164, 161],\n",
            "       [161, 107],\n",
            "       [161, 107],\n",
            "       [139, 137],\n",
            "       [138, 137],\n",
            "       [139, 137],\n",
            "       [138, 137],\n",
            "       [138, 137],\n",
            "       [138, 137],\n",
            "       [139, 137],\n",
            "       [138, 133],\n",
            "       [242, 165],\n",
            "       [242, 165],\n",
            "       [165,  64],\n",
            "       [165, 133],\n",
            "       [242, 165],\n",
            "       [165, 133],\n",
            "       [242, 165],\n",
            "       [165, 129],\n",
            "       [242, 165],\n",
            "       [242, 165],\n",
            "       [165,  82],\n",
            "       [154,  21],\n",
            "       [173, 154],\n",
            "       [154, 151],\n",
            "       [154,  21],\n",
            "       [173, 154],\n",
            "       [173, 154]]), indices=array([[0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1]], dtype=int32))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "macOxKfyfdFa",
        "outputId": "3cb8768d-86f6-4c44-9283-39e0d2b05c5e"
      },
      "source": [
        "import tensorflow as tf\n",
        "y_true = np.array(labels)\n",
        "y_true = tf.identity(y_true)\n",
        "y_pred = np.array(final_predictions)\n",
        "y_pred = tf.identity(y_pred)\n",
        "print(y_pred.shape,y_true.shape)\n",
        "k = 3\n",
        "recall, update_recall = tf.compat.v1.metrics.recall_at_top_k(y_true, y_pred, k=3)\n",
        "precision, update_precision = tf.compat.v1.metrics.precision_at_top_k(y_true, y_pred, k=3)\n",
        "\n",
        "tmp_rank = tf.nn.top_k(y_pred, 3)\n",
        "stream_vars = [i for i in tf.local_variables()]\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.local_variables_initializer())\n",
        "    # print(\"precision\",sess.run(update_precision))\n",
        "    # print(\"precision\",s|ess.run(precision))\n",
        "\n",
        "    print(\"update_recall: \",sess.run(update_recall ))\n",
        "    print(\"recall\",sess.run(recall))\n",
        "\n",
        "    print(\"STREAM_VARS: \",(sess.run(stream_vars)))\n",
        "    print(\"TMP_RANK: \",sess.run(tmp_rank))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(417, 3) (417,)\n",
            "update_recall:  0.8992805755395683\n",
            "recall 0.8992805755395683\n",
            "STREAM_VARS:  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 375.0, 42.0, 0.0, 0.0]\n",
            "TMP_RANK:  TopKV2(values=array([[109,   3,   2],\n",
            "       [109,  11,   2],\n",
            "       [153, 109,   2],\n",
            "       ...,\n",
            "       [154, 111,  21],\n",
            "       [175, 173, 154],\n",
            "       [173, 167, 154]]), indices=array([[0, 2, 1],\n",
            "       [0, 2, 1],\n",
            "       [1, 0, 2],\n",
            "       ...,\n",
            "       [0, 2, 1],\n",
            "       [2, 1, 0],\n",
            "       [0, 2, 1]], dtype=int32))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wp5F3C6LRvtA",
        "outputId": "2e28d345-f304-4e43-f708-e928eed60913"
      },
      "source": [
        "import tensorflow as tf\n",
        "y_true = np.array(labels)\n",
        "y_true = tf.identity(y_true)\n",
        "y_pred = np.array(final_predictions)\n",
        "y_pred = tf.identity(y_pred)\n",
        "print(y_pred.shape,y_true.shape)\n",
        "k = 4\n",
        "recall, update_recall = tf.compat.v1.metrics.recall_at_top_k(y_true, y_pred, k=4)\n",
        "precision, update_precision = tf.compat.v1.metrics.precision_at_top_k(y_true, y_pred, k=4)\n",
        "\n",
        "tmp_rank = tf.nn.top_k(y_pred, 4)\n",
        "stream_vars = [i for i in tf.local_variables()]\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.local_variables_initializer())\n",
        "    # print(\"precision\",sess.run(update_precision))\n",
        "    # print(\"precision\",s|ess.run(precision))\n",
        "\n",
        "    print(\"update_recall: \",sess.run(update_recall ))\n",
        "    print(\"recall\",sess.run(recall))\n",
        "\n",
        "    print(\"STREAM_VARS: \",(sess.run(stream_vars)))\n",
        "    print(\"TMP_RANK: \",sess.run(tmp_rank))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(417, 4) (417,)\n",
            "update_recall:  0.9424460431654677\n",
            "recall 0.9424460431654677\n",
            "STREAM_VARS:  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 393.0, 24.0, 0.0, 0.0]\n",
            "TMP_RANK:  TopKV2(values=array([[109,   4,   3,   2],\n",
            "       [109,  11,   4,   2],\n",
            "       [153, 109,   4,   2],\n",
            "       ...,\n",
            "       [154, 151, 111,  21],\n",
            "       [175, 173, 167, 154],\n",
            "       [175, 173, 167, 154]]), indices=array([[0, 3, 2, 1],\n",
            "       [0, 2, 3, 1],\n",
            "       [1, 0, 3, 2],\n",
            "       ...,\n",
            "       [0, 3, 2, 1],\n",
            "       [2, 1, 3, 0],\n",
            "       [3, 0, 2, 1]], dtype=int32))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4EYtX2SRvyB",
        "outputId": "93d411b5-3719-4a0d-9680-7638248cf4d3"
      },
      "source": [
        "import tensorflow as tf\n",
        "y_true = np.array(labels)\n",
        "y_true = tf.identity(y_true)\n",
        "y_pred = np.array(final_predictions)\n",
        "y_pred = tf.identity(y_pred)\n",
        "print(y_pred.shape,y_true.shape)\n",
        "k = 5\n",
        "recall, update_recall = tf.compat.v1.metrics.recall_at_top_k(y_true, y_pred, k=5)\n",
        "precision, update_precision = tf.compat.v1.metrics.precision_at_top_k(y_true, y_pred, k=5)\n",
        "\n",
        "tmp_rank = tf.nn.top_k(y_pred, 4)\n",
        "stream_vars = [i for i in tf.local_variables()]\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.local_variables_initializer())\n",
        "    print(\"precision\",sess.run(update_precision))\n",
        "    print(\"precision\",sess.run(precision))\n",
        "\n",
        "    print(\"update_recall: \",sess.run(update_recall ))\n",
        "    print(\"recall\",sess.run(recall))\n",
        "\n",
        "    print(\"STREAM_VARS: \",(sess.run(stream_vars)))\n",
        "    print(\"TMP_RANK: \",sess.run(tmp_rank))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(417, 5) (417,)\n",
            "precision 0.1908872901678657\n",
            "precision 0.1908872901678657\n",
            "update_recall:  0.9544364508393285\n",
            "recall 0.9544364508393285\n",
            "STREAM_VARS:  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 398.0, 19.0, 398.0, 1687.0]\n",
            "TMP_RANK:  TopKV2(values=array([[109,  15,   4,   3],\n",
            "       [109,  11,   4,   2],\n",
            "       [153, 109,   4,   2],\n",
            "       ...,\n",
            "       [154, 151, 148, 111],\n",
            "       [175, 173, 167, 154],\n",
            "       [175, 174, 173, 167]]), indices=array([[0, 4, 3, 2],\n",
            "       [0, 2, 3, 1],\n",
            "       [1, 0, 3, 2],\n",
            "       ...,\n",
            "       [0, 3, 4, 2],\n",
            "       [2, 1, 3, 0],\n",
            "       [3, 4, 0, 2]], dtype=int32))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nb9D-RQMfpsW"
      },
      "source": [
        "### other baselines on unseen set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jo6uiquXftFL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogNFKF6sftJX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzo_t16LOYQg"
      },
      "source": [
        "data = pd.read_csv(\"science_learning_objectives.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "CCCOEZEzR1_h",
        "outputId": "b6deaa67-88b8-4ed4-e50c-4a96a00cd242"
      },
      "source": [
        "data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>learning_objectives</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A complete chemical equation represents the re...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A chemical equation is balanced so that the nu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>In a combination reaction two or more substanc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Decomposition reactions are opposite to combin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Reactions in which heat is given out along wit...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1316</th>\n",
              "      <td>Pollutants are the substances which contaminat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1317</th>\n",
              "      <td>Carbon monoxide nitrogen oxides carbon dioxide...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1318</th>\n",
              "      <td>Increasing levels of greenhouse gases like CO ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1319</th>\n",
              "      <td>Water pollution is the contamination of water ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1320</th>\n",
              "      <td>Water which is purified and fit for drinking i...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1321 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                    learning_objectives\n",
              "0     A complete chemical equation represents the re...\n",
              "1     A chemical equation is balanced so that the nu...\n",
              "2     In a combination reaction two or more substanc...\n",
              "3     Decomposition reactions are opposite to combin...\n",
              "4     Reactions in which heat is given out along wit...\n",
              "...                                                 ...\n",
              "1316  Pollutants are the substances which contaminat...\n",
              "1317  Carbon monoxide nitrogen oxides carbon dioxide...\n",
              "1318  Increasing levels of greenhouse gases like CO ...\n",
              "1319  Water pollution is the contamination of water ...\n",
              "1320  Water which is purified and fit for drinking i...\n",
              "\n",
              "[1321 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPwOVt2MXPsK"
      },
      "source": [
        "lo_features = data[\"learning_objectives\"].values\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGKvfo-jXVXR"
      },
      "source": [
        "test_input_ids = []\n",
        "test_attention_masks = []\n",
        "for sent in lo_features:\n",
        "\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 128,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        truncation=True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    test_input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    test_attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "test_input_ids = torch.cat(test_input_ids, dim=0)\n",
        "test_attention_masks = torch.cat(test_attention_masks, dim=0)\n",
        "# labels = torch.tensor(labels)\n",
        "\n",
        "# Set the batch size.  \n",
        "batch_size = 32  \n",
        "test_poincare_tensor = torch.tensor(taxonomy_vectors,dtype=torch.float)\n",
        "\n",
        "# Create the DataLoader.\n",
        "# prediction_data = TensorDataset(test_input_ids, test_attention_masks, test_poincare_tensor)\n",
        "# prediction_sampler = SequentialSampler(prediction_data)\n",
        "# prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qj7_UH1_W7yK",
        "outputId": "fdb52667-4e50-4710-d2fa-1823025a1fe2"
      },
      "source": [
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(test_input_ids)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "cos = torch.nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "\n",
        "input_ids = test_input_ids.to('cuda')\n",
        "attention_masks = test_attention_masks.to('cuda')\n",
        "test_poincare_tensor = test_poincare_tensor.to('cuda')\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "for input_id,attention_mask in zip(input_ids, attention_masks):\n",
        "  with torch.no_grad():\n",
        "    outputs = model(input_id.reshape(1,-1),attention_mask.reshape(1,-1))\n",
        "    \n",
        "  distances = cos(outputs,test_poincare_tensor)#torch.topk(cos(outputs,test_poincare_tensor),20,largest=True)\n",
        "  distances,indices = torch.topk(distances,5,largest=True)\n",
        "  predictions.append(test_labels[indices.cpu().numpy()])\n",
        "print(len(predictions))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting labels for 1,321 test sentences...\n",
            "1321\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_8oXT1VXcJe",
        "outputId": "b6ea92b4-81f1-47b3-d8ad-2462a6c7bc8f"
      },
      "source": [
        "predictions[1302]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['science>>light', 'science>>light, shadows and reflections',\n",
              "       'science>>light - reflection and refraction',\n",
              "       'science>>human eye and colourful world', 'science'], dtype='<U116')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "nSLzMSHfXc52",
        "outputId": "d278785b-9e78-4b0b-da5d-474a993d4388"
      },
      "source": [
        "lo_features[1303]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Splitting of light into its constituent colours is known as dispersion.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awniXXkBapJS"
      },
      "source": [
        "final_result = {'lo':[],'label1':[],'label2':[],'label3':[],'label4':[],'label5':[]}\n",
        "for lo,labels in zip(lo_features,predictions):\n",
        "  final_result['lo'].append(lo)\n",
        "  final_result['label1'].append(labels[0])\n",
        "  final_result['label2'].append(labels[1])\n",
        "  final_result['label3'].append(labels[2])\n",
        "  final_result['label4'].append(labels[3])\n",
        "  final_result['label5'].append(labels[4])\n",
        "final_result = pd.DataFrame(final_result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 738
        },
        "id": "CU8dkp8sbhB0",
        "outputId": "2c0d337c-6692-4cd3-e354-58ed99ff0461"
      },
      "source": [
        "final_result"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>lo</th>\n",
              "      <th>label1</th>\n",
              "      <th>label2</th>\n",
              "      <th>label3</th>\n",
              "      <th>label4</th>\n",
              "      <th>label5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A complete chemical equation represents the re...</td>\n",
              "      <td>science&gt;&gt;chemical reactions and equations</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;chemical kinetics</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;classification ...</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;coordination co...</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;thermodynamics</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A chemical equation is balanced so that the nu...</td>\n",
              "      <td>science&gt;&gt;chemical reactions and equations</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;chemical kinetics</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;structure of atom</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;coordination co...</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;chemical bondin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>In a combination reaction two or more substanc...</td>\n",
              "      <td>science&gt;&gt;chemical reactions and equations</td>\n",
              "      <td>science&gt;&gt;physical and chemical changes</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;chemical kinetics</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;coordination co...</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;chemical bondin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Decomposition reactions are opposite to combin...</td>\n",
              "      <td>science&gt;&gt;chemical reactions and equations</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;chemical kinetics</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;coordination co...</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;classification ...</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part ii&gt;&gt;organic chemis...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Reactions in which heat is given out along wit...</td>\n",
              "      <td>science&gt;&gt;chemical reactions and equations</td>\n",
              "      <td>science&gt;&gt;combustion and flame</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;thermodynamics</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;chemical kinetics</td>\n",
              "      <td>physics&gt;&gt;physics : part - ii&gt;&gt;thermal properti...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1316</th>\n",
              "      <td>Pollutants are the substances which contaminat...</td>\n",
              "      <td>science&gt;&gt;pollution of air and water</td>\n",
              "      <td>science&gt;&gt;wastewater story</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part ii&gt;&gt;environmental ...</td>\n",
              "      <td>science&gt;&gt;sustainable management of natural res...</td>\n",
              "      <td>science&gt;&gt;our environment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1317</th>\n",
              "      <td>Carbon monoxide nitrogen oxides carbon dioxide...</td>\n",
              "      <td>science&gt;&gt;pollution of air and water</td>\n",
              "      <td>science&gt;&gt;our environment</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part ii&gt;&gt;environmental ...</td>\n",
              "      <td>science&gt;&gt;natural resources</td>\n",
              "      <td>science&gt;&gt;garbage in, garbage out</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1318</th>\n",
              "      <td>Increasing levels of greenhouse gases like CO ...</td>\n",
              "      <td>science&gt;&gt;pollution of air and water</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part ii&gt;&gt;environmental ...</td>\n",
              "      <td>science&gt;&gt;combustion and flame</td>\n",
              "      <td>science&gt;&gt;our environment</td>\n",
              "      <td>science&gt;&gt;natural resources</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1319</th>\n",
              "      <td>Water pollution is the contamination of water ...</td>\n",
              "      <td>science&gt;&gt;pollution of air and water</td>\n",
              "      <td>science&gt;&gt;wastewater story</td>\n",
              "      <td>science&gt;&gt;water: a precious resource</td>\n",
              "      <td>science&gt;&gt;sustainable management of natural res...</td>\n",
              "      <td>political science&gt;&gt;political science : contemp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1320</th>\n",
              "      <td>Water which is purified and fit for drinking i...</td>\n",
              "      <td>science&gt;&gt;wastewater story</td>\n",
              "      <td>science&gt;&gt;pollution of air and water</td>\n",
              "      <td>science&gt;&gt;sustainable management of natural res...</td>\n",
              "      <td>science&gt;&gt;water: a precious resource</td>\n",
              "      <td>science&gt;&gt;water</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1321 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                     lo  ...                                             label5\n",
              "0     A complete chemical equation represents the re...  ...      chemistry>>chemistry : part i>>thermodynamics\n",
              "1     A chemical equation is balanced so that the nu...  ...  chemistry>>chemistry : part i>>chemical bondin...\n",
              "2     In a combination reaction two or more substanc...  ...  chemistry>>chemistry : part i>>chemical bondin...\n",
              "3     Decomposition reactions are opposite to combin...  ...  chemistry>>chemistry : part ii>>organic chemis...\n",
              "4     Reactions in which heat is given out along wit...  ...  physics>>physics : part - ii>>thermal properti...\n",
              "...                                                 ...  ...                                                ...\n",
              "1316  Pollutants are the substances which contaminat...  ...                           science>>our environment\n",
              "1317  Carbon monoxide nitrogen oxides carbon dioxide...  ...                   science>>garbage in, garbage out\n",
              "1318  Increasing levels of greenhouse gases like CO ...  ...                         science>>natural resources\n",
              "1319  Water pollution is the contamination of water ...  ...  political science>>political science : contemp...\n",
              "1320  Water which is purified and fit for drinking i...  ...                                     science>>water\n",
              "\n",
              "[1321 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9ONWDk3bitg"
      },
      "source": [
        "final_result.to_csv(\"lo_with_hierarchy_labeled.csv\",index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2ZqAEVBNbqdS",
        "outputId": "46d54c3c-21b4-4494-b504-e59e7e4cf41d"
      },
      "source": [
        "final_result.head(20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>lo</th>\n",
              "      <th>label1</th>\n",
              "      <th>label2</th>\n",
              "      <th>label3</th>\n",
              "      <th>label4</th>\n",
              "      <th>label5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A complete chemical equation represents the re...</td>\n",
              "      <td>science&gt;&gt;chemical reactions and equations</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;chemical kinetics</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;classification ...</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;coordination co...</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;thermodynamics</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A chemical equation is balanced so that the nu...</td>\n",
              "      <td>science&gt;&gt;chemical reactions and equations</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;chemical kinetics</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;structure of atom</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;coordination co...</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;chemical bondin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>In a combination reaction two or more substanc...</td>\n",
              "      <td>science&gt;&gt;chemical reactions and equations</td>\n",
              "      <td>science&gt;&gt;physical and chemical changes</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;chemical kinetics</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;coordination co...</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;chemical bondin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Decomposition reactions are opposite to combin...</td>\n",
              "      <td>science&gt;&gt;chemical reactions and equations</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;chemical kinetics</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;coordination co...</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;classification ...</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part ii&gt;&gt;organic chemis...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Reactions in which heat is given out along wit...</td>\n",
              "      <td>science&gt;&gt;chemical reactions and equations</td>\n",
              "      <td>science&gt;&gt;combustion and flame</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;thermodynamics</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;chemical kinetics</td>\n",
              "      <td>physics&gt;&gt;physics : part - ii&gt;&gt;thermal properti...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Reactions in which energy is absorbed are know...</td>\n",
              "      <td>science&gt;&gt;chemical reactions and equations</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;chemical kinetics</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;thermodynamics</td>\n",
              "      <td>science&gt;&gt;physical and chemical changes</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;states of matter</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>When an element displaces another element from...</td>\n",
              "      <td>science&gt;&gt;chemical reactions and equations</td>\n",
              "      <td>science&gt;&gt;physical and chemical changes</td>\n",
              "      <td>science&gt;&gt;periodic classification of elements</td>\n",
              "      <td>science&gt;&gt;metals and non-metal</td>\n",
              "      <td>science&gt;&gt;atoms and molecules</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Precipitation reactions produce insoluble salts.</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part ii&gt;&gt;amines</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part ii&gt;&gt;aldehydes, ket...</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part ii&gt;&gt;the p-block el...</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;solutions</td>\n",
              "      <td>chemistry</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Two different atoms or groups of atoms (ions) ...</td>\n",
              "      <td>science&gt;&gt;chemical reactions and equations</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;chemical kinetics</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;chemical bondin...</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;coordination co...</td>\n",
              "      <td>science&gt;&gt;physical and chemical changes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Reactions also involve the gain or loss of oxy...</td>\n",
              "      <td>science&gt;&gt;chemical reactions and equations</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;chemical kinetics</td>\n",
              "      <td>science&gt;&gt;physical and chemical changes</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;thermodynamics</td>\n",
              "      <td>science&gt;&gt;carbon and its compounds</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Oxidation is the gain of oxygen or loss of hyd...</td>\n",
              "      <td>science&gt;&gt;chemical reactions and equations</td>\n",
              "      <td>science&gt;&gt;chemical effects of electric current</td>\n",
              "      <td>science&gt;&gt;physical and chemical changes</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;thermodynamics</td>\n",
              "      <td>chemistry</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Reduction is the loss of oxygen or gain of hyd...</td>\n",
              "      <td>science&gt;&gt;chemical reactions and equations</td>\n",
              "      <td>science&gt;&gt;chemical effects of electric current</td>\n",
              "      <td>science&gt;&gt;carbon and its compounds</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part ii&gt;&gt;hydrogen</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part ii&gt;&gt;hydrocarbons</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Acid-base indicators are dyes or mixtures of d...</td>\n",
              "      <td>science&gt;&gt;acids, bases and salts</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part ii&gt;&gt;chemistry in e...</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part ii&gt;&gt;biomolecules</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part ii&gt;&gt;aldehydes, ket...</td>\n",
              "      <td>chemistry</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>When an acid reacts with a metal hydrogen gas ...</td>\n",
              "      <td>science&gt;&gt;acids, bases and salts</td>\n",
              "      <td>science&gt;&gt;materials : metals and non-metals</td>\n",
              "      <td>science&gt;&gt;metals and non-metal</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part ii&gt;&gt;amines</td>\n",
              "      <td>science&gt;&gt;chemical effects of electric current</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>When a base reacts with a metal along with the...</td>\n",
              "      <td>science&gt;&gt;acids, bases and salts</td>\n",
              "      <td>science&gt;&gt;materials : metals and non-metals</td>\n",
              "      <td>science&gt;&gt;metals and non-metal</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part ii&gt;&gt;the p-block el...</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part ii&gt;&gt;amines</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Acidic and basic solutions in water conduct el...</td>\n",
              "      <td>science&gt;&gt;chemical effects of electric current</td>\n",
              "      <td>physical science&gt;&gt;physical science (physics)&gt;&gt;...</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;electrochemistry</td>\n",
              "      <td>science&gt;&gt;acids, bases and salts</td>\n",
              "      <td>chemistry</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Mixing concentrated acids or bases with water ...</td>\n",
              "      <td>science&gt;&gt;acids, bases and salts</td>\n",
              "      <td>science&gt;&gt;metals and non-metal</td>\n",
              "      <td>science&gt;&gt;materials : metals and non-metals</td>\n",
              "      <td>science&gt;&gt;chemical effects of electric current</td>\n",
              "      <td>science&gt;&gt;separation of substances</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>Acids and bases neutralise each other to form ...</td>\n",
              "      <td>science&gt;&gt;acids, bases and salts</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part ii&gt;&gt;aldehydes, ket...</td>\n",
              "      <td>science&gt;&gt;carbon and its compounds</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part ii&gt;&gt;chemistry in e...</td>\n",
              "      <td>science&gt;&gt;why do we fall ill?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>Water of crystallisation is the fixed number o...</td>\n",
              "      <td>science&gt;&gt;is matter around us pure</td>\n",
              "      <td>science&gt;&gt;separation of substances</td>\n",
              "      <td>science&gt;&gt;physical and chemical changes</td>\n",
              "      <td>science&gt;&gt;matter in our surroundings</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;surface chemistry</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>Salts have various uses in everyday life and i...</td>\n",
              "      <td>science&gt;&gt;acids, bases and salts</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part ii&gt;&gt;chemistry in e...</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part ii&gt;&gt;the p-block el...</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part i&gt;&gt;surface chemistry</td>\n",
              "      <td>chemistry&gt;&gt;chemistry : part ii&gt;&gt;the s-block el...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   lo  ...                                             label5\n",
              "0   A complete chemical equation represents the re...  ...      chemistry>>chemistry : part i>>thermodynamics\n",
              "1   A chemical equation is balanced so that the nu...  ...  chemistry>>chemistry : part i>>chemical bondin...\n",
              "2   In a combination reaction two or more substanc...  ...  chemistry>>chemistry : part i>>chemical bondin...\n",
              "3   Decomposition reactions are opposite to combin...  ...  chemistry>>chemistry : part ii>>organic chemis...\n",
              "4   Reactions in which heat is given out along wit...  ...  physics>>physics : part - ii>>thermal properti...\n",
              "5   Reactions in which energy is absorbed are know...  ...    chemistry>>chemistry : part i>>states of matter\n",
              "6   When an element displaces another element from...  ...                       science>>atoms and molecules\n",
              "7    Precipitation reactions produce insoluble salts.  ...                                          chemistry\n",
              "8   Two different atoms or groups of atoms (ions) ...  ...             science>>physical and chemical changes\n",
              "9   Reactions also involve the gain or loss of oxy...  ...                  science>>carbon and its compounds\n",
              "10  Oxidation is the gain of oxygen or loss of hyd...  ...                                          chemistry\n",
              "11  Reduction is the loss of oxygen or gain of hyd...  ...       chemistry>>chemistry : part ii>>hydrocarbons\n",
              "12  Acid-base indicators are dyes or mixtures of d...  ...                                          chemistry\n",
              "13  When an acid reacts with a metal hydrogen gas ...  ...      science>>chemical effects of electric current\n",
              "14  When a base reacts with a metal along with the...  ...             chemistry>>chemistry : part ii>>amines\n",
              "15  Acidic and basic solutions in water conduct el...  ...                                          chemistry\n",
              "16  Mixing concentrated acids or bases with water ...  ...                  science>>separation of substances\n",
              "17  Acids and bases neutralise each other to form ...  ...                       science>>why do we fall ill?\n",
              "18  Water of crystallisation is the fixed number o...  ...   chemistry>>chemistry : part i>>surface chemistry\n",
              "19  Salts have various uses in everyday life and i...  ...  chemistry>>chemistry : part ii>>the s-block el...\n",
              "\n",
              "[20 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U69eoLqfk4EO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}